{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwiks9635/My_Neural_Network_Architecture/blob/main/Forward_Forward_Algorithm(Img_classification).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Using the Forward-Forward Algorithm for Image Classification**"
      ],
      "metadata": {
        "id": "kllMEq_5u51j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "The following example explores how to use the Forward-Forward algorithm to perform\n",
        "training instead of the traditionally-used method of backpropagation, as proposed by\n",
        "Hinton in\n",
        "[The Forward-Forward Algorithm: Some Preliminary Investigations](https://www.cs.toronto.edu/~hinton/FFA13.pdf)\n",
        "(2022).\n",
        "\n",
        "The concept was inspired by the understanding behind\n",
        "[Boltzmann Machines](http://www.cs.toronto.edu/~fritz/absps/dbm.pdf). Backpropagation\n",
        "involves calculating the difference between actual and predicted output via a cost\n",
        "function to adjust network weights. On the other hand, the FF Algorithm suggests the\n",
        "analogy of neurons which get \"excited\" based on looking at a certain recognized\n",
        "combination of an image and its correct corresponding label.\n",
        "\n",
        "This method takes certain inspiration from the biological learning process that occurs in\n",
        "the cortex. A significant advantage that this method brings is the fact that\n",
        "backpropagation through the network does not need to be performed anymore, and that\n",
        "weight updates are local to the layer itself.\n",
        "\n",
        "As this is yet still an experimental method, it does not yield state-of-the-art results.\n",
        "But with proper tuning, it is supposed to come close to the same.\n",
        "Through this example, we will examine a process that allows us to implement the\n",
        "Forward-Forward algorithm within the layers themselves, instead of the traditional method\n",
        "of relying on the global loss functions and optimizers.\n",
        "\n",
        "The tutorial is structured as follows:\n",
        "\n",
        "- Perform necessary imports\n",
        "- Load the [MNIST dataset](http://yann.lecun.com/exdb/mnist/)\n",
        "- Visualize Random samples from the MNIST dataset\n",
        "- Define a `FFDense` Layer to override `call` and implement a custom `forwardforward`\n",
        "method which performs weight updates.\n",
        "- Define a `FFNetwork` Layer to override `train_step`, `predict` and implement 2 custom\n",
        "functions for per-sample prediction and overlaying labels\n",
        "- Convert MNIST from `NumPy` arrays to `tf.data.Dataset`\n",
        "- Fit the network\n",
        "- Visualize results\n",
        "- Perform inference on test samples\n",
        "\n",
        "As this example requires the customization of certain core functions with\n",
        "`keras.layers.Layer` and `keras.models.Model`, refer to the following resources for\n",
        "a primer on how to do so:\n",
        "\n",
        "- [Customizing what happens in `model.fit()`](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit)\n",
        "- [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)"
      ],
      "metadata": {
        "id": "UaTv9KS7vCo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.compiler.tf2xla.python import xla"
      ],
      "metadata": {
        "id": "C7U1wAz-vDIq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "print(\"4 Random Training samples and labels\")\n",
        "idx1, idx2, idx3, idx4 = random.sample(range(0, x_train.shape[0]), 4)\n",
        "\n",
        "img1 = (x_train[idx1], y_train[idx1])\n",
        "img2 = (x_train[idx2], y_train[idx2])\n",
        "img3 = (x_train[idx3], y_train[idx3])\n",
        "img4 = (x_train[idx4], y_train[idx4])\n",
        "\n",
        "imgs = [img1, img2, img3, img4]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "for idx, item in enumerate(imgs):\n",
        "    image, label = item[0], item[1]\n",
        "    plt.subplot(2, 2, idx + 1)\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    plt.title(f\"Label : {label}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "JM-5YO7WwssF",
        "outputId": "574d8d18-d93c-4e0f-f532-dcc15dbbeb79"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "4 Random Training samples and labels\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define `FFDense` custom layer\n",
        "\n",
        "In this custom layer, we have a base `keras.layers.Dense` object which acts as the\n",
        "base `Dense` layer within. Since weight updates will happen within the layer itself, we\n",
        "add an `keras.optimizers.Optimizer` object that is accepted from the user. Here, we\n",
        "use `Adam` as our optimizer with a rather higher learning rate of `0.03`.\n",
        "\n",
        "Following the algorithm's specifics, we must set a `threshold` parameter that will be\n",
        "used to make the positive-negative decision in each prediction. This is set to a default\n",
        "of 2.0.\n",
        "As the epochs are localized to the layer itself, we also set a `num_epochs` parameter\n",
        "(defaults to 50).\n",
        "\n",
        "We override the `call` method in order to perform a normalization over the complete\n",
        "input space followed by running it through the base `Dense` layer as would happen in a\n",
        "normal `Dense` layer call.\n",
        "\n",
        "We implement the Forward-Forward algorithm which accepts 2 kinds of input tensors, each\n",
        "representing the positive and negative samples respectively. We write a custom training\n",
        "loop here with the use of `tf.GradientTape()`, within which we calculate a loss per\n",
        "sample by taking the distance of the prediction from the threshold to understand the\n",
        "error and taking its mean to get a `mean_loss` metric.\n",
        "\n",
        "With the help of `tf.GradientTape()` we calculate the gradient updates for the trainable\n",
        "base `Dense` layer and apply them using the layer's local optimizer.\n",
        "\n",
        "Finally, we return the `call` result as the `Dense` results of the positive and negative\n",
        "samples while also returning the last `mean_loss` metric and all the loss values over a\n",
        "certain all-epoch run."
      ],
      "metadata": {
        "id": "AifvXuto6dQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FFDense(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom ForwardForward-enabled Dense layer. It has an implementation of the\n",
        "    Forward-Forward network internally for use.\n",
        "    This layer must be used in conjunction with the `FFNetwork` model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        units,\n",
        "        optimizer,\n",
        "        loss_metric,\n",
        "        num_epochs=50,\n",
        "        use_bias=True,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = keras.layers.Dense(\n",
        "            units=units,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "        )\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_metric = loss_metric\n",
        "        self.threshold = 1.5\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    # We perform a normalization step before we run the input through the Dense\n",
        "    # layer.\n",
        "\n",
        "    def call(self, x):\n",
        "        x_norm = tf.norm(x, ord=2, axis=1, keepdims=True)\n",
        "        x_norm = x_norm + 1e-4\n",
        "        x_dir = x / x_norm\n",
        "        res = self.dense(x_dir)\n",
        "        return self.relu(res)\n",
        "\n",
        "    # The Forward-Forward algorithm is below. We first perform the Dense-layer\n",
        "    # operation and then get a Mean Square value for all positive and negative\n",
        "    # samples respectively.\n",
        "    # The custom loss function finds the distance between the Mean-squared\n",
        "    # result and the threshold value we set (a hyperparameter) that will define\n",
        "    # whether the prediction is positive or negative in nature. Once the loss is\n",
        "    # calculated, we get a mean across the entire batch combined and perform a\n",
        "    # gradient calculation and optimization step. This does not technically\n",
        "    # qualify as backpropagation since there is no gradient being\n",
        "    # sent to any previous layer and is completely local in nature.\n",
        "\n",
        "    def forward_forward(self, x_pos, x_neg):\n",
        "        for i in range(self.num_epochs):\n",
        "            with tf.GradientTape() as tape:\n",
        "                g_pos = tf.math.reduce_mean(tf.math.pow(self.call(x_pos), 2), 1)\n",
        "                g_neg = tf.math.reduce_mean(tf.math.pow(self.call(x_neg), 2), 1)\n",
        "\n",
        "                loss = tf.math.log(\n",
        "                    1\n",
        "                    + tf.math.exp(\n",
        "                        tf.concat([-g_pos + self.threshold, g_neg - self.threshold], 0)\n",
        "                    )\n",
        "                )\n",
        "                mean_loss = tf.cast(tf.math.reduce_mean(loss), tf.float32)\n",
        "                self.loss_metric.update_state([mean_loss])\n",
        "            gradients = tape.gradient(mean_loss, self.dense.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.dense.trainable_weights))\n",
        "        return (\n",
        "            tf.stop_gradient(self.call(x_pos)),\n",
        "            tf.stop_gradient(self.call(x_neg)),\n",
        "            self.loss_metric.result(),\n",
        "        )"
      ],
      "metadata": {
        "id": "UddxAvX3xucM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the `FFNetwork` Custom Model\n",
        "\n",
        "With our custom layer defined, we also need to override the `train_step` method and\n",
        "define a custom `keras.models.Model` that works with our `FFDense` layer.\n",
        "\n",
        "For this algorithm, we must 'embed' the labels onto the original image. To do so, we\n",
        "exploit the structure of MNIST images where the top-left 10 pixels are always zeros. We\n",
        "use that as a label space in order to visually one-hot-encode the labels within the image\n",
        "itself. This action is performed by the `overlay_y_on_x` function.\n",
        "\n",
        "We break down the prediction function with a per-sample prediction function which is then\n",
        "called over the entire test set by the overriden `predict()` function. The prediction is\n",
        "performed here with the help of measuring the `excitation` of the neurons per layer for\n",
        "each image. This is then summed over all layers to calculate a network-wide 'goodness\n",
        "score'. The label with the highest 'goodness score' is then chosen as the sample\n",
        "prediction.\n",
        "\n",
        "The `train_step` function is overriden to act as the main controlling loop for running\n",
        "training on each layer as per the number of epochs per layer."
      ],
      "metadata": {
        "id": "V3h_TKab6g15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FFNetwork(keras.Model):\n",
        "    \"\"\"\n",
        "    A [`keras.Model`](/api/models/model#model-class) that supports a `FFDense` network creation. This model\n",
        "    can work for any kind of classification task. It has an internal\n",
        "    implementation with some details specific to the MNIST dataset which can be\n",
        "    changed as per the use-case.\n",
        "    \"\"\"\n",
        "\n",
        "    # Since each layer runs gradient-calculation and optimization locally, each\n",
        "    # layer has its own optimizer that we pass. As a standard choice, we pass\n",
        "    # the `Adam` optimizer with a default learning rate of 0.03 as that was\n",
        "    # found to be the best rate after experimentation.\n",
        "    # Loss is tracked using `loss_var` and `loss_count` variables.\n",
        "    # Use legacy optimizer for Layer Optimizer to fix issue\n",
        "    # https://github.com/keras-team/keras-io/issues/1241\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dims,\n",
        "        layer_optimizer=keras.optimizers.legacy.Adam(learning_rate=0.03),\n",
        "        **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer_optimizer = layer_optimizer\n",
        "        self.loss_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.loss_count = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.layer_list = [keras.Input(shape=(dims[0],))]\n",
        "        for d in range(len(dims) - 1):\n",
        "            self.layer_list += [\n",
        "                FFDense(\n",
        "                    dims[d + 1],\n",
        "                    optimizer=self.layer_optimizer,\n",
        "                    loss_metric=keras.metrics.Mean(),\n",
        "                )\n",
        "            ]\n",
        "\n",
        "    # This function makes a dynamic change to the image wherein the labels are\n",
        "    # put on top of the original image (for this example, as MNIST has 10\n",
        "    # unique labels, we take the top-left corner's first 10 pixels). This\n",
        "    # function returns the original data tensor with the first 10 pixels being\n",
        "    # a pixel-based one-hot representation of the labels.\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def overlay_y_on_x(self, data):\n",
        "        X_sample, y_sample = data\n",
        "        max_sample = tf.reduce_max(X_sample, axis=0, keepdims=True)\n",
        "        max_sample = tf.cast(max_sample, dtype=tf.float64)\n",
        "        X_zeros = tf.zeros([10], dtype=tf.float64)\n",
        "        X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])\n",
        "        X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])\n",
        "        return X_sample, y_sample\n",
        "\n",
        "    # A custom `predict_one_sample` performs predictions by passing the images\n",
        "    # through the network, measures the results produced by each layer (i.e.\n",
        "    # how high/low the output values are with respect to the set threshold for\n",
        "    # each label) and then simply finding the label with the highest values.\n",
        "    # In such a case, the images are tested for their 'goodness' with all\n",
        "    # labels.\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def predict_one_sample(self, x):\n",
        "        goodness_per_label = []\n",
        "        x = tf.reshape(x, [tf.shape(x)[0] * tf.shape(x)[1]])\n",
        "        for label in range(10):\n",
        "            h, label = self.overlay_y_on_x(data=(x, label))\n",
        "            h = tf.reshape(h, [-1, tf.shape(h)[0]])\n",
        "            goodness = []\n",
        "            for layer_idx in range(1, len(self.layer_list)):\n",
        "                layer = self.layer_list[layer_idx]\n",
        "                h = layer(h)\n",
        "                goodness += [tf.math.reduce_mean(tf.math.pow(h, 2), 1)]\n",
        "            goodness_per_label += [\n",
        "                tf.expand_dims(tf.reduce_sum(goodness, keepdims=True), 1)\n",
        "            ]\n",
        "        goodness_per_label = tf.concat(goodness_per_label, 1)\n",
        "        return tf.cast(tf.argmax(goodness_per_label, 1), tf.float64)\n",
        "\n",
        "    def predict(self, data):\n",
        "        x = data\n",
        "        preds = list()\n",
        "        preds = tf.map_fn(fn=self.predict_one_sample, elems=x)\n",
        "        return np.asarray(preds, dtype=int)\n",
        "\n",
        "    # This custom `train_step` function overrides the internal `train_step`\n",
        "    # implementation. We take all the input image tensors, flatten them and\n",
        "    # subsequently produce positive and negative samples on the images.\n",
        "    # A positive sample is an image that has the right label encoded on it with\n",
        "    # the `overlay_y_on_x` function. A negative sample is an image that has an\n",
        "    # erroneous label present on it.\n",
        "    # With the samples ready, we pass them through each `FFLayer` and perform\n",
        "    # the Forward-Forward computation on it. The returned loss is the final\n",
        "    # loss value over all the layers.\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Flatten op\n",
        "        x = tf.reshape(x, [-1, tf.shape(x)[1] * tf.shape(x)[2]])\n",
        "\n",
        "        x_pos, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, y))\n",
        "\n",
        "        random_y = tf.random.shuffle(y)\n",
        "        x_neg, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, random_y))\n",
        "\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, FFDense):\n",
        "                print(f\"Training layer {idx+1} now : \")\n",
        "                h_pos, h_neg, loss = layer.forward_forward(h_pos, h_neg)\n",
        "                self.loss_var.assign_add(loss)\n",
        "                self.loss_count.assign_add(1.0)\n",
        "            else:\n",
        "                print(f\"Passing layer {idx+1} now : \")\n",
        "                x = layer(x)\n",
        "        mean_res = tf.math.divide(self.loss_var, self.loss_count)\n",
        "        return {\"FinalLoss\": mean_res}"
      ],
      "metadata": {
        "id": "PP_6S1chz4jm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype(float) / 255\n",
        "x_test = x_test.astype(float) / 255\n",
        "y_train = y_train.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "train_dataset = train_dataset.batch(60000)\n",
        "test_dataset = test_dataset.batch(10000)"
      ],
      "metadata": {
        "id": "eVS3q_lu27Mi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit the network and visualize results\n",
        "\n",
        "Having performed all previous set-up, we are now going to run `model.fit()` and run 250\n",
        "model epochs, which will perform 50*250 epochs on each layer. We get to see the plotted loss\n",
        "curve as each layer is trained."
      ],
      "metadata": {
        "id": "teaC0-l77Hoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FFNetwork(dims=[784, 500, 500])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.03),\n",
        "    loss=\"mse\",\n",
        "    jit_compile=True,\n",
        "    metrics=[keras.metrics.Mean()])\n",
        "\n",
        "epochs = 250\n",
        "history = model.fit(train_dataset, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilhcPONb5tzp",
        "outputId": "3cc7e21e-6577-4670-d3e6-75055719348f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "Training layer 1 now : \n",
            "Training layer 2 now : \n",
            "Training layer 1 now : \n",
            "Training layer 2 now : \n",
            "1/1 [==============================] - 91s 91s/step - FinalLoss: 0.7281\n",
            "Epoch 2/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.7085\n",
            "Epoch 3/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.7009\n",
            "Epoch 4/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.6788\n",
            "Epoch 5/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.6555\n",
            "Epoch 6/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.6334\n",
            "Epoch 7/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.6135\n",
            "Epoch 8/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5959\n",
            "Epoch 9/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5802\n",
            "Epoch 10/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5663\n",
            "Epoch 11/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5538\n",
            "Epoch 12/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5427\n",
            "Epoch 13/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5325\n",
            "Epoch 14/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5233\n",
            "Epoch 15/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5149\n",
            "Epoch 16/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5072\n",
            "Epoch 17/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.5001\n",
            "Epoch 18/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4936\n",
            "Epoch 19/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4875\n",
            "Epoch 20/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4818\n",
            "Epoch 21/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4765\n",
            "Epoch 22/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4716\n",
            "Epoch 23/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4670\n",
            "Epoch 24/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4627\n",
            "Epoch 25/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4586\n",
            "Epoch 26/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4548\n",
            "Epoch 27/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4511\n",
            "Epoch 28/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4476\n",
            "Epoch 29/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4442\n",
            "Epoch 30/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4411\n",
            "Epoch 31/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4381\n",
            "Epoch 32/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4352\n",
            "Epoch 33/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4325\n",
            "Epoch 34/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4299\n",
            "Epoch 35/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4274\n",
            "Epoch 36/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4250\n",
            "Epoch 37/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4228\n",
            "Epoch 38/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4206\n",
            "Epoch 39/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4185\n",
            "Epoch 40/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4165\n",
            "Epoch 41/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4145\n",
            "Epoch 42/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4126\n",
            "Epoch 43/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4107\n",
            "Epoch 44/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4090\n",
            "Epoch 45/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4073\n",
            "Epoch 46/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4056\n",
            "Epoch 47/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4040\n",
            "Epoch 48/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4025\n",
            "Epoch 49/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.4010\n",
            "Epoch 50/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3995\n",
            "Epoch 51/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3980\n",
            "Epoch 52/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3966\n",
            "Epoch 53/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3953\n",
            "Epoch 54/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3940\n",
            "Epoch 55/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3927\n",
            "Epoch 56/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3914\n",
            "Epoch 57/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3901\n",
            "Epoch 58/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3889\n",
            "Epoch 59/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3878\n",
            "Epoch 60/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3867\n",
            "Epoch 61/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3856\n",
            "Epoch 62/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3845\n",
            "Epoch 63/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3834\n",
            "Epoch 64/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3823\n",
            "Epoch 65/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3813\n",
            "Epoch 66/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3803\n",
            "Epoch 67/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3793\n",
            "Epoch 68/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3783\n",
            "Epoch 69/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3774\n",
            "Epoch 70/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3765\n",
            "Epoch 71/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3755\n",
            "Epoch 72/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3746\n",
            "Epoch 73/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3737\n",
            "Epoch 74/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3729\n",
            "Epoch 75/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3720\n",
            "Epoch 76/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3712\n",
            "Epoch 77/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3704\n",
            "Epoch 78/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3696\n",
            "Epoch 79/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3687\n",
            "Epoch 80/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3680\n",
            "Epoch 81/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3672\n",
            "Epoch 82/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3664\n",
            "Epoch 83/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3657\n",
            "Epoch 84/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3650\n",
            "Epoch 85/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3643\n",
            "Epoch 86/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3636\n",
            "Epoch 87/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3629\n",
            "Epoch 88/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3622\n",
            "Epoch 89/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3615\n",
            "Epoch 90/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3608\n",
            "Epoch 91/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3602\n",
            "Epoch 92/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3596\n",
            "Epoch 93/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3589\n",
            "Epoch 94/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3583\n",
            "Epoch 95/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3577\n",
            "Epoch 96/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3571\n",
            "Epoch 97/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3565\n",
            "Epoch 98/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3559\n",
            "Epoch 99/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3553\n",
            "Epoch 100/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3547\n",
            "Epoch 101/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3541\n",
            "Epoch 102/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3536\n",
            "Epoch 103/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3530\n",
            "Epoch 104/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3525\n",
            "Epoch 105/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3519\n",
            "Epoch 106/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3514\n",
            "Epoch 107/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3508\n",
            "Epoch 108/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3503\n",
            "Epoch 109/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3498\n",
            "Epoch 110/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3493\n",
            "Epoch 111/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3488\n",
            "Epoch 112/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3483\n",
            "Epoch 113/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3478\n",
            "Epoch 114/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3473\n",
            "Epoch 115/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3468\n",
            "Epoch 116/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3464\n",
            "Epoch 117/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3459\n",
            "Epoch 118/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3454\n",
            "Epoch 119/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3450\n",
            "Epoch 120/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3445\n",
            "Epoch 121/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3441\n",
            "Epoch 122/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3437\n",
            "Epoch 123/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3432\n",
            "Epoch 124/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3428\n",
            "Epoch 125/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3424\n",
            "Epoch 126/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3419\n",
            "Epoch 127/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3415\n",
            "Epoch 128/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3411\n",
            "Epoch 129/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3407\n",
            "Epoch 130/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3403\n",
            "Epoch 131/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3399\n",
            "Epoch 132/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3395\n",
            "Epoch 133/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3392\n",
            "Epoch 134/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3388\n",
            "Epoch 135/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3384\n",
            "Epoch 136/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3380\n",
            "Epoch 137/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3377\n",
            "Epoch 138/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3373\n",
            "Epoch 139/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3369\n",
            "Epoch 140/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3366\n",
            "Epoch 141/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3362\n",
            "Epoch 142/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3358\n",
            "Epoch 143/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3355\n",
            "Epoch 144/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3351\n",
            "Epoch 145/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3348\n",
            "Epoch 146/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3345\n",
            "Epoch 147/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3341\n",
            "Epoch 148/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3338\n",
            "Epoch 149/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3335\n",
            "Epoch 150/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3331\n",
            "Epoch 151/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3328\n",
            "Epoch 152/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3325\n",
            "Epoch 153/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3322\n",
            "Epoch 154/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3319\n",
            "Epoch 155/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3316\n",
            "Epoch 156/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3313\n",
            "Epoch 157/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3310\n",
            "Epoch 158/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3307\n",
            "Epoch 159/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3304\n",
            "Epoch 160/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3301\n",
            "Epoch 161/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3298\n",
            "Epoch 162/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3295\n",
            "Epoch 163/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3292\n",
            "Epoch 164/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3289\n",
            "Epoch 165/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3287\n",
            "Epoch 166/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3284\n",
            "Epoch 167/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3281\n",
            "Epoch 168/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3278\n",
            "Epoch 169/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3276\n",
            "Epoch 170/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3273\n",
            "Epoch 171/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3270\n",
            "Epoch 172/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3268\n",
            "Epoch 173/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3265\n",
            "Epoch 174/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3262\n",
            "Epoch 175/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3260\n",
            "Epoch 176/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3257\n",
            "Epoch 177/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3255\n",
            "Epoch 178/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3252\n",
            "Epoch 179/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3250\n",
            "Epoch 180/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3247\n",
            "Epoch 181/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3245\n",
            "Epoch 182/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3242\n",
            "Epoch 183/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3240\n",
            "Epoch 184/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3238\n",
            "Epoch 185/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3235\n",
            "Epoch 186/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3233\n",
            "Epoch 187/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3231\n",
            "Epoch 188/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3228\n",
            "Epoch 189/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3226\n",
            "Epoch 190/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3224\n",
            "Epoch 191/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3222\n",
            "Epoch 192/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3219\n",
            "Epoch 193/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3217\n",
            "Epoch 194/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3215\n",
            "Epoch 195/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3213\n",
            "Epoch 196/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3211\n",
            "Epoch 197/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3209\n",
            "Epoch 198/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3207\n",
            "Epoch 199/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3205\n",
            "Epoch 200/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3203\n",
            "Epoch 201/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3201\n",
            "Epoch 202/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3198\n",
            "Epoch 203/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3196\n",
            "Epoch 204/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3194\n",
            "Epoch 205/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3192\n",
            "Epoch 206/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3190\n",
            "Epoch 207/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3188\n",
            "Epoch 208/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3186\n",
            "Epoch 209/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3184\n",
            "Epoch 210/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3182\n",
            "Epoch 211/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3181\n",
            "Epoch 212/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3179\n",
            "Epoch 213/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3177\n",
            "Epoch 214/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3175\n",
            "Epoch 215/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3173\n",
            "Epoch 216/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3171\n",
            "Epoch 217/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3169\n",
            "Epoch 218/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3168\n",
            "Epoch 219/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3166\n",
            "Epoch 220/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3164\n",
            "Epoch 221/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3162\n",
            "Epoch 222/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3160\n",
            "Epoch 223/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3159\n",
            "Epoch 224/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3157\n",
            "Epoch 225/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3155\n",
            "Epoch 226/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3154\n",
            "Epoch 227/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3152\n",
            "Epoch 228/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3150\n",
            "Epoch 229/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3149\n",
            "Epoch 230/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3147\n",
            "Epoch 231/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3145\n",
            "Epoch 232/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3144\n",
            "Epoch 233/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3142\n",
            "Epoch 234/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3140\n",
            "Epoch 235/250\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.3139\n",
            "Epoch 236/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3137\n",
            "Epoch 237/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3136\n",
            "Epoch 238/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3134\n",
            "Epoch 239/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3133\n",
            "Epoch 240/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3131\n",
            "Epoch 241/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3130\n",
            "Epoch 242/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3128\n",
            "Epoch 243/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3127\n",
            "Epoch 244/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3125\n",
            "Epoch 245/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3123\n",
            "Epoch 246/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3122\n",
            "Epoch 247/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3121\n",
            "Epoch 248/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3119\n",
            "Epoch 249/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3118\n",
            "Epoch 250/250\n",
            "1/1 [==============================] - 6s 6s/step - FinalLoss: 0.3116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform inference and testing\n",
        "\n",
        "Having trained the model to a large extent, we now see how it performs on the\n",
        "test set. We calculate the Accuracy Score to understand the results closely."
      ],
      "metadata": {
        "id": "1cA_erdI7Nyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(tf.convert_to_tensor(x_test))\n",
        "\n",
        "preds = preds.reshape((preds.shape[0], preds.shape[1]))\n",
        "\n",
        "results = accuracy_score(preds, y_test)\n",
        "\n",
        "print(f\"Test Accuracy score : {results*100}%\")\n",
        "\n",
        "plt.plot(range(len(history.history[\"FinalLoss\"])), history.history[\"FinalLoss\"])\n",
        "plt.title(\"Loss over training\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "lp6XrLMb6I1j",
        "outputId": "a2eada67-952f-4693-c95f-59018f0a17b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy score : 97.74000000000001%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This example has hereby demonstrated how the Forward-Forward algorithm works using\n",
        "the TensorFlow and Keras packages. While the investigation results presented by Prof. Hinton\n",
        "in their paper are currently still limited to smaller models and datasets like MNIST and\n",
        "Fashion-MNIST, subsequent results on larger models like LLMs are expected in future\n",
        "papers.\n",
        "\n",
        "Through the paper, Prof. Hinton has reported results of 1.36% test accuracy error with a\n",
        "2000-units, 4 hidden-layer, fully-connected network run over 60 epochs (while mentioning\n",
        "that backpropagation takes only 20 epochs to achieve similar performance). Another run of\n",
        "doubling the learning rate and training for 40 epochs yields a slightly worse error rate\n",
        "of 1.46%\n",
        "\n",
        "The current example does not yield state-of-the-art results. But with proper tuning of\n",
        "the Learning Rate, model architecture (number of units in `Dense` layers, kernel\n",
        "activations, initializations, regularization etc.), the results can be improved\n",
        "to match the claims of the paper."
      ],
      "metadata": {
        "id": "0TrkyvPM7QZb"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}