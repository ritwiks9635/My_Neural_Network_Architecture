{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwiks9635/My_Neural_Network_Architecture/blob/main/Conditional_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conditional GAN**"
      ],
      "metadata": {
        "id": "KqGWXYKO-2Yy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative Adversarial Networks (GANs) let us generate novel image data, video data,\n",
        "or audio data from a random input. Typically, the random input is sampled\n",
        "from a normal distribution, before going through a series of transformations that turn\n",
        "it into something plausible (image, video, audio, etc.).\n",
        "\n",
        "However, a simple [DCGAN](https://arxiv.org/abs/1511.06434) doesn't let us control\n",
        "the appearance (e.g. class) of the samples we're generating. For instance,\n",
        "with a GAN that generates MNIST handwritten digits, a simple DCGAN wouldn't let us\n",
        "choose the class of digits we're generating.\n",
        "To be able to control what we generate, we need to _condition_ the GAN output\n",
        "on a semantic input, such as the class of an image.\n",
        "\n",
        "In this example, we'll build a **Conditional GAN** that can generate MNIST handwritten\n",
        "digits conditioned on a given class. Such a model can have various useful applications:\n",
        "\n",
        "* let's say you are dealing with an\n",
        "[imbalanced image dataset](https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data),\n",
        "and you'd like to gather more examples for the skewed class to balance the dataset.\n",
        "Data collection can be a costly process on its own. You could instead train a Conditional GAN and use\n",
        "it to generate novel images for the class that needs balancing.\n",
        "* Since the generator learns to associate the generated samples with the class labels,\n",
        "its representations can also be used for [other downstream tasks](https://arxiv.org/abs/1809.11096).\n",
        "\n",
        "Following are the references used for developing this example:\n",
        "\n",
        "* [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784)\n",
        "* [Lecture on Conditional Generation from Coursera](https://www.coursera.org/lecture/build-basic-generative-adversarial-networks-gans/conditional-generation-inputs-2OPrG)\n",
        "\n",
        "If you need a refresher on GANs, you can refer to the \"Generative adversarial networks\"\n",
        "section of\n",
        "[this resource](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-12/r-3/232).\n",
        "\n",
        "This example requires TensorFlow 2.5 or higher, as well as TensorFlow Docs, which can be\n",
        "installed using the following command:"
      ],
      "metadata": {
        "id": "oIIVRp-aVCPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0qfor_v-2B0",
        "outputId": "047a708f-b9df-4dd3-c429-cc5746d04ad7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow_docs.vis import embed\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import imageio"
      ],
      "metadata": {
        "id": "Ln8AR31eAEQa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_channels = 1\n",
        "num_classes = 10\n",
        "image_size = 28\n",
        "latent_dim = 128"
      ],
      "metadata": {
        "id": "OVmIZ4iQAMEo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use all the available examples from both the training and test\n",
        "# sets.\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "all_digits = np.concatenate([x_train, x_test])\n",
        "all_labels = np.concatenate([y_train, y_test])\n",
        "\n",
        "# Scale the pixel values to [0, 1] range, add a channel dimension to\n",
        "# the images, and one-hot encode the labels.\n",
        "all_digits = all_digits.astype(\"float32\") / 255.0\n",
        "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
        "all_labels = keras.utils.to_categorical(all_labels, 10)\n",
        "\n",
        "# Create tf.data.Dataset.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((all_digits, all_labels))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "print(f\"Shape of training images: {all_digits.shape}\")\n",
        "print(f\"Shape of training labels: {all_labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQCnCSXHAPcS",
        "outputId": "8bd22ba1-3633-4a0b-9dbd-96787e5426de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "Shape of training images: (70000, 28, 28, 1)\n",
            "Shape of training labels: (70000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating the number of input channel for the generator and discriminator\n",
        "\n",
        "In a regular (unconditional) GAN, we start by sampling noise (of some fixed\n",
        "dimension) from a normal distribution. In our case, we also need to account\n",
        "for the class labels. We will have to add the number of classes to\n",
        "the input channels of the generator (noise input) as well as the discriminator\n",
        "(generated image input)."
      ],
      "metadata": {
        "id": "mqimufOeVJfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator_in_channels = latent_dim + num_classes\n",
        "discriminator_in_channels = num_channels + num_classes\n",
        "print(generator_in_channels)\n",
        "print(discriminator_in_channels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REYMrSsYAh14",
        "outputId": "fb956fc4-6829-4f1a-e853-52b282a6e26c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "138\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the discriminator and generator\n",
        "\n",
        "The model definitions (`discriminator`, `generator`, and `ConditionalGAN`) have been\n",
        "adapted from [this example](https://keras.io/guides/customizing_what_happens_in_fit/)."
      ],
      "metadata": {
        "id": "9FrFyBFnVPNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        layers.InputLayer((28, 28, discriminator_in_channels)),\n",
        "        layers.Conv2D(64, (3, 3), strides = (2, 2), padding = \"same\"),\n",
        "        layers.LeakyReLU(alpha = 0.2),\n",
        "        layers.Conv2D(128, (3, 3), strides = (2, 2), padding = \"same\"),\n",
        "        layers.LeakyReLU(alpha = 0.2),\n",
        "        layers.GlobalMaxPooling2D(),\n",
        "        layers.Dense(1)\n",
        "    ], name = \"discriminator\")\n",
        "\n",
        "\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        layers.InputLayer((generator_in_channels,)),\n",
        "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
        "        # 7x7x(128 + num_classes) map.\n",
        "        layers.Dense(7 * 7 * generator_in_channels),\n",
        "        layers.LeakyReLU(alpha = 0.2),\n",
        "        layers.Reshape((7, 7, generator_in_channels)),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides = (2, 2), padding = \"same\"),\n",
        "        layers.LeakyReLU(alpha = 0.2),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides = (2, 2), padding = \"same\"),\n",
        "        layers.LeakyReLU(alpha = 0.2),\n",
        "        layers.Conv2D(1, (7, 7), padding = \"same\", activation = \"sigmoid\")\n",
        "    ], name = \"generator\")"
      ],
      "metadata": {
        "id": "UeyqjRROA75h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a `ConditionalGAN` model"
      ],
      "metadata": {
        "id": "fR_MbM4hVSHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalGAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
        "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data.\n",
        "        real_images, one_hot_labels = data\n",
        "\n",
        "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
        "        # the images. This is for the discriminator.\n",
        "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
        "        image_one_hot_labels = tf.repeat(\n",
        "            image_one_hot_labels, repeats=[image_size * image_size]\n",
        "        )\n",
        "        image_one_hot_labels = tf.reshape(\n",
        "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space and concatenate the labels.\n",
        "        # This is for the generator.\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        random_vector_labels = tf.concat(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Decode the noise (guided by labels) to fake images.\n",
        "        generated_images = self.generator(random_vector_labels)\n",
        "\n",
        "        # Combine them with real images. Note that we are concatenating the labels\n",
        "        # with these images here.\n",
        "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
        "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
        "        combined_images = tf.concat(\n",
        "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
        "        )\n",
        "\n",
        "        # Assemble labels discriminating real from fake images.\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "\n",
        "        # Train the discriminator.\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space.\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        random_vector_labels = tf.concat(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Assemble labels that say \"all real images\".\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_images = self.generator(random_vector_labels)\n",
        "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
        "            predictions = self.discriminator(fake_image_and_labels)\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Monitor loss.\n",
        "        self.gen_loss_tracker.update_state(g_loss)\n",
        "        self.disc_loss_tracker.update_state(d_loss)\n",
        "        return {\n",
        "            \"g_loss\": self.gen_loss_tracker.result(),\n",
        "            \"d_loss\": self.disc_loss_tracker.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "gSr0UywoGBAD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Conditional GAN"
      ],
      "metadata": {
        "id": "Mgxs2RbhVa69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cond_gan = ConditionalGAN(\n",
        "    discriminator = discriminator, generator = generator, latent_dim = latent_dim)\n",
        "\n",
        "cond_gan.compile(\n",
        "    d_optimizer = keras.optimizers.Adam(learning_rate = 0.0003),\n",
        "    g_optimizer = keras.optimizers.Adam(learning_rate = 0.0003),\n",
        "    loss_fn = keras.losses.BinaryCrossentropy(from_logits = True))\n",
        "\n",
        "cond_gan.fit(dataset, epochs = 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My8tv8kOH_Oj",
        "outputId": "c856271e-4cdc-4cb9-f988-2aee5a5d0848"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1094/1094 [==============================] - 41s 29ms/step - g_loss: 1.6619 - d_loss: 0.4056\n",
            "Epoch 2/20\n",
            "1094/1094 [==============================] - 29s 27ms/step - g_loss: 1.4430 - d_loss: 0.4689\n",
            "Epoch 3/20\n",
            "1094/1094 [==============================] - 30s 27ms/step - g_loss: 1.5257 - d_loss: 0.4193\n",
            "Epoch 4/20\n",
            "1094/1094 [==============================] - 30s 28ms/step - g_loss: 2.1295 - d_loss: 0.2627\n",
            "Epoch 5/20\n",
            "1094/1094 [==============================] - 30s 27ms/step - g_loss: 2.2563 - d_loss: 0.2739\n",
            "Epoch 6/20\n",
            "1094/1094 [==============================] - 30s 27ms/step - g_loss: 1.0596 - d_loss: 0.6217\n",
            "Epoch 7/20\n",
            "1094/1094 [==============================] - 30s 27ms/step - g_loss: 0.8652 - d_loss: 0.6617\n",
            "Epoch 8/20\n",
            "1094/1094 [==============================] - 31s 28ms/step - g_loss: 0.8371 - d_loss: 0.6640\n",
            "Epoch 9/20\n",
            "1094/1094 [==============================] - 30s 27ms/step - g_loss: 0.8670 - d_loss: 0.6625\n",
            "Epoch 10/20\n",
            "1094/1094 [==============================] - 30s 27ms/step - g_loss: 0.7946 - d_loss: 0.6780\n",
            "Epoch 11/20\n",
            "1094/1094 [==============================] - 30s 27ms/step - g_loss: 0.7666 - d_loss: 0.6819\n",
            "Epoch 12/20\n",
            "1094/1094 [==============================] - 31s 28ms/step - g_loss: 0.7707 - d_loss: 0.6795\n",
            "Epoch 13/20\n",
            "1094/1094 [==============================] - 30s 27ms/step - g_loss: 0.7620 - d_loss: 0.6815\n",
            "Epoch 14/20\n",
            "1094/1094 [==============================] - 30s 27ms/step - g_loss: 0.7557 - d_loss: 0.6795\n",
            "Epoch 15/20\n",
            "1094/1094 [==============================] - 30s 27ms/step - g_loss: 0.7647 - d_loss: 0.6772\n",
            "Epoch 16/20\n",
            "1094/1094 [==============================] - 30s 28ms/step - g_loss: 0.7560 - d_loss: 0.6728\n",
            "Epoch 17/20\n",
            "1094/1094 [==============================] - 30s 27ms/step - g_loss: 0.7693 - d_loss: 0.6701\n",
            "Epoch 18/20\n",
            "1094/1094 [==============================] - 30s 27ms/step - g_loss: 0.7772 - d_loss: 0.6670\n",
            "Epoch 19/20\n",
            "1094/1094 [==============================] - 30s 27ms/step - g_loss: 0.7795 - d_loss: 0.6667\n",
            "Epoch 20/20\n",
            "1094/1094 [==============================] - 30s 27ms/step - g_loss: 0.7967 - d_loss: 0.6570\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79b9bc4a7100>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpolating between classes with the trained generator"
      ],
      "metadata": {
        "id": "NX8_SvWOVfer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We first extract the trained generator from our Conditional GAN.\n",
        "trained_gen = cond_gan.generator\n",
        "\n",
        "# Choose the number of intermediate images that would be generated in\n",
        "# between the interpolation + 2 (start and last images).\n",
        "num_interpolation = 9  # @param {type:\"integer\"}\n",
        "\n",
        "# Sample noise for the interpolation.\n",
        "interpolation_noise = tf.random.normal(shape=(1, latent_dim))\n",
        "interpolation_noise = tf.repeat(interpolation_noise, repeats=num_interpolation)\n",
        "interpolation_noise = tf.reshape(interpolation_noise, (num_interpolation, latent_dim))\n",
        "\n",
        "\n",
        "def interpolate_class(first_number, second_number):\n",
        "    # Convert the start and end labels to one-hot encoded vectors.\n",
        "    first_label = keras.utils.to_categorical([first_number], num_classes)\n",
        "    second_label = keras.utils.to_categorical([second_number], num_classes)\n",
        "    first_label = tf.cast(first_label, tf.float32)\n",
        "    second_label = tf.cast(second_label, tf.float32)\n",
        "\n",
        "    # Calculate the interpolation vector between the two labels.\n",
        "    percent_second_label = tf.linspace(0, 1, num_interpolation)[:, None]\n",
        "    percent_second_label = tf.cast(percent_second_label, tf.float32)\n",
        "    interpolation_labels = (\n",
        "        first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
        "    )\n",
        "\n",
        "    # Combine the noise and the labels and run inference with the generator.\n",
        "    noise_and_labels = tf.concat([interpolation_noise, interpolation_labels], 1)\n",
        "    fake = trained_gen.predict(noise_and_labels)\n",
        "    return fake\n",
        "\n",
        "\n",
        "start_class = 1  # @param {type:\"slider\", min:0, max:9, step:1}\n",
        "end_class = 7  # @param {type:\"slider\", min:0, max:9, step:1}\n",
        "\n",
        "fake_images = interpolate_class(start_class, end_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdnoRuIpNuyd",
        "outputId": "a2e7282f-7327-42ba-d7df-cd936b242a72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 412ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we first sample noise from a normal distribution and then we repeat that for\n",
        "`num_interpolation` times and reshape the result accordingly.\n",
        "We then distribute it uniformly for `num_interpolation`\n",
        "with the label identities being present in some proportion."
      ],
      "metadata": {
        "id": "sz0eXL2EVoLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fake_images *= 255.0\n",
        "converted_images = fake_images.astype(np.uint8)\n",
        "converted_images = tf.image.resize(converted_images, (96, 96)).numpy().astype(np.uint8)\n",
        "imageio.mimsave(\"animation.gif\", converted_images, fps=1)\n",
        "embed.embed_file(\"animation.gif\")"
      ],
      "metadata": {
        "id": "qkbkTVnOPIp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can further improve the performance of this model with recipes like\n",
        "[WGAN-GP](https://keras.io/examples/generative/wgan_gp).\n",
        "Conditional generation is also widely used in many modern image generation architectures like\n",
        "[VQ-GANs](https://arxiv.org/abs/2012.09841), [DALL-E](https://openai.com/blog/dall-e/),\n",
        "etc.\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/conditional-gan) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/conditional-GAN)."
      ],
      "metadata": {
        "id": "kmQxkKD-Vp9Y"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPNkQ2xArKyH7iVL1kOPMV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}