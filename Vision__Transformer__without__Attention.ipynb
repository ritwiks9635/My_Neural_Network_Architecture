{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwiks9635/My_Neural_Network_Architecture/blob/main/Vision__Transformer__without__Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A Vision Transformer without Attention**"
      ],
      "metadata": {
        "id": "yMskKbobT1TM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Introduction***\n",
        "\n",
        "Vision Transformers (ViTs) have sparked a wave of research at the intersection of Transformers and Computer Vision (CV).\n",
        "\n",
        "ViTs can simultaneously model long- and short-range dependencies, thanks to the Multi-Head Self-Attention mechanism in the Transformer block. Many researchers believe that the success of ViTs are purely due to the attention layer, and they seldom think about other parts of the ViT model.\n",
        "\n",
        "In the academic paper When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism the authors propose to demystify the success of ViTs with the introduction of a NO PARAMETER operation in place of the attention operation. They swap the attention operation with a shifting operation.\n",
        "\n",
        "In this example, we minimally implement the paper with close alignement to the author's official implementation."
      ],
      "metadata": {
        "id": "1QyY8tYhBjTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMp7nwmHTqeC",
        "outputId": "0a65b475-a259-4c7a-f8d1-4486b7cc46fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.3/612.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.22.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z5tUWSdKVNLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import pathlib\n",
        "import glob\n",
        "\n",
        "seed = 42\n",
        "keras.utils.set_random_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygjkfmBEVOnz",
        "outputId": "ea0586ee-c3f4-4200-b0b0-6cf0ebbf844b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "    # DATA\n",
        "    batch_size = 256\n",
        "    buffer_size = batch_size * 2\n",
        "    input_shape = (32, 32, 3)\n",
        "    num_classes = 10\n",
        "\n",
        "    # AUGMENTATION\n",
        "    image_size = 48\n",
        "\n",
        "    # ARCHITECTURE\n",
        "    patch_size = 4\n",
        "    projected_dim = 96\n",
        "    num_shift_blocks_per_stages = [2, 4, 8, 2]\n",
        "    epsilon = 1e-5\n",
        "    stochastic_depth_rate = 0.2\n",
        "    mlp_dropout_rate = 0.2\n",
        "    num_div = 12\n",
        "    shift_pixel = 1\n",
        "    mlp_expand_ratio = 2\n",
        "\n",
        "    # OPTIMIZER\n",
        "    lr_start = 1e-5\n",
        "    lr_max = 1e-3\n",
        "    weight_decay = 1e-4\n",
        "\n",
        "    # TRAINING\n",
        "    epochs = 100\n",
        "\n",
        "    # INFERENCE\n",
        "    label_map = {\n",
        "        0: \"airplane\",\n",
        "        1: \"automobile\",\n",
        "        2: \"bird\",\n",
        "        3: \"cat\",\n",
        "        4: \"deer\",\n",
        "        5: \"dog\",\n",
        "        6: \"frog\",\n",
        "        7: \"horse\",\n",
        "        8: \"ship\",\n",
        "        9: \"truck\",\n",
        "    }\n",
        "    tf_ds_batch_size = 20\n",
        "\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "feBUFQJBV32o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = ((x_train[:40000], y_train[:40000]),(x_train[40000:], y_train[40000:]))\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_ds = train_ds.shuffle(config.buffer_size).batch(config.batch_size).prefetch(AUTO)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_ds = val_ds.batch(config.batch_size).prefetch(AUTO)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_ds = test_ds.batch(config.batch_size).prefetch(AUTO)"
      ],
      "metadata": {
        "id": "nv40QtfPWYqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "170ca2fb-dd1c-4396-f2b9-e3b591c02af2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZCiCQ2na31T",
        "outputId": "1e2a52f1-853d-4351-ed26-5cf06c47140c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Data Augmentation***\n",
        "\n",
        "The augmentation pipeline consists of:\n",
        "Rescaling\n",
        "\n",
        "Resizing\n",
        "\n",
        "Random cropping\n",
        "\n",
        "Random horizontal flipping\n",
        "\n",
        "Note: The image data augmentation layers do not apply data transformations at inference time. This means that when these layers are called with training=False they behave differently. Refer to the documentation for more details."
      ],
      "metadata": {
        "id": "G2pNRMwJBW5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_augmention_model():\n",
        "  data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Resizing(config.input_shape[0] + 20, config.input_shape[0] + 20),\n",
        "        layers.RandomCrop(config.image_size, config.image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.Rescaling(1.0/255.0)\n",
        "    ])\n",
        "  return data_augmentation"
      ],
      "metadata": {
        "id": "TaAqcbk5bh4o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The ShiftViT architecture***\n",
        "\n",
        "In this section, we build the architecture proposed in the ShiftViT paper.\n",
        "\n",
        "Figure 1: The entire architecutre of ShiftViT.Source\n",
        "\n",
        "The architecture as shown in Fig. 1, is inspired by Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. Here the authors propose a modular architecture with 4 stages. Each stage works on its own spatial size, creating a hierarchical architecture.\n",
        "\n",
        "An input image of size HxWx3 is split into non-overlapping patches of size 4x4. This is done via the patchify layer which results in individual tokens of feature size 48 (4x4x3). Each stage comprises two parts:\n",
        "\n",
        "Embedding Generation\n",
        "Stacked Shift Blocks\n",
        "We discuss the stages and the modules in detail in what follows.\n",
        "\n",
        "Note: Compared to the official implementation we restructure some key components to better fit the Keras API.\n",
        "\n",
        "***The ShiftViT Block***\n",
        "\n",
        "Figure 2: From the Model to a Shift Block.\n",
        "Each stage in the ShiftViT architecture comprises of a Shift Block as shown in Fig 2.\n",
        "\n",
        "Figure 3: The Shift ViT Block. Source\n",
        "The Shift Block as shown in Fig. 3, comprises of the following:\n",
        "\n",
        "Shift Operation\n",
        "\n",
        "Linear Normalization\n",
        "\n",
        "MLP Layer"
      ],
      "metadata": {
        "id": "kPcQFkX7A3hW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(layers.Layer):\n",
        "    \"\"\"Get the MLP layer for each shift block.\n",
        "\n",
        "    Args:\n",
        "        mlp_expand_ratio (int): The ratio with which the first feature map is expanded.\n",
        "        mlp_dropout_rate (float): The rate for dropout.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mlp_expand_ratio, mlp_dropout_rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mlp_expand_ratio = mlp_expand_ratio\n",
        "        self.mlp_dropout_rate = mlp_dropout_rate\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_channels = input_shape[-1]\n",
        "        initial_filters = int(self.mlp_expand_ratio * input_channels)\n",
        "\n",
        "        self.mlp = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(\n",
        "                    units=initial_filters,\n",
        "                    activation=tf.nn.gelu,\n",
        "                ),\n",
        "                layers.Dropout(rate=self.mlp_dropout_rate),\n",
        "                layers.Dense(units=input_channels),\n",
        "                layers.Dropout(rate=self.mlp_dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "caNo5tmOdQvJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The DropPath layer***\n",
        "\n",
        "Stochastic depth is a regularization technique that randomly drops a set of layers. During inference, the layers are kept as they are. It is very similar to Dropout, but it operates on a block of layers rather than on individual nodes present inside a layer."
      ],
      "metadata": {
        "id": "tVHitykPAp8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DropPath(layers.Layer):\n",
        "    \"\"\"Drop Path also known as the Stochastic Depth layer.\n",
        "\n",
        "    Refernece:\n",
        "        - https://keras.io/examples/vision/cct/#stochastic-depth-for-regularization\n",
        "        - github.com:rwightman/pytorch-image-models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, drop_path_prob, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.drop_path_prob = drop_path_prob\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        if training:\n",
        "            keep_prob = 1 - self.drop_path_prob\n",
        "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
        "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
        "            random_tensor = tf.floor(random_tensor)\n",
        "            return (x / keep_prob) * random_tensor\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ta3oCkfLg_GN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Block***\n",
        "\n",
        "The most important operation in this paper is the shift operation. In this section, we describe the shift operation and compare it with its original implementation provided by the authors.\n",
        "\n",
        "A generic feature map is assumed to have the shape [N, H, W, C]. Here we choose a num_div parameter that decides the division size of the channels. The first 4 divisions are shifted (1 pixel) in the left, right, up, and down direction. The remaining splits are kept as is. After partial shifting the shifted channels are padded and the overflown pixels are chopped off. This completes the partial shifting operation.\n",
        "\n",
        "In the original implementation, the code is approximately:\n",
        "\n",
        "out[:, g * 0:g * 1, :, :-1] = x[:, g * 0:g * 1, :, 1:]  # shift left\n",
        "out[:, g * 1:g * 2, :, 1:] = x[:, g * 1:g * 2, :, :-1]  # shift right\n",
        "out[:, g * 2:g * 3, :-1, :] = x[:, g * 2:g * 3, 1:, :]  # shift up\n",
        "out[:, g * 3:g * 4, 1:, :] = x[:, g * 3:g * 4, :-1, :]  # shift down\n",
        "\n",
        "out[:, g * 4:, :, :] = x[:, g * 4:, :, :]  # no shift\n",
        "In TensorFlow it would be infeasible for us to assign shifted channels to a tensor in the middle of the training process. This is why we have resorted to the following procedure:\n",
        "\n",
        "Split the channels with the num_div parameter.\n",
        "Select each of the first four spilts and shift and pad them in the respective directions.\n",
        "After shifting and padding, we concatenate the channel back.\n",
        "Figure 4: The TensorFlow style shifting\n",
        "The entire procedure is explained in the Fig. 4."
      ],
      "metadata": {
        "id": "YeM7VnplAcSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShiftViTBlock(layers.Layer):\n",
        "    \"\"\"A unit ShiftViT Block\n",
        "\n",
        "    Args:\n",
        "        shift_pixel (int): The number of pixels to shift. Default to 1.\n",
        "        mlp_expand_ratio (int): The ratio with which MLP features are\n",
        "            expanded. Default to 2.\n",
        "        mlp_dropout_rate (float): The dropout rate used in MLP.\n",
        "        num_div (int): The number of divisions of the feature map's channel.\n",
        "            Totally, 4/num_div of channels will be shifted. Defaults to 12.\n",
        "        epsilon (float): Epsilon constant.\n",
        "        drop_path_prob (float): The drop probability for drop path.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        epsilon,\n",
        "        drop_path_prob,\n",
        "        mlp_dropout_rate,\n",
        "        num_div=12,\n",
        "        shift_pixel=1,\n",
        "        mlp_expand_ratio=2,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.shift_pixel = shift_pixel\n",
        "        self.mlp_expand_ratio = mlp_expand_ratio\n",
        "        self.mlp_dropout_rate = mlp_dropout_rate\n",
        "        self.num_div = num_div\n",
        "        self.epsilon = epsilon\n",
        "        self.drop_path_prob = drop_path_prob\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.H = input_shape[1]\n",
        "        self.W = input_shape[2]\n",
        "        self.C = input_shape[3]\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=self.epsilon)\n",
        "        self.drop_path = (\n",
        "            DropPath(drop_path_prob=self.drop_path_prob)\n",
        "            if self.drop_path_prob > 0.0\n",
        "            else layers.Activation(\"linear\")\n",
        "        )\n",
        "        self.mlp = MLP(\n",
        "            mlp_expand_ratio=self.mlp_expand_ratio,\n",
        "            mlp_dropout_rate=self.mlp_dropout_rate,\n",
        "        )\n",
        "\n",
        "    def get_shift_pad(self, x, mode):\n",
        "        \"\"\"Shifts the channels according to the mode chosen.\"\"\"\n",
        "        if mode == \"left\":\n",
        "            offset_height = 0\n",
        "            offset_width = 0\n",
        "            target_height = 0\n",
        "            target_width = self.shift_pixel\n",
        "        elif mode == \"right\":\n",
        "            offset_height = 0\n",
        "            offset_width = self.shift_pixel\n",
        "            target_height = 0\n",
        "            target_width = self.shift_pixel\n",
        "        elif mode == \"up\":\n",
        "            offset_height = 0\n",
        "            offset_width = 0\n",
        "            target_height = self.shift_pixel\n",
        "            target_width = 0\n",
        "        else:\n",
        "            offset_height = self.shift_pixel\n",
        "            offset_width = 0\n",
        "            target_height = self.shift_pixel\n",
        "            target_width = 0\n",
        "        crop = tf.image.crop_to_bounding_box(\n",
        "            x,\n",
        "            offset_height=offset_height,\n",
        "            offset_width=offset_width,\n",
        "            target_height=self.H - target_height,\n",
        "            target_width=self.W - target_width,\n",
        "        )\n",
        "        shift_pad = tf.image.pad_to_bounding_box(\n",
        "            crop,\n",
        "            offset_height=offset_height,\n",
        "            offset_width=offset_width,\n",
        "            target_height=self.H,\n",
        "            target_width=self.W,\n",
        "        )\n",
        "        return shift_pad\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        # Split the feature maps\n",
        "        x_splits = tf.split(x, num_or_size_splits=self.C // self.num_div, axis=-1)\n",
        "\n",
        "        # Shift the feature maps\n",
        "        x_splits[0] = self.get_shift_pad(x_splits[0], mode=\"left\")\n",
        "        x_splits[1] = self.get_shift_pad(x_splits[1], mode=\"right\")\n",
        "        x_splits[2] = self.get_shift_pad(x_splits[2], mode=\"up\")\n",
        "        x_splits[3] = self.get_shift_pad(x_splits[3], mode=\"down\")\n",
        "\n",
        "        # Concatenate the shifted and unshifted feature maps\n",
        "        x = tf.concat(x_splits, axis=-1)\n",
        "\n",
        "        # Add the residual connection\n",
        "        shortcut = x\n",
        "        x = shortcut + self.drop_path(self.mlp(self.layer_norm(x)), training=training)\n",
        "        return x"
      ],
      "metadata": {
        "id": "frFtLoMRhKzh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The ShiftViT blocks***\n",
        "\n",
        "Figure 5: Shift Blocks in the architecture. Source\n",
        "Each stage of the architecture has shift blocks as shown in Fig.5. Each of these blocks contain a variable number of stacked ShiftViT block (as built in the earlier section).\n",
        "Shift blocks are followed by a PatchMerging layer that scales down feature inputs. The PatchMerging layer helps in the pyramidal structure of the model."
      ],
      "metadata": {
        "id": "3YXs6BvgANYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The PatchMerging layer***\n",
        "\n",
        "This layer merges the two adjacent tokens. This layer helps in scaling the features down spatially and increasing the features up channel wise. We use a Conv2D layer to merge the patches"
      ],
      "metadata": {
        "id": "xCmdpcwXACsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging(layers.Layer):\n",
        "    \"\"\"The Patch Merging layer.\n",
        "\n",
        "    Args:\n",
        "        epsilon (float): The epsilon constant.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, epsilon, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        filters = 2 * input_shape[-1]\n",
        "        self.reduction = layers.Conv2D(\n",
        "            filters=filters, kernel_size=2, strides=2, padding=\"same\", use_bias=False\n",
        "        )\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=self.epsilon)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Apply the patch merging algorithm on the feature maps\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.reduction(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YSNfm8HchuK-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Stacked Shift Blocks***\n",
        "\n",
        "Each stage will have a variable number of stacked ShiftViT Blocks, as suggested in the paper. This is a generic layer that will contain the stacked shift vit blocks with the patch merging layer as well. Combining the two operations (shift ViT block and patch merging) is a design choice we picked for better code reusability."
      ],
      "metadata": {
        "id": "GJmIwjlZ_6gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: This layer will have a different depth of stacking\n",
        "# for different stages on the model.\n",
        "class StackedShiftBlocks(layers.Layer):\n",
        "    \"\"\"The layer containing stacked ShiftViTBlocks.\n",
        "\n",
        "    Args:\n",
        "        epsilon (float): The epsilon constant.\n",
        "        mlp_dropout_rate (float): The dropout rate used in the MLP block.\n",
        "        num_shift_blocks (int): The number of shift vit blocks for this stage.\n",
        "        stochastic_depth_rate (float): The maximum drop path rate chosen.\n",
        "        is_merge (boolean): A flag that determines the use of the Patch Merge\n",
        "            layer after the shift vit blocks.\n",
        "        num_div (int): The division of channels of the feature map. Defaults to 12.\n",
        "        shift_pixel (int): The number of pixels to shift. Defaults to 1.\n",
        "        mlp_expand_ratio (int): The ratio with which the initial dense layer of\n",
        "            the MLP is expanded Defaults to 2.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        epsilon,\n",
        "        mlp_dropout_rate,\n",
        "        num_shift_blocks,\n",
        "        stochastic_depth_rate,\n",
        "        is_merge,\n",
        "        num_div=12,\n",
        "        shift_pixel=1,\n",
        "        mlp_expand_ratio=2,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.epsilon = epsilon\n",
        "        self.mlp_dropout_rate = mlp_dropout_rate\n",
        "        self.num_shift_blocks = num_shift_blocks\n",
        "        self.stochastic_depth_rate = stochastic_depth_rate\n",
        "        self.is_merge = is_merge\n",
        "        self.num_div = num_div\n",
        "        self.shift_pixel = shift_pixel\n",
        "        self.mlp_expand_ratio = mlp_expand_ratio\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        # Calculate stochastic depth probabilities.\n",
        "        # Reference: https://keras.io/examples/vision/cct/#the-final-cct-model\n",
        "        dpr = [\n",
        "            x\n",
        "            for x in np.linspace(\n",
        "                start=0, stop=self.stochastic_depth_rate, num=self.num_shift_blocks\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Build the shift blocks as a list of ShiftViT Blocks\n",
        "        self.shift_blocks = list()\n",
        "        for num in range(self.num_shift_blocks):\n",
        "            self.shift_blocks.append(\n",
        "                ShiftViTBlock(\n",
        "                    num_div=self.num_div,\n",
        "                    epsilon=self.epsilon,\n",
        "                    drop_path_prob=dpr[num],\n",
        "                    mlp_dropout_rate=self.mlp_dropout_rate,\n",
        "                    shift_pixel=self.shift_pixel,\n",
        "                    mlp_expand_ratio=self.mlp_expand_ratio,\n",
        "                )\n",
        "            )\n",
        "        if self.is_merge:\n",
        "            self.patch_merge = PatchMerging(epsilon=self.epsilon)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        for shift_block in self.shift_blocks:\n",
        "            x = shift_block(x, training=training)\n",
        "        if self.is_merge:\n",
        "            x = self.patch_merge(x)\n",
        "        return x\n",
        "\n",
        "    # Since this is a custom layer, we need to overwrite get_config()\n",
        "    # so that model can be easily saved & loaded after training\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"epsilon\": self.epsilon,\n",
        "                \"mlp_dropout_rate\": self.mlp_dropout_rate,\n",
        "                \"num_shift_blocks\": self.num_shift_blocks,\n",
        "                \"stochastic_depth_rate\": self.stochastic_depth_rate,\n",
        "                \"is_merge\": self.is_merge,\n",
        "                \"num_div\": self.num_div,\n",
        "                \"shift_pixel\": self.shift_pixel,\n",
        "                \"mlp_expand_ratio\": self.mlp_expand_ratio,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ],
      "metadata": {
        "id": "HQeFP-SxsWhp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The ShiftViT model***\n",
        "\n",
        "Build the ShiftViT custom model."
      ],
      "metadata": {
        "id": "gh9DmGtU_yrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShiftViTModel(keras.Model):\n",
        "    \"\"\"The ShiftViT Model.\n",
        "\n",
        "    Args:\n",
        "        data_augmentation (keras.Model): A data augmentation model.\n",
        "        projected_dim (int): The dimension to which the patches of the image are\n",
        "            projected.\n",
        "        patch_size (int): The patch size of the images.\n",
        "        num_shift_blocks_per_stages (list[int]): A list of all the number of shit\n",
        "            blocks per stage.\n",
        "        epsilon (float): The epsilon constant.\n",
        "        mlp_dropout_rate (float): The dropout rate used in the MLP block.\n",
        "        stochastic_depth_rate (float): The maximum drop rate probability.\n",
        "        num_div (int): The number of divisions of the channesl of the feature\n",
        "            map. Defaults to 12.\n",
        "        shift_pixel (int): The number of pixel to shift. Default to 1.\n",
        "        mlp_expand_ratio (int): The ratio with which the initial mlp dense layer\n",
        "            is expanded to. Defaults to 2.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_augmentation,\n",
        "        projected_dim,\n",
        "        patch_size,\n",
        "        num_shift_blocks_per_stages,\n",
        "        epsilon,\n",
        "        mlp_dropout_rate,\n",
        "        stochastic_depth_rate,\n",
        "        num_div=12,\n",
        "        shift_pixel=1,\n",
        "        mlp_expand_ratio=2,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.data_augmentation = data_augmentation\n",
        "        self.patch_projection = layers.Conv2D(\n",
        "            filters=projected_dim,\n",
        "            kernel_size=patch_size,\n",
        "            strides=patch_size,\n",
        "            padding=\"same\",\n",
        "        )\n",
        "        self.stages = list()\n",
        "        for index, num_shift_blocks in enumerate(num_shift_blocks_per_stages):\n",
        "            if index == len(num_shift_blocks_per_stages) - 1:\n",
        "                # This is the last stage, do not use the patch merge here.\n",
        "                is_merge = False\n",
        "            else:\n",
        "                is_merge = True\n",
        "            # Build the stages.\n",
        "            self.stages.append(\n",
        "                StackedShiftBlocks(\n",
        "                    epsilon=epsilon,\n",
        "                    mlp_dropout_rate=mlp_dropout_rate,\n",
        "                    num_shift_blocks=num_shift_blocks,\n",
        "                    stochastic_depth_rate=stochastic_depth_rate,\n",
        "                    is_merge=is_merge,\n",
        "                    num_div=num_div,\n",
        "                    shift_pixel=shift_pixel,\n",
        "                    mlp_expand_ratio=mlp_expand_ratio,\n",
        "                )\n",
        "            )\n",
        "        self.global_avg_pool = layers.GlobalAveragePooling2D()\n",
        "\n",
        "        self.classifier = layers.Dense(config.num_classes)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"data_augmentation\": self.data_augmentation,\n",
        "                \"patch_projection\": self.patch_projection,\n",
        "                \"stages\": self.stages,\n",
        "                \"global_avg_pool\": self.global_avg_pool,\n",
        "                \"classifier\": self.classifier,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "    def _calculate_loss(self, data, training=False):\n",
        "        (images, labels) = data\n",
        "\n",
        "        # Augment the images\n",
        "        augmented_images = self.data_augmentation(images, training=training)\n",
        "\n",
        "        # Create patches and project the pathces.\n",
        "        projected_patches = self.patch_projection(augmented_images)\n",
        "\n",
        "        # Pass through the stages\n",
        "        x = projected_patches\n",
        "        for stage in self.stages:\n",
        "            x = stage(x, training=training)\n",
        "\n",
        "        # Get the logits.\n",
        "        x = self.global_avg_pool(x)\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        # Calculate the loss and return it.\n",
        "        total_loss = self.compiled_loss(labels, logits)\n",
        "        return total_loss, labels, logits\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            total_loss, labels, logits = self._calculate_loss(\n",
        "                data=inputs, training=True\n",
        "            )\n",
        "\n",
        "        # Apply gradients.\n",
        "        train_vars = [\n",
        "            self.data_augmentation.trainable_variables,\n",
        "            self.patch_projection.trainable_variables,\n",
        "            self.global_avg_pool.trainable_variables,\n",
        "            self.classifier.trainable_variables,\n",
        "        ]\n",
        "        train_vars = train_vars + [stage.trainable_variables for stage in self.stages]\n",
        "\n",
        "        # Optimize the gradients.\n",
        "        grads = tape.gradient(total_loss, train_vars)\n",
        "        trainable_variable_list = []\n",
        "        for (grad, var) in zip(grads, train_vars):\n",
        "            for g, v in zip(grad, var):\n",
        "                trainable_variable_list.append((g, v))\n",
        "        self.optimizer.apply_gradients(trainable_variable_list)\n",
        "\n",
        "        # Update the metrics\n",
        "        self.compiled_metrics.update_state(labels, logits)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        _, labels, logits = self._calculate_loss(data=data, training=False)\n",
        "\n",
        "        # Update the metrics\n",
        "        self.compiled_metrics.update_state(labels, logits)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def call(self, images):\n",
        "        augmented_images = self.data_augmentation(images)\n",
        "        x = self.patch_projection(augmented_images)\n",
        "        for stage in self.stages:\n",
        "            x = stage(x, training=False)\n",
        "        x = self.global_avg_pool(x)\n",
        "        logits = self.classifier(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "P8zbX2SSs3tc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Instantiate the model***"
      ],
      "metadata": {
        "id": "ZDttlse6_rAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ShiftViTModel(\n",
        "    data_augmentation=get_augmention_model(),\n",
        "    projected_dim=config.projected_dim,\n",
        "    patch_size=config.patch_size,\n",
        "    num_shift_blocks_per_stages=config.num_shift_blocks_per_stages,\n",
        "    epsilon=config.epsilon,\n",
        "    mlp_dropout_rate=config.mlp_dropout_rate,\n",
        "    stochastic_depth_rate=config.stochastic_depth_rate,\n",
        "    num_div=config.num_div,\n",
        "    shift_pixel=config.shift_pixel,\n",
        "    mlp_expand_ratio=config.mlp_expand_ratio,\n",
        ")"
      ],
      "metadata": {
        "id": "B6O2eCWrtGz_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Learning rate schedule***\n",
        "\n",
        "In many experiments, we want to warm up the model with a slowly increasing learning rate and then cool down the model with a slowly decaying learning rate. In the warmup cosine decay, the learning rate linearly increases for the warmup steps and then decays with a cosine decay."
      ],
      "metadata": {
        "id": "oW_dwR0C_iF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some code is taken from:\n",
        "# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2.\n",
        "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\"A LearningRateSchedule that uses a warmup cosine decay schedule.\"\"\"\n",
        "\n",
        "    def __init__(self, lr_start, lr_max, warmup_steps, total_steps):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            lr_start: The initial learning rate\n",
        "            lr_max: The maximum learning rate to which lr should increase to in\n",
        "                the warmup steps\n",
        "            warmup_steps: The number of steps for which the model warms up\n",
        "            total_steps: The total number of steps for the model training\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.lr_start = lr_start\n",
        "        self.lr_max = lr_max\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.total_steps = total_steps\n",
        "        self.pi = tf.constant(np.pi)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        # Check whether the total number of steps is larger than the warmup\n",
        "        # steps. If not, then throw a value error.\n",
        "        if self.total_steps < self.warmup_steps:\n",
        "            raise ValueError(\n",
        "                f\"Total number of steps {self.total_steps} must be\"\n",
        "                + f\"larger or equal to warmup steps {self.warmup_steps}.\"\n",
        "            )\n",
        "\n",
        "        # `cos_annealed_lr` is a graph that increases to 1 from the initial\n",
        "        # step to the warmup step. After that this graph decays to -1 at the\n",
        "        # final step mark.\n",
        "        cos_annealed_lr = tf.cos(\n",
        "            self.pi\n",
        "            * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
        "            / tf.cast(self.total_steps - self.warmup_steps, tf.float32)\n",
        "        )\n",
        "\n",
        "        # Shift the mean of the `cos_annealed_lr` graph to 1. Now the grpah goes\n",
        "        # from 0 to 2. Normalize the graph with 0.5 so that now it goes from 0\n",
        "        # to 1. With the normalized graph we scale it with `lr_max` such that\n",
        "        # it goes from 0 to `lr_max`\n",
        "        learning_rate = 0.5 * self.lr_max * (1 + cos_annealed_lr)\n",
        "\n",
        "        # Check whether warmup_steps is more than 0.\n",
        "        if self.warmup_steps > 0:\n",
        "            # Check whether lr_max is larger that lr_start. If not, throw a value\n",
        "            # error.\n",
        "            if self.lr_max < self.lr_start:\n",
        "                raise ValueError(\n",
        "                    f\"lr_start {self.lr_start} must be smaller or\"\n",
        "                    + f\"equal to lr_max {self.lr_max}.\"\n",
        "                )\n",
        "\n",
        "            # Calculate the slope with which the learning rate should increase\n",
        "            # in the warumup schedule. The formula for slope is m = ((b-a)/steps)\n",
        "            slope = (self.lr_max - self.lr_start) / self.warmup_steps\n",
        "\n",
        "            # With the formula for a straight line (y = mx+c) build the warmup\n",
        "            # schedule\n",
        "            warmup_rate = slope * tf.cast(step, tf.float32) + self.lr_start\n",
        "\n",
        "            # When the current step is lesser that warmup steps, get the line\n",
        "            # graph. When the current step is greater than the warmup steps, get\n",
        "            # the scaled cos graph.\n",
        "            learning_rate = tf.where(\n",
        "                step < self.warmup_steps, warmup_rate, learning_rate\n",
        "            )\n",
        "\n",
        "        # When the current step is more that the total steps, return 0 else return\n",
        "        # the calculated graph.\n",
        "        return tf.where(\n",
        "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
        "        )\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            \"lr_start\": self.lr_start,\n",
        "            \"lr_max\": self.lr_max,\n",
        "            \"total_steps\": self.total_steps,\n",
        "            \"warmup_steps\": self.warmup_steps,\n",
        "        }\n",
        "        return config"
      ],
      "metadata": {
        "id": "_vX-BUVetdhQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# pass sample data to the model so that input shape is available at the time of\n",
        "# saving the model\n",
        "sample_ds, _ = next(iter(train_ds))\n",
        "model(sample_ds, training=False)\n",
        "\n",
        "# Get the total number of steps for training.\n",
        "total_steps = int((len(x_train) / config.batch_size) * config.epochs)\n",
        "\n",
        "# Calculate the number of steps for warmup.\n",
        "warmup_epoch_percentage = 0.15\n",
        "warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
        "\n",
        "# Initialize the warmupcosine schedule.\n",
        "scheduled_lrs = WarmUpCosine(\n",
        "    lr_start=1e-5,\n",
        "    lr_max=1e-3,\n",
        "    warmup_steps=warmup_steps,\n",
        "    total_steps=total_steps,\n",
        ")\n",
        "\n",
        "# Get the optimizer.\n",
        "optimizer = tfa.optimizers.AdamW(\n",
        "    learning_rate=scheduled_lrs, weight_decay=config.weight_decay\n",
        ")\n",
        "\n",
        "# Compile and pretrain the model.\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\n",
        "        keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "        keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=config.epochs,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_accuracy\",\n",
        "            patience=5,\n",
        "            mode=\"auto\",\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Evaluate the model with the test dataset.\n",
        "print(\"TESTING\")\n",
        "loss, acc_top1, acc_top5 = model.evaluate(test_ds)\n",
        "print(f\"Loss: {loss:0.2f}\")\n",
        "print(f\"Top 1 test accuracy: {acc_top1*100:0.2f}%\")\n",
        "print(f\"Top 5 test accuracy: {acc_top5*100:0.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDI0c64Et8xU",
        "outputId": "abb06991-65a1-4006-f50f-472d446852fd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "157/157 [==============================] - 52s 190ms/step - loss: 2.3725 - accuracy: 0.1493 - top-5-accuracy: 0.6082 - val_loss: 2.1262 - val_accuracy: 0.2647 - val_top-5-accuracy: 0.7758\n",
            "Epoch 2/100\n",
            "157/157 [==============================] - 27s 169ms/step - loss: 1.9447 - accuracy: 0.2919 - top-5-accuracy: 0.8089 - val_loss: 1.9171 - val_accuracy: 0.3461 - val_top-5-accuracy: 0.8727\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 26s 168ms/step - loss: 1.7015 - accuracy: 0.3846 - top-5-accuracy: 0.8764 - val_loss: 1.5880 - val_accuracy: 0.4434 - val_top-5-accuracy: 0.9021\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 26s 169ms/step - loss: 1.5642 - accuracy: 0.4336 - top-5-accuracy: 0.9057 - val_loss: 1.4331 - val_accuracy: 0.4861 - val_top-5-accuracy: 0.9242\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 27s 169ms/step - loss: 1.4580 - accuracy: 0.4733 - top-5-accuracy: 0.9201 - val_loss: 1.4172 - val_accuracy: 0.4932 - val_top-5-accuracy: 0.9332\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 27s 170ms/step - loss: 1.3759 - accuracy: 0.5061 - top-5-accuracy: 0.9319 - val_loss: 1.3523 - val_accuracy: 0.5273 - val_top-5-accuracy: 0.9375\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 27s 173ms/step - loss: 1.3097 - accuracy: 0.5310 - top-5-accuracy: 0.9401 - val_loss: 1.3015 - val_accuracy: 0.5399 - val_top-5-accuracy: 0.9425\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 27s 171ms/step - loss: 1.2536 - accuracy: 0.5510 - top-5-accuracy: 0.9456 - val_loss: 1.1906 - val_accuracy: 0.5841 - val_top-5-accuracy: 0.9526\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 27s 169ms/step - loss: 1.2037 - accuracy: 0.5675 - top-5-accuracy: 0.9513 - val_loss: 1.1903 - val_accuracy: 0.5781 - val_top-5-accuracy: 0.9511\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 27s 169ms/step - loss: 1.1610 - accuracy: 0.5855 - top-5-accuracy: 0.9561 - val_loss: 1.1892 - val_accuracy: 0.5870 - val_top-5-accuracy: 0.9494\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 27s 171ms/step - loss: 1.1205 - accuracy: 0.6026 - top-5-accuracy: 0.9569 - val_loss: 1.0872 - val_accuracy: 0.6117 - val_top-5-accuracy: 0.9622\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 27s 169ms/step - loss: 1.0754 - accuracy: 0.6179 - top-5-accuracy: 0.9617 - val_loss: 1.2408 - val_accuracy: 0.5756 - val_top-5-accuracy: 0.9426\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 27s 169ms/step - loss: 1.0642 - accuracy: 0.6232 - top-5-accuracy: 0.9641 - val_loss: 1.0573 - val_accuracy: 0.6245 - val_top-5-accuracy: 0.9641\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 27s 169ms/step - loss: 1.0102 - accuracy: 0.6417 - top-5-accuracy: 0.9668 - val_loss: 1.0307 - val_accuracy: 0.6412 - val_top-5-accuracy: 0.9652\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 27s 170ms/step - loss: 1.0006 - accuracy: 0.6469 - top-5-accuracy: 0.9681 - val_loss: 1.0143 - val_accuracy: 0.6513 - val_top-5-accuracy: 0.9658\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 27s 169ms/step - loss: 0.9614 - accuracy: 0.6609 - top-5-accuracy: 0.9709 - val_loss: 1.0200 - val_accuracy: 0.6330 - val_top-5-accuracy: 0.9674\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 27s 170ms/step - loss: 0.9226 - accuracy: 0.6752 - top-5-accuracy: 0.9732 - val_loss: 1.0271 - val_accuracy: 0.6498 - val_top-5-accuracy: 0.9620\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 27s 170ms/step - loss: 0.8742 - accuracy: 0.6913 - top-5-accuracy: 0.9771 - val_loss: 0.8965 - val_accuracy: 0.6906 - val_top-5-accuracy: 0.9738\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 27s 170ms/step - loss: 0.8425 - accuracy: 0.7034 - top-5-accuracy: 0.9778 - val_loss: 0.8448 - val_accuracy: 0.7050 - val_top-5-accuracy: 0.9755\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 27s 170ms/step - loss: 0.8222 - accuracy: 0.7106 - top-5-accuracy: 0.9801 - val_loss: 0.8198 - val_accuracy: 0.7161 - val_top-5-accuracy: 0.9795\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 27s 171ms/step - loss: 0.7845 - accuracy: 0.7211 - top-5-accuracy: 0.9816 - val_loss: 0.8789 - val_accuracy: 0.7022 - val_top-5-accuracy: 0.9724\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 27s 171ms/step - loss: 0.7540 - accuracy: 0.7343 - top-5-accuracy: 0.9837 - val_loss: 0.7877 - val_accuracy: 0.7289 - val_top-5-accuracy: 0.9797\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 27s 169ms/step - loss: 0.7432 - accuracy: 0.7375 - top-5-accuracy: 0.9841 - val_loss: 0.8022 - val_accuracy: 0.7200 - val_top-5-accuracy: 0.9793\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 27s 171ms/step - loss: 0.7284 - accuracy: 0.7449 - top-5-accuracy: 0.9858 - val_loss: 0.8412 - val_accuracy: 0.7115 - val_top-5-accuracy: 0.9778\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 27s 169ms/step - loss: 0.7123 - accuracy: 0.7469 - top-5-accuracy: 0.9861 - val_loss: 0.7986 - val_accuracy: 0.7249 - val_top-5-accuracy: 0.9790\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 27s 170ms/step - loss: 0.6821 - accuracy: 0.7597 - top-5-accuracy: 0.9869 - val_loss: 0.7693 - val_accuracy: 0.7421 - val_top-5-accuracy: 0.9814\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 27s 170ms/step - loss: 0.6589 - accuracy: 0.7697 - top-5-accuracy: 0.9881 - val_loss: 0.7579 - val_accuracy: 0.7419 - val_top-5-accuracy: 0.9835\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 27s 170ms/step - loss: 0.6441 - accuracy: 0.7744 - top-5-accuracy: 0.9898 - val_loss: 0.7735 - val_accuracy: 0.7383 - val_top-5-accuracy: 0.9797\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 27s 169ms/step - loss: 0.6401 - accuracy: 0.7743 - top-5-accuracy: 0.9885 - val_loss: 0.7671 - val_accuracy: 0.7452 - val_top-5-accuracy: 0.9798\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 27s 171ms/step - loss: 0.6185 - accuracy: 0.7823 - top-5-accuracy: 0.9901 - val_loss: 0.7633 - val_accuracy: 0.7416 - val_top-5-accuracy: 0.9802\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 27s 170ms/step - loss: 0.6044 - accuracy: 0.7860 - top-5-accuracy: 0.9898 - val_loss: 0.7273 - val_accuracy: 0.7581 - val_top-5-accuracy: 0.9814\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 27s 169ms/step - loss: 0.5819 - accuracy: 0.7940 - top-5-accuracy: 0.9915 - val_loss: 0.7320 - val_accuracy: 0.7526 - val_top-5-accuracy: 0.9822\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 27s 171ms/step - loss: 0.5742 - accuracy: 0.7964 - top-5-accuracy: 0.9919 - val_loss: 0.7307 - val_accuracy: 0.7587 - val_top-5-accuracy: 0.9821\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 27s 170ms/step - loss: 0.5554 - accuracy: 0.8039 - top-5-accuracy: 0.9926 - val_loss: 0.7108 - val_accuracy: 0.7682 - val_top-5-accuracy: 0.9836\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 27s 170ms/step - loss: 0.5419 - accuracy: 0.8072 - top-5-accuracy: 0.9928 - val_loss: 0.7343 - val_accuracy: 0.7599 - val_top-5-accuracy: 0.9847\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 27s 170ms/step - loss: 0.5191 - accuracy: 0.8174 - top-5-accuracy: 0.9937 - val_loss: 0.7459 - val_accuracy: 0.7568 - val_top-5-accuracy: 0.9814\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 27s 170ms/step - loss: 0.5086 - accuracy: 0.8192 - top-5-accuracy: 0.9940 - val_loss: 0.7416 - val_accuracy: 0.7530 - val_top-5-accuracy: 0.9850\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 27s 171ms/step - loss: 0.4993 - accuracy: 0.8232 - top-5-accuracy: 0.9941 - val_loss: 0.7157 - val_accuracy: 0.7671 - val_top-5-accuracy: 0.9829\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 27s 170ms/step - loss: 0.4752 - accuracy: 0.8306 - top-5-accuracy: 0.9952 - val_loss: 0.7147 - val_accuracy: 0.7649 - val_top-5-accuracy: 0.9848\n",
            "TESTING\n",
            "40/40 [==============================] - 2s 59ms/step - loss: 0.7410 - accuracy: 0.7567 - top-5-accuracy: 0.9828\n",
            "Loss: 0.74\n",
            "Top 1 test accuracy: 75.67%\n",
            "Top 5 test accuracy: 98.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Save trained model***\n",
        "\n",
        "Since we created the model by Subclassing, we can't save the model in HDF5 format.\n",
        "\n",
        "It can be saved in TF SavedModel format only. In general, this is the recommended format for saving models as well."
      ],
      "metadata": {
        "id": "pVnVuz112tgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"ShiftViT\")"
      ],
      "metadata": {
        "id": "2GUm8Z550Ag_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Model inference***\n",
        "\n",
        "**Download sample data for inference**"
      ],
      "metadata": {
        "id": "jnqMGcQE2ioX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q 'https://tinyurl.com/2p9483sw' -O inference_set.zip\n",
        "!unzip -q inference_set.zip"
      ],
      "metadata": {
        "id": "VOfO5Cy50FyU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom objects are not included when the model is saved.\n",
        "# At loading time, these objects need to be passed for reconstruction of the model\n",
        "saved_model = tf.keras.models.load_model(\n",
        "    \"ShiftViT\",\n",
        "    custom_objects={\"WarmUpCosine\": WarmUpCosine, \"AdamW\": tfa.optimizers.AdamW},\n",
        ")"
      ],
      "metadata": {
        "id": "DRlhGjuM1i-W"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Utility functions for inference***"
      ],
      "metadata": {
        "id": "_LGaM4c02UNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(img_path):\n",
        "    # read image file from string path\n",
        "    img = tf.io.read_file(img_path)\n",
        "\n",
        "    # decode jpeg to uint8 tensor\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "\n",
        "    # resize image to match input size accepted by model\n",
        "    # use `method` as `nearest` to preserve dtype of input passed to `resize()`\n",
        "    img = tf.image.resize(\n",
        "        img, [config.input_shape[0], config.input_shape[1]], method=\"nearest\"\n",
        "    )\n",
        "    return img\n",
        "\n",
        "\n",
        "def create_tf_dataset(image_dir):\n",
        "    data_dir = pathlib.Path(image_dir)\n",
        "\n",
        "    # create tf.data dataset using directory of images\n",
        "    predict_ds = tf.data.Dataset.list_files(str(data_dir / \"*.jpg\"), shuffle=False)\n",
        "\n",
        "    # use map to convert string paths to uint8 image tensors\n",
        "    # setting `num_parallel_calls' helps in processing multiple images parallely\n",
        "    predict_ds = predict_ds.map(process_image, num_parallel_calls=AUTO)\n",
        "\n",
        "    # create a Prefetch Dataset for better latency & throughput\n",
        "    predict_ds = predict_ds.batch(config.tf_ds_batch_size).prefetch(AUTO)\n",
        "    return predict_ds\n",
        "\n",
        "\n",
        "def predict(predict_ds):\n",
        "    # ShiftViT model returns logits (non-normalized predictions)\n",
        "    logits = saved_model.predict(predict_ds)\n",
        "\n",
        "    # normalize predictions by calling softmax()\n",
        "    probabilities = tf.nn.softmax(logits)\n",
        "    return probabilities\n",
        "\n",
        "\n",
        "def get_predicted_class(probabilities):\n",
        "    pred_label = np.argmax(probabilities)\n",
        "    predicted_class = config.label_map[pred_label]\n",
        "    return predicted_class\n",
        "\n",
        "\n",
        "def get_confidence_scores(probabilities):\n",
        "    # get the indices of the probability scores sorted in descending order\n",
        "    labels = np.argsort(probabilities)[::-1]\n",
        "    confidences = {\n",
        "        config.label_map[label]: np.round((probabilities[label]) * 100, 2)\n",
        "        for label in labels\n",
        "    }\n",
        "    return confidences"
      ],
      "metadata": {
        "id": "X37Fg6gH1mM-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir = \"inference_set\"\n",
        "predict_ds = create_tf_dataset(img_dir)\n",
        "probabilities = predict(predict_ds)\n",
        "print(f\"probabilities: {probabilities[0]}\")\n",
        "confidences = get_confidence_scores(probabilities[0])\n",
        "print(confidences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCaYekXu1wWg",
        "outputId": "c267db59-d1be-439e-da36-fcea3779f9e0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "probabilities: [9.9790812e-01 1.7290583e-06 8.1658807e-05 2.7093054e-05 1.5788730e-05\n",
            " 1.9483309e-06 7.6940869e-06 2.6148241e-06 1.9510021e-03 2.3278919e-06]\n",
            "{'airplane': 99.79, 'ship': 0.2, 'bird': 0.01, 'cat': 0.0, 'deer': 0.0, 'frog': 0.0, 'horse': 0.0, 'truck': 0.0, 'dog': 0.0, 'automobile': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images in predict_ds:\n",
        "    for i in range(min(6, probabilities.shape[0])):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        predicted_class = get_predicted_class(probabilities[i])\n",
        "        plt.title(predicted_class)\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "W8O7wbq814tj",
        "outputId": "10f3d649-d9a9-4ead-da2e-404644da756d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Conclusion***\n",
        "\n",
        "The most impactful contribution of the paper is not the novel architecture, but the idea that hierarchical ViTs trained with no attention can perform quite well. This opens up the question of how essential attention is to the performance of ViTs.\n",
        "\n",
        "For curious minds, we would suggest reading the ConvNexT paper which attends more to the training paradigms and architectural details of ViTs rather than providing a novel architecture based on attention.\n",
        "\n",
        "Acknowledgements:\n",
        "\n",
        "We would like to thank PyImageSearch for providing us with resources that helped in the completion of this project.\n",
        "We would like to thank JarvisLabs.ai for providing with the GPU credits.\n",
        "We would like to thank Manim Community for the manim library.A personal note of thanks to Puja Roychowdhury for helping us with the Learning Rate Schedule."
      ],
      "metadata": {
        "id": "2HmDFPx-2Enw"
      }
    }
  ]
}