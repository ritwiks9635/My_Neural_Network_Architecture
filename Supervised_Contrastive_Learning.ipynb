{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwiks9635/My_Neural_Network_Architecture/blob/main/Supervised_Contrastive_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Supervised Contrastive Learning**"
      ],
      "metadata": {
        "id": "in2veHjHcoj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "[Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n",
        "(Prannay Khosla et al.) is a training methodology that outperforms\n",
        "supervised training with crossentropy on classification tasks.\n",
        "\n",
        "Essentially, training an image classification model with Supervised Contrastive\n",
        "Learning is performed in two phases:\n",
        "\n",
        "1. Training an encoder to learn to produce vector representations of input images such\n",
        "that representations of images in the same class will be more similar compared to\n",
        "representations of images in different classes.\n",
        "2. Training a classifier on top of the frozen encoder.\n",
        "\n",
        "Note that this example requires [TensorFlow Addons](https://www.tensorflow.org/addons),\n",
        "which you can install using the following command:"
      ],
      "metadata": {
        "id": "480mmvPQkjcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow_addons"
      ],
      "metadata": {
        "id": "fEezb3mOX_Zv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "105a0b54-7c98-4482-a6c8-9a87b48b2143"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZu2IW-FYcZ0",
        "outputId": "5aeed35b-5b60-41d0-9923-df250a6ee462"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "print(f\"x_train shape :: {x_train.shape}, y_train shape is :: {y_train.shape}\")\n",
        "print(f\"x_train shape :: {x_test.shape}, y_train shape is :: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqvz9l9mY8bv",
        "outputId": "80866b98-074c-4006-8d12-16200a072de2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n",
            "x_train shape :: (50000, 32, 32, 3), y_train shape is :: (50000, 1)\n",
            "x_train shape :: (10000, 32, 32, 3), y_train shape is :: (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.02)\n",
        "    ])\n",
        "\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ],
      "metadata": {
        "id": "jGTdiTlZaz50"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the encoder model\n",
        "\n",
        "The encoder model takes the image as input and turns it into a 2048-dimensional\n",
        "feature vector."
      ],
      "metadata": {
        "id": "1-flqnTUkuE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_encoder():\n",
        "    resnet = keras.applications.ResNet50V2(\n",
        "        include_top = False, input_shape = input_shape, weights = None, pooling = \"avg\")\n",
        "\n",
        "    inputs = keras.Input(shape = input_shape)\n",
        "    augmented = data_augmentation(inputs)\n",
        "    outputs = resnet(augmented)\n",
        "    model = keras.Model(inputs, outputs, name = \"cifar10-encoder\")\n",
        "    return model\n",
        "\n",
        "encoder = create_encoder()\n",
        "encoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udRoGmJtbd8e",
        "outputId": "f5e0ba17-8bf0-412e-f077-846d3cc6d2ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cifar10-encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, 32, 32, 3)         7         \n",
            "                                                                 \n",
            " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23564807 (89.89 MB)\n",
            "Trainable params: 23519360 (89.72 MB)\n",
            "Non-trainable params: 45447 (177.53 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 265\n",
        "hidden_units = 512\n",
        "projection_units = 128\n",
        "num_epochs = 50\n",
        "dropout_rate = 0.5\n",
        "temperature = 0.05"
      ],
      "metadata": {
        "id": "QM91M4POgm0P"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the classification model\n",
        "\n",
        "The classification model adds a fully-connected layer on top of the encoder,\n",
        "plus a softmax layer with the target classes."
      ],
      "metadata": {
        "id": "bROtx14ikw4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_classifier(encoder, trainable = True):\n",
        "    for layer in encoder.layers:\n",
        "        layer.trainable = trainable\n",
        "\n",
        "    inputs = keras.Input(shape = input_shape)\n",
        "    features = encoder(inputs)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    features = layers.Dense(hidden_units, activation = \"relu\")(features)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    outputs = layers.Dense(num_classes, activation = \"softmax\")(features)\n",
        "\n",
        "    model = keras.Model(inputs = inputs, outputs = outputs, name = \"cifar10_classifier\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer = keras.optimizers.Adam(learning_rate),\n",
        "        loss = keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics = [keras.metrics.SparseCategoricalAccuracy()])\n",
        "    return model"
      ],
      "metadata": {
        "id": "bxk7AqnGfIdI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 1: Train the baseline classification model\n",
        "\n",
        "In this experiment, a baseline classifier is trained as usual, i.e., the\n",
        "encoder and the classifier parts are trained together as a single model\n",
        "to minimize the crossentropy loss."
      ],
      "metadata": {
        "id": "R0yVw3v9k2FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = create_encoder()\n",
        "classifier = create_classifier(encoder)\n",
        "classifier.summary()\n",
        "\n",
        "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpU1WdtagJqj",
        "outputId": "de84248d-2944-4718-87d3-0e94408f4a47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cifar10_classifier\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " cifar10-encoder (Functiona  (None, 2048)              23564807  \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1049088   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24619025 (93.91 MB)\n",
            "Trainable params: 24573578 (93.74 MB)\n",
            "Non-trainable params: 45447 (177.53 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "189/189 [==============================] - 61s 110ms/step - loss: 1.8480 - sparse_categorical_accuracy: 0.3263\n",
            "Epoch 2/50\n",
            "189/189 [==============================] - 19s 98ms/step - loss: 1.4042 - sparse_categorical_accuracy: 0.4960\n",
            "Epoch 3/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 1.2406 - sparse_categorical_accuracy: 0.5623\n",
            "Epoch 4/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 1.1054 - sparse_categorical_accuracy: 0.6153\n",
            "Epoch 5/50\n",
            "189/189 [==============================] - 19s 102ms/step - loss: 0.9864 - sparse_categorical_accuracy: 0.6596\n",
            "Epoch 6/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.9070 - sparse_categorical_accuracy: 0.6869\n",
            "Epoch 7/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.8220 - sparse_categorical_accuracy: 0.7180\n",
            "Epoch 8/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 0.7979 - sparse_categorical_accuracy: 0.7273\n",
            "Epoch 9/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.7297 - sparse_categorical_accuracy: 0.7508\n",
            "Epoch 10/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.6812 - sparse_categorical_accuracy: 0.7699\n",
            "Epoch 11/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.6187 - sparse_categorical_accuracy: 0.7903\n",
            "Epoch 12/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 0.9473 - sparse_categorical_accuracy: 0.6770\n",
            "Epoch 13/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 0.7639 - sparse_categorical_accuracy: 0.7400\n",
            "Epoch 14/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.7516 - sparse_categorical_accuracy: 0.7474\n",
            "Epoch 15/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.6338 - sparse_categorical_accuracy: 0.7844\n",
            "Epoch 16/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 0.5751 - sparse_categorical_accuracy: 0.8069\n",
            "Epoch 17/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.5117 - sparse_categorical_accuracy: 0.8265\n",
            "Epoch 18/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.4607 - sparse_categorical_accuracy: 0.8438\n",
            "Epoch 19/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 0.4326 - sparse_categorical_accuracy: 0.8557\n",
            "Epoch 20/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.4005 - sparse_categorical_accuracy: 0.8638\n",
            "Epoch 21/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 0.3755 - sparse_categorical_accuracy: 0.8739\n",
            "Epoch 22/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.3549 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 23/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.3379 - sparse_categorical_accuracy: 0.8845\n",
            "Epoch 24/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 0.3230 - sparse_categorical_accuracy: 0.8908\n",
            "Epoch 25/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 0.2992 - sparse_categorical_accuracy: 0.8989\n",
            "Epoch 26/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.2808 - sparse_categorical_accuracy: 0.9062\n",
            "Epoch 27/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.3323 - sparse_categorical_accuracy: 0.8919\n",
            "Epoch 28/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.3267 - sparse_categorical_accuracy: 0.8908\n",
            "Epoch 29/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 0.3396 - sparse_categorical_accuracy: 0.8869\n",
            "Epoch 30/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.2477 - sparse_categorical_accuracy: 0.9161\n",
            "Epoch 31/50\n",
            "189/189 [==============================] - 19s 103ms/step - loss: 0.2123 - sparse_categorical_accuracy: 0.9282\n",
            "Epoch 32/50\n",
            "189/189 [==============================] - 19s 102ms/step - loss: 0.2093 - sparse_categorical_accuracy: 0.9297\n",
            "Epoch 33/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.1958 - sparse_categorical_accuracy: 0.9342\n",
            "Epoch 34/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 0.1859 - sparse_categorical_accuracy: 0.9372\n",
            "Epoch 35/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 0.1748 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 36/50\n",
            "189/189 [==============================] - 21s 112ms/step - loss: 0.1786 - sparse_categorical_accuracy: 0.9404\n",
            "Epoch 37/50\n",
            "189/189 [==============================] - 20s 106ms/step - loss: 0.1687 - sparse_categorical_accuracy: 0.9432\n",
            "Epoch 38/50\n",
            "189/189 [==============================] - 19s 102ms/step - loss: 0.1694 - sparse_categorical_accuracy: 0.9442\n",
            "Epoch 39/50\n",
            "189/189 [==============================] - 20s 107ms/step - loss: 0.1621 - sparse_categorical_accuracy: 0.9457\n",
            "Epoch 40/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.1477 - sparse_categorical_accuracy: 0.9493\n",
            "Epoch 41/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.1376 - sparse_categorical_accuracy: 0.9536\n",
            "Epoch 42/50\n",
            "189/189 [==============================] - 19s 103ms/step - loss: 0.1410 - sparse_categorical_accuracy: 0.9524\n",
            "Epoch 43/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.1351 - sparse_categorical_accuracy: 0.9550\n",
            "Epoch 44/50\n",
            "189/189 [==============================] - 20s 104ms/step - loss: 0.1259 - sparse_categorical_accuracy: 0.9576\n",
            "Epoch 45/50\n",
            "189/189 [==============================] - 19s 102ms/step - loss: 0.1164 - sparse_categorical_accuracy: 0.9610\n",
            "Epoch 46/50\n",
            "189/189 [==============================] - 20s 105ms/step - loss: 0.1213 - sparse_categorical_accuracy: 0.9591\n",
            "Epoch 47/50\n",
            "189/189 [==============================] - 19s 103ms/step - loss: 0.1205 - sparse_categorical_accuracy: 0.9596\n",
            "Epoch 48/50\n",
            "189/189 [==============================] - 19s 102ms/step - loss: 0.1121 - sparse_categorical_accuracy: 0.9622\n",
            "Epoch 49/50\n",
            "189/189 [==============================] - 19s 102ms/step - loss: 0.1092 - sparse_categorical_accuracy: 0.9642\n",
            "Epoch 50/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 0.1030 - sparse_categorical_accuracy: 0.9658\n",
            "313/313 [==============================] - 5s 12ms/step - loss: 1.0134 - sparse_categorical_accuracy: 0.7881\n",
            "Test accuracy: 78.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 2: Use supervised contrastive learning\n",
        "\n",
        "In this experiment, the model is trained in two phases. In the first phase,\n",
        "the encoder is pretrained to optimize the supervised contrastive loss,\n",
        "described in [Prannay Khosla et al.](https://arxiv.org/abs/2004.11362).\n",
        "\n",
        "In the second phase, the classifier is trained using the trained encoder with\n",
        "its weights freezed; only the weights of fully-connected layers with the\n",
        "softmax are optimized.\n",
        "\n",
        "### 1. Supervised contrastive learning loss function"
      ],
      "metadata": {
        "id": "EU8SMM9-lzga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
        "    def __init__(self, temperature=1, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
        "        # Normalize feature vectors\n",
        "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
        "        # Compute logits\n",
        "        logits = tf.divide(\n",
        "            tf.matmul(\n",
        "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
        "            ),\n",
        "            self.temperature,\n",
        "        )\n",
        "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
        "\n",
        "\n",
        "def add_projection_head(encoder):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    features = encoder(inputs)\n",
        "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
        "    model = keras.Model(\n",
        "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "k0OozZ_ojptr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = create_encoder()\n",
        "\n",
        "encoder_with_projection_head = add_projection_head(encoder)\n",
        "encoder_with_projection_head.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=SupervisedContrastiveLoss(temperature),\n",
        ")\n",
        "\n",
        "encoder_with_projection_head.summary()\n",
        "\n",
        "history = encoder_with_projection_head.fit(\n",
        "    x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs\n",
        ")\n",
        "\n",
        "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbY2uKiUkBpm",
        "outputId": "70de6b4f-7ab0-43fc-88e7-f35ae3ad3b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cifar-encoder_with_projection-head\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " cifar10-encoder (Functiona  (None, 2048)              23564807  \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               262272    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23827079 (90.89 MB)\n",
            "Trainable params: 23781632 (90.72 MB)\n",
            "Non-trainable params: 45447 (177.53 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "189/189 [==============================] - 44s 101ms/step - loss: 5.3206\n",
            "Epoch 2/50\n",
            "189/189 [==============================] - 19s 102ms/step - loss: 5.0688\n",
            "Epoch 3/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 4.9244\n",
            "Epoch 4/50\n",
            "189/189 [==============================] - 19s 99ms/step - loss: 4.8028\n",
            "Epoch 5/50\n",
            "189/189 [==============================] - 19s 99ms/step - loss: 4.6940\n",
            "Epoch 6/50\n",
            "189/189 [==============================] - 20s 106ms/step - loss: 4.6037\n",
            "Epoch 7/50\n",
            "189/189 [==============================] - 20s 104ms/step - loss: 4.5189\n",
            "Epoch 8/50\n",
            "189/189 [==============================] - 20s 103ms/step - loss: 4.4579\n",
            "Epoch 9/50\n",
            "189/189 [==============================] - 19s 102ms/step - loss: 4.3957\n",
            "Epoch 10/50\n",
            "189/189 [==============================] - 20s 105ms/step - loss: 4.3335\n",
            "Epoch 11/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 4.2762\n",
            "Epoch 12/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 4.2337\n",
            "Epoch 13/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 4.1852\n",
            "Epoch 14/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 4.1486\n",
            "Epoch 15/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 4.1027\n",
            "Epoch 16/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 4.0591\n",
            "Epoch 17/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 4.0155\n",
            "Epoch 18/50\n",
            "189/189 [==============================] - 19s 102ms/step - loss: 3.9815\n",
            "Epoch 19/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.9492\n",
            "Epoch 20/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.9120\n",
            "Epoch 21/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 3.8904\n",
            "Epoch 22/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.8584\n",
            "Epoch 23/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.8345\n",
            "Epoch 24/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.8138\n",
            "Epoch 25/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.7816\n",
            "Epoch 26/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 3.7645\n",
            "Epoch 27/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.7423\n",
            "Epoch 28/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.7225\n",
            "Epoch 29/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 3.6987\n",
            "Epoch 30/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.6919\n",
            "Epoch 31/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 3.6666\n",
            "Epoch 32/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.6477\n",
            "Epoch 33/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.6366\n",
            "Epoch 34/50\n",
            "189/189 [==============================] - 19s 102ms/step - loss: 3.6359\n",
            "Epoch 35/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.6119\n",
            "Epoch 36/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 3.6003\n",
            "Epoch 37/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 3.5937\n",
            "Epoch 38/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.5825\n",
            "Epoch 39/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 3.5687\n",
            "Epoch 40/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.5586\n",
            "Epoch 41/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.5536\n",
            "Epoch 42/50\n",
            "189/189 [==============================] - 19s 101ms/step - loss: 3.5406\n",
            "Epoch 43/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.5310\n",
            "Epoch 44/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.5276\n",
            "Epoch 45/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.5134\n",
            "Epoch 46/50\n",
            "189/189 [==============================] - 19s 100ms/step - loss: 3.5111\n",
            "Epoch 47/50\n",
            "110/189 [================>.............] - ETA: 7s - loss: 3.4860"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Train the classifier with the frozen encoder"
      ],
      "metadata": {
        "id": "-AREpik4l6VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = create_classifier(encoder, trainable=False)\n",
        "\n",
        "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUrFfNOYkOff",
        "outputId": "26d154ff-6d35-47cb-afa8-712c3f2077d3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "189/189 [==============================] - 10s 28ms/step - loss: 0.1360 - sparse_categorical_accuracy: 0.9665\n",
            "Epoch 2/50\n",
            "189/189 [==============================] - 5s 29ms/step - loss: 0.1058 - sparse_categorical_accuracy: 0.9717\n",
            "Epoch 3/50\n",
            "189/189 [==============================] - 5s 29ms/step - loss: 0.1049 - sparse_categorical_accuracy: 0.9706\n",
            "Epoch 4/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.1026 - sparse_categorical_accuracy: 0.9712\n",
            "Epoch 5/50\n",
            "189/189 [==============================] - 5s 29ms/step - loss: 0.0997 - sparse_categorical_accuracy: 0.9724\n",
            "Epoch 6/50\n",
            "189/189 [==============================] - 5s 29ms/step - loss: 0.1021 - sparse_categorical_accuracy: 0.9726\n",
            "Epoch 7/50\n",
            "189/189 [==============================] - 6s 30ms/step - loss: 0.0983 - sparse_categorical_accuracy: 0.9724\n",
            "Epoch 8/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0964 - sparse_categorical_accuracy: 0.9734\n",
            "Epoch 9/50\n",
            "189/189 [==============================] - 6s 30ms/step - loss: 0.0981 - sparse_categorical_accuracy: 0.9712\n",
            "Epoch 10/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0974 - sparse_categorical_accuracy: 0.9717\n",
            "Epoch 11/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0973 - sparse_categorical_accuracy: 0.9722\n",
            "Epoch 12/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0981 - sparse_categorical_accuracy: 0.9729\n",
            "Epoch 13/50\n",
            "189/189 [==============================] - 5s 27ms/step - loss: 0.0976 - sparse_categorical_accuracy: 0.9717\n",
            "Epoch 14/50\n",
            "189/189 [==============================] - 6s 29ms/step - loss: 0.0982 - sparse_categorical_accuracy: 0.9720\n",
            "Epoch 15/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0965 - sparse_categorical_accuracy: 0.9722\n",
            "Epoch 16/50\n",
            "189/189 [==============================] - 6s 29ms/step - loss: 0.0989 - sparse_categorical_accuracy: 0.9715\n",
            "Epoch 17/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0972 - sparse_categorical_accuracy: 0.9724\n",
            "Epoch 18/50\n",
            "189/189 [==============================] - 5s 29ms/step - loss: 0.0983 - sparse_categorical_accuracy: 0.9721\n",
            "Epoch 19/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0959 - sparse_categorical_accuracy: 0.9728\n",
            "Epoch 20/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0963 - sparse_categorical_accuracy: 0.9723\n",
            "Epoch 21/50\n",
            "189/189 [==============================] - 6s 29ms/step - loss: 0.0970 - sparse_categorical_accuracy: 0.9723\n",
            "Epoch 22/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0958 - sparse_categorical_accuracy: 0.9725\n",
            "Epoch 23/50\n",
            "189/189 [==============================] - 6s 29ms/step - loss: 0.0963 - sparse_categorical_accuracy: 0.9720\n",
            "Epoch 24/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0949 - sparse_categorical_accuracy: 0.9728\n",
            "Epoch 25/50\n",
            "189/189 [==============================] - 6s 29ms/step - loss: 0.0968 - sparse_categorical_accuracy: 0.9722\n",
            "Epoch 26/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0971 - sparse_categorical_accuracy: 0.9718\n",
            "Epoch 27/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0975 - sparse_categorical_accuracy: 0.9724\n",
            "Epoch 28/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0974 - sparse_categorical_accuracy: 0.9725\n",
            "Epoch 29/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0956 - sparse_categorical_accuracy: 0.9734\n",
            "Epoch 30/50\n",
            "189/189 [==============================] - 6s 29ms/step - loss: 0.0945 - sparse_categorical_accuracy: 0.9726\n",
            "Epoch 31/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0952 - sparse_categorical_accuracy: 0.9722\n",
            "Epoch 32/50\n",
            "189/189 [==============================] - 6s 29ms/step - loss: 0.0960 - sparse_categorical_accuracy: 0.9729\n",
            "Epoch 33/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.1002 - sparse_categorical_accuracy: 0.9716\n",
            "Epoch 34/50\n",
            "189/189 [==============================] - 6s 29ms/step - loss: 0.0984 - sparse_categorical_accuracy: 0.9715\n",
            "Epoch 35/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0975 - sparse_categorical_accuracy: 0.9713\n",
            "Epoch 36/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.1004 - sparse_categorical_accuracy: 0.9712\n",
            "Epoch 37/50\n",
            "189/189 [==============================] - 6s 29ms/step - loss: 0.0974 - sparse_categorical_accuracy: 0.9715\n",
            "Epoch 38/50\n",
            "189/189 [==============================] - 5s 28ms/step - loss: 0.0967 - sparse_categorical_accuracy: 0.9725\n",
            "Epoch 39/50\n",
            "189/189 [==============================] - 6s 30ms/step - loss: 0.0956 - sparse_categorical_accuracy: 0.9711\n",
            "Epoch 40/50\n",
            "189/189 [==============================] - 5s 29ms/step - loss: 0.0970 - sparse_categorical_accuracy: 0.9718\n",
            "Epoch 41/50\n",
            "189/189 [==============================] - 6s 30ms/step - loss: 0.0997 - sparse_categorical_accuracy: 0.9713\n",
            "Epoch 42/50\n",
            "189/189 [==============================] - 5s 29ms/step - loss: 0.0945 - sparse_categorical_accuracy: 0.9724\n",
            "Epoch 43/50\n",
            "189/189 [==============================] - 6s 30ms/step - loss: 0.0986 - sparse_categorical_accuracy: 0.9711\n",
            "Epoch 44/50\n",
            "189/189 [==============================] - 5s 29ms/step - loss: 0.0934 - sparse_categorical_accuracy: 0.9730\n",
            "Epoch 45/50\n",
            "189/189 [==============================] - 5s 29ms/step - loss: 0.0976 - sparse_categorical_accuracy: 0.9721\n",
            "Epoch 46/50\n",
            "189/189 [==============================] - 6s 30ms/step - loss: 0.0980 - sparse_categorical_accuracy: 0.9716\n",
            "Epoch 47/50\n",
            "189/189 [==============================] - 5s 29ms/step - loss: 0.1011 - sparse_categorical_accuracy: 0.9712\n",
            "Epoch 48/50\n",
            "189/189 [==============================] - 6s 30ms/step - loss: 0.0965 - sparse_categorical_accuracy: 0.9721\n",
            "Epoch 49/50\n",
            "189/189 [==============================] - 5s 29ms/step - loss: 0.0943 - sparse_categorical_accuracy: 0.9724\n",
            "Epoch 50/50\n",
            "189/189 [==============================] - 6s 29ms/step - loss: 0.0980 - sparse_categorical_accuracy: 0.9727\n",
            "313/313 [==============================] - 5s 12ms/step - loss: 0.9672 - sparse_categorical_accuracy: 0.7961\n",
            "Test accuracy: 79.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get to an improved test accuracy.\n",
        "\n",
        "\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "As shown in the experiments, using the supervised contrastive learning technique\n",
        "outperformed the conventional technique in terms of the test accuracy. Note that\n",
        "the same training budget (i.e., number of epochs) was given to each technique.\n",
        "Supervised contrastive learning pays off when the encoder involves a complex\n",
        "architecture, like ResNet, and multi-class problems with many labels.\n",
        "In addition, large batch sizes and multi-layer projection heads\n",
        "improve its effectiveness. See the [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n",
        "paper for more details.\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/supervised-contrastive-learning-cifar10)\n",
        "and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/supervised-contrastive-learning)."
      ],
      "metadata": {
        "id": "AJ955R_LmBFh"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}