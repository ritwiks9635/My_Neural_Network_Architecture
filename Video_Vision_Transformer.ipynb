{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwiks9635/My_Neural_Network_Architecture/blob/main/Video_Vision_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Video Vision Transformer**"
      ],
      "metadata": {
        "id": "fqJFp1ZjVjvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Videos are sequences of images. Let's assume you have an image\n",
        "representation model (CNN, ViT, etc.) and a sequence model\n",
        "(RNN, LSTM, etc.) at hand. We ask you to tweak the model for video\n",
        "classification. The simplest approach would be to apply the image\n",
        "model to individual frames, use the sequence model to learn\n",
        "sequences of image features, then apply a classification head on\n",
        "the learned sequence representation.\n",
        "The Keras example\n",
        "[Video Classification with a CNN-RNN Architecture](https://keras.io/examples/vision/video_classification/)\n",
        "explains this approach in detail. Alernatively, you can also\n",
        "build a hybrid Transformer-based model for video classification as shown in the Keras example\n",
        "[Video Classification with Transformers](https://keras.io/examples/vision/video_transformers/).\n",
        "\n",
        "In this example, we minimally implement\n",
        "[ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691)\n",
        "by Arnab et al., a **pure Transformer-based** model\n",
        "for video classification. The authors propose a novel embedding scheme\n",
        "and a number of Transformer variants to model video clips. We implement\n",
        "the embedding scheme and one of the variants of the Transformer\n",
        "architecture, for simplicity.\n",
        "\n",
        "This example requires TensorFlow 2.6 or higher, and the `medmnist`\n",
        "package, which can be installed by running the code cell below."
      ],
      "metadata": {
        "id": "M2KoQVtxVsZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq medmnist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ0KqIeEZjX7",
        "outputId": "f73d14c1-c07d-4634-9067-dd8ad0679763"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import imageio\n",
        "import medmnist\n",
        "import ipywidgets\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "SEED = 42\n",
        "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
        "keras.utils.set_random_seed(SEED)"
      ],
      "metadata": {
        "id": "MflL1_NZZ--3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n",
        "\n",
        "The hyperparameters are chosen via hyperparameter\n",
        "search. You can learn more about the process in the \"conclusion\" section."
      ],
      "metadata": {
        "id": "kTDuT3uFV79W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA\n",
        "DATASET_NAME =  \"organmnist3d\"\n",
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "INPUT_SHAPE = (28, 28, 28, 1)\n",
        "NUM_CLASSES = 11\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "\n",
        "# TRAINING\n",
        "EPOCHS = 60\n",
        "\n",
        "# TUBELET EMBEDDING\n",
        "PATCH_SIZE = (8, 8, 8)\n",
        "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
        "\n",
        "# ViViT ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "PROJECTION_DIM = 128\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 8"
      ],
      "metadata": {
        "id": "ajeoZNKlaEVQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "For our example we use the\n",
        "[MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification](https://medmnist.com/)\n",
        "dataset. The videos are lightweight and easy to train on."
      ],
      "metadata": {
        "id": "OB-xbVDNWA3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_prepare_dataset(data_info: dict):\n",
        "    \"\"\"Utility function to download the dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_info (dict): Dataset metadata.\n",
        "    \"\"\"\n",
        "    data_path = keras.utils.get_file(origin=data_info[\"url\"], md5_hash=data_info[\"MD5\"])\n",
        "\n",
        "    with np.load(data_path) as data:\n",
        "        # Get videos\n",
        "        train_videos = data[\"train_images\"]\n",
        "        valid_videos = data[\"val_images\"]\n",
        "        test_videos = data[\"test_images\"]\n",
        "\n",
        "        # Get labels\n",
        "        train_labels = data[\"train_labels\"].flatten()\n",
        "        valid_labels = data[\"val_labels\"].flatten()\n",
        "        test_labels = data[\"test_labels\"].flatten()\n",
        "\n",
        "    return (\n",
        "        (train_videos, train_labels),\n",
        "        (valid_videos, valid_labels),\n",
        "        (test_videos, test_labels),\n",
        "    )\n",
        "\n",
        "\n",
        "# Get the metadata of the dataset\n",
        "info = medmnist.INFO[DATASET_NAME]\n",
        "\n",
        "# Get the dataset\n",
        "prepared_dataset = download_and_prepare_dataset(info)\n",
        "(train_videos, train_labels) = prepared_dataset[0]\n",
        "(valid_videos, valid_labels) = prepared_dataset[1]\n",
        "(test_videos, test_labels) = prepared_dataset[2]"
      ],
      "metadata": {
        "id": "m6yVG6oxaTPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1713d1-ec61-482f-8899-2550562182d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://zenodo.org/record/6496656/files/organmnist3d.npz?download=1\n",
            "32657407/32657407 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `tf.data` pipeline"
      ],
      "metadata": {
        "id": "SfXKQGhSWIFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
        "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
        "    # Preprocess images\n",
        "    frames = tf.image.convert_image_dtype(\n",
        "        frames[\n",
        "            ..., tf.newaxis\n",
        "        ],  # The new axis is to help for further processing with Conv3D layers\n",
        "        tf.float32)\n",
        "    # Parse label\n",
        "    label = tf.cast(label, tf.float32)\n",
        "    return frames, label\n",
        "\n",
        "\n",
        "def prepare_dataloader(\n",
        "    videos: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    loader_type: str = \"train\",\n",
        "    batch_size: int = BATCH_SIZE):\n",
        "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
        "\n",
        "    if loader_type == \"train\":\n",
        "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
        "\n",
        "    dataloader = (\n",
        "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .batch(batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE))\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
        "validloader = prepare_dataloader(valid_videos, valid_labels, \"valid\")\n",
        "testloader = prepare_dataloader(test_videos, test_labels, \"test\")"
      ],
      "metadata": {
        "id": "b3UxK15IJ_rZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tubelet Embedding\n",
        "\n",
        "In ViTs, an image is divided into patches, which are then spatially\n",
        "flattened, a process known as tokenization. For a video, one can\n",
        "repeat this process for individual frames. **Uniform frame sampling**\n",
        "as suggested by the authors is a tokenization scheme in which we\n",
        "sample frames from the video clip and perform simple ViT tokenization.\n",
        "\n",
        "| ![uniform frame sampling](https://i.imgur.com/aaPyLPX.png) |\n",
        "| :--: |\n",
        "| Uniform Frame Sampling [Source](https://arxiv.org/abs/2103.15691) |\n",
        "\n",
        "**Tubelet Embedding** is different in terms of capturing temporal\n",
        "information from the video.\n",
        "First, we extract volumes from the video -- these volumes contain\n",
        "patches of the frame and the temporal information as well. The volumes\n",
        "are then flattened to build video tokens.\n",
        "\n",
        "| ![tubelet embedding](https://i.imgur.com/9G7QTfV.png) |\n",
        "| :--: |\n",
        "| Tubelet Embedding [Source](https://arxiv.org/abs/2103.15691) |"
      ],
      "metadata": {
        "id": "6fhxNh4-WO8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TubeletEmbedding(layers.Layer):\n",
        "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.projection = layers.Conv3D(\n",
        "            filters=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            strides=patch_size,\n",
        "            padding=\"VALID\")\n",
        "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
        "\n",
        "    def call(self, videos):\n",
        "        projected_patches = self.projection(videos)\n",
        "        flattened_patches = self.flatten(projected_patches)\n",
        "        return flattened_patches"
      ],
      "metadata": {
        "id": "2_yaTwt2UHy4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Embedding\n",
        "\n",
        "This layer adds positional information to the encoded video tokens."
      ],
      "metadata": {
        "id": "--UqABShWW_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_tokens, _ = input_shape\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_tokens, output_dim=self.embed_dim)\n",
        "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
        "\n",
        "    def call(self, encoded_tokens):\n",
        "        # Encode the positions and add it to the encoded tokens\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_tokens = encoded_tokens + encoded_positions\n",
        "        return encoded_tokens"
      ],
      "metadata": {
        "id": "zdtDAQXXKo5B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Vision Transformer\n",
        "\n",
        "The authors suggest 4 variants of Vision Transformer:\n",
        "\n",
        "- Spatio-temporal attention\n",
        "- Factorized encoder\n",
        "- Factorized self-attention\n",
        "- Factorized dot-product attention\n",
        "\n",
        "In this example, we will implement the **Spatio-temporal attention**\n",
        "model for simplicity. The following code snippet is heavily inspired from\n",
        "[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/).\n",
        "One can also refer to the\n",
        "[official repository of ViViT](https://github.com/google-research/scenic/tree/main/scenic/projects/vivit)\n",
        "which contains all the variants, implemented in JAX."
      ],
      "metadata": {
        "id": "HNExKG9IWmvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vivit_classifier(\n",
        "    tubelet_embedder,\n",
        "    positional_encoder,\n",
        "    input_shape=INPUT_SHAPE,\n",
        "    transformer_layers=NUM_LAYERS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    embed_dim=PROJECTION_DIM,\n",
        "    layer_norm_eps=LAYER_NORM_EPS,\n",
        "    num_classes=NUM_CLASSES):\n",
        "    # Get the input layer\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Create patches.\n",
        "    patches = tubelet_embedder(inputs)\n",
        "    # Encode patches.\n",
        "    encoded_patches = positional_encoder(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization and MHSA\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1)(x1, x1)\n",
        "\n",
        "        # Skip connection\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        # Layer Normalization and MLP\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
        "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
        "            ])(x3)\n",
        "\n",
        "        # Skip connection\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Layer normalization and Global average pooling.\n",
        "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
        "    representation = layers.GlobalAvgPool1D()(representation)\n",
        "\n",
        "    # Classify outputs.\n",
        "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "2OTodt5RTn1c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment():\n",
        "    # Initialize model\n",
        "    model = create_vivit_classifier(\n",
        "        tubelet_embedder=TubeletEmbedding(\n",
        "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE),\n",
        "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM))\n",
        "\n",
        "    # Compile the model with the optimizer, loss function\n",
        "    # and the metrics.\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ])\n",
        "\n",
        "    # Train the model.\n",
        "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOUOouNzUlZq",
        "outputId": "d4b16258-df77-4b8e-f418-bfd49f8e1bdc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "31/31 [==============================] - 38s 119ms/step - loss: 2.5485 - accuracy: 0.1111 - top-5-accuracy: 0.5597 - val_loss: 2.3513 - val_accuracy: 0.1988 - val_top-5-accuracy: 0.5776\n",
            "Epoch 2/60\n",
            "31/31 [==============================] - 1s 45ms/step - loss: 2.2323 - accuracy: 0.1800 - top-5-accuracy: 0.6687 - val_loss: 2.0809 - val_accuracy: 0.2174 - val_top-5-accuracy: 0.7205\n",
            "Epoch 3/60\n",
            "31/31 [==============================] - 1s 43ms/step - loss: 2.0593 - accuracy: 0.2150 - top-5-accuracy: 0.7767 - val_loss: 1.8950 - val_accuracy: 0.3106 - val_top-5-accuracy: 0.8075\n",
            "Epoch 4/60\n",
            "31/31 [==============================] - 2s 52ms/step - loss: 1.9996 - accuracy: 0.2263 - top-5-accuracy: 0.7973 - val_loss: 1.7612 - val_accuracy: 0.3230 - val_top-5-accuracy: 0.8509\n",
            "Epoch 5/60\n",
            "31/31 [==============================] - 2s 52ms/step - loss: 1.8056 - accuracy: 0.3107 - top-5-accuracy: 0.8663 - val_loss: 1.7071 - val_accuracy: 0.4099 - val_top-5-accuracy: 0.8447\n",
            "Epoch 6/60\n",
            "31/31 [==============================] - 2s 66ms/step - loss: 1.6118 - accuracy: 0.4023 - top-5-accuracy: 0.8899 - val_loss: 1.2863 - val_accuracy: 0.4596 - val_top-5-accuracy: 0.9503\n",
            "Epoch 7/60\n",
            "31/31 [==============================] - 2s 56ms/step - loss: 1.4305 - accuracy: 0.4362 - top-5-accuracy: 0.9424 - val_loss: 1.3678 - val_accuracy: 0.4783 - val_top-5-accuracy: 0.9503\n",
            "Epoch 8/60\n",
            "31/31 [==============================] - 1s 46ms/step - loss: 1.3613 - accuracy: 0.4825 - top-5-accuracy: 0.9434 - val_loss: 1.0644 - val_accuracy: 0.5466 - val_top-5-accuracy: 0.9814\n",
            "Epoch 9/60\n",
            "31/31 [==============================] - 1s 48ms/step - loss: 1.2330 - accuracy: 0.5185 - top-5-accuracy: 0.9527 - val_loss: 1.0638 - val_accuracy: 0.5404 - val_top-5-accuracy: 0.9752\n",
            "Epoch 10/60\n",
            "31/31 [==============================] - 1s 45ms/step - loss: 1.2077 - accuracy: 0.5288 - top-5-accuracy: 0.9465 - val_loss: 1.0038 - val_accuracy: 0.5590 - val_top-5-accuracy: 0.9752\n",
            "Epoch 11/60\n",
            "31/31 [==============================] - 1s 44ms/step - loss: 1.1370 - accuracy: 0.5556 - top-5-accuracy: 0.9578 - val_loss: 0.8712 - val_accuracy: 0.6708 - val_top-5-accuracy: 0.9876\n",
            "Epoch 12/60\n",
            "31/31 [==============================] - 4s 111ms/step - loss: 0.9875 - accuracy: 0.6327 - top-5-accuracy: 0.9743 - val_loss: 0.8267 - val_accuracy: 0.6894 - val_top-5-accuracy: 0.9814\n",
            "Epoch 13/60\n",
            "31/31 [==============================] - 1s 42ms/step - loss: 0.8961 - accuracy: 0.6790 - top-5-accuracy: 0.9774 - val_loss: 1.0101 - val_accuracy: 0.5963 - val_top-5-accuracy: 0.9752\n",
            "Epoch 14/60\n",
            "31/31 [==============================] - 1s 42ms/step - loss: 0.9447 - accuracy: 0.6502 - top-5-accuracy: 0.9794 - val_loss: 0.6952 - val_accuracy: 0.7578 - val_top-5-accuracy: 0.9876\n",
            "Epoch 15/60\n",
            "31/31 [==============================] - 1s 43ms/step - loss: 0.7505 - accuracy: 0.7160 - top-5-accuracy: 0.9907 - val_loss: 0.6066 - val_accuracy: 0.8012 - val_top-5-accuracy: 0.9938\n",
            "Epoch 16/60\n",
            "31/31 [==============================] - 1s 42ms/step - loss: 0.7366 - accuracy: 0.7366 - top-5-accuracy: 0.9887 - val_loss: 0.6133 - val_accuracy: 0.7950 - val_top-5-accuracy: 0.9938\n",
            "Epoch 17/60\n",
            "31/31 [==============================] - 1s 45ms/step - loss: 0.6445 - accuracy: 0.7613 - top-5-accuracy: 0.9907 - val_loss: 0.5820 - val_accuracy: 0.7950 - val_top-5-accuracy: 0.9938\n",
            "Epoch 18/60\n",
            "31/31 [==============================] - 1s 46ms/step - loss: 0.5501 - accuracy: 0.8076 - top-5-accuracy: 0.9907 - val_loss: 0.4618 - val_accuracy: 0.8261 - val_top-5-accuracy: 0.9938\n",
            "Epoch 19/60\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 0.5587 - accuracy: 0.7984 - top-5-accuracy: 0.9928 - val_loss: 0.8123 - val_accuracy: 0.7391 - val_top-5-accuracy: 0.9876\n",
            "Epoch 20/60\n",
            "31/31 [==============================] - 1s 43ms/step - loss: 0.6096 - accuracy: 0.7850 - top-5-accuracy: 0.9928 - val_loss: 0.5618 - val_accuracy: 0.8012 - val_top-5-accuracy: 1.0000\n",
            "Epoch 21/60\n",
            "31/31 [==============================] - 1s 42ms/step - loss: 0.6134 - accuracy: 0.7716 - top-5-accuracy: 0.9938 - val_loss: 0.5461 - val_accuracy: 0.7640 - val_top-5-accuracy: 0.9938\n",
            "Epoch 22/60\n",
            "31/31 [==============================] - 1s 41ms/step - loss: 0.4616 - accuracy: 0.8333 - top-5-accuracy: 0.9979 - val_loss: 0.4504 - val_accuracy: 0.8571 - val_top-5-accuracy: 0.9938\n",
            "Epoch 23/60\n",
            "31/31 [==============================] - 1s 43ms/step - loss: 0.3915 - accuracy: 0.8642 - top-5-accuracy: 0.9959 - val_loss: 0.3762 - val_accuracy: 0.8820 - val_top-5-accuracy: 0.9876\n",
            "Epoch 24/60\n",
            "31/31 [==============================] - 1s 43ms/step - loss: 0.4365 - accuracy: 0.8477 - top-5-accuracy: 0.9938 - val_loss: 0.4121 - val_accuracy: 0.8696 - val_top-5-accuracy: 0.9876\n",
            "Epoch 25/60\n",
            "31/31 [==============================] - 2s 67ms/step - loss: 0.3445 - accuracy: 0.8879 - top-5-accuracy: 0.9979 - val_loss: 0.4603 - val_accuracy: 0.8199 - val_top-5-accuracy: 0.9814\n",
            "Epoch 26/60\n",
            "31/31 [==============================] - 1s 46ms/step - loss: 0.3336 - accuracy: 0.8704 - top-5-accuracy: 0.9990 - val_loss: 0.3668 - val_accuracy: 0.9130 - val_top-5-accuracy: 0.9814\n",
            "Epoch 27/60\n",
            "31/31 [==============================] - 1s 43ms/step - loss: 0.2594 - accuracy: 0.9095 - top-5-accuracy: 1.0000 - val_loss: 0.4399 - val_accuracy: 0.8820 - val_top-5-accuracy: 0.9938\n",
            "Epoch 28/60\n",
            "31/31 [==============================] - 1s 44ms/step - loss: 0.2566 - accuracy: 0.9156 - top-5-accuracy: 1.0000 - val_loss: 0.3608 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
            "Epoch 29/60\n",
            "31/31 [==============================] - 1s 42ms/step - loss: 0.2555 - accuracy: 0.9064 - top-5-accuracy: 0.9990 - val_loss: 0.5006 - val_accuracy: 0.8385 - val_top-5-accuracy: 0.9876\n",
            "Epoch 30/60\n",
            "31/31 [==============================] - 1s 46ms/step - loss: 0.2911 - accuracy: 0.8971 - top-5-accuracy: 1.0000 - val_loss: 0.4318 - val_accuracy: 0.8571 - val_top-5-accuracy: 0.9876\n",
            "Epoch 31/60\n",
            "31/31 [==============================] - 2s 68ms/step - loss: 0.1921 - accuracy: 0.9393 - top-5-accuracy: 1.0000 - val_loss: 0.3032 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
            "Epoch 32/60\n",
            "31/31 [==============================] - 2s 53ms/step - loss: 0.1515 - accuracy: 0.9434 - top-5-accuracy: 1.0000 - val_loss: 0.3066 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
            "Epoch 33/60\n",
            "31/31 [==============================] - 1s 44ms/step - loss: 0.1278 - accuracy: 0.9619 - top-5-accuracy: 1.0000 - val_loss: 0.3565 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9938\n",
            "Epoch 34/60\n",
            "31/31 [==============================] - 1s 44ms/step - loss: 0.1220 - accuracy: 0.9609 - top-5-accuracy: 1.0000 - val_loss: 0.3964 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9938\n",
            "Epoch 35/60\n",
            "31/31 [==============================] - 1s 45ms/step - loss: 0.1508 - accuracy: 0.9486 - top-5-accuracy: 1.0000 - val_loss: 0.4699 - val_accuracy: 0.8509 - val_top-5-accuracy: 1.0000\n",
            "Epoch 36/60\n",
            "31/31 [==============================] - 1s 45ms/step - loss: 0.1168 - accuracy: 0.9660 - top-5-accuracy: 0.9990 - val_loss: 0.3909 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9938\n",
            "Epoch 37/60\n",
            "31/31 [==============================] - 2s 64ms/step - loss: 0.0984 - accuracy: 0.9691 - top-5-accuracy: 1.0000 - val_loss: 0.4649 - val_accuracy: 0.8385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 38/60\n",
            "31/31 [==============================] - 1s 41ms/step - loss: 0.0810 - accuracy: 0.9712 - top-5-accuracy: 1.0000 - val_loss: 0.4586 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9876\n",
            "Epoch 39/60\n",
            "31/31 [==============================] - 1s 41ms/step - loss: 0.0669 - accuracy: 0.9835 - top-5-accuracy: 1.0000 - val_loss: 0.4668 - val_accuracy: 0.8882 - val_top-5-accuracy: 0.9938\n",
            "Epoch 40/60\n",
            "31/31 [==============================] - 1s 41ms/step - loss: 0.0760 - accuracy: 0.9722 - top-5-accuracy: 1.0000 - val_loss: 0.4045 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9876\n",
            "Epoch 41/60\n",
            "31/31 [==============================] - 1s 42ms/step - loss: 0.0646 - accuracy: 0.9784 - top-5-accuracy: 1.0000 - val_loss: 0.4428 - val_accuracy: 0.8882 - val_top-5-accuracy: 0.9938\n",
            "Epoch 42/60\n",
            "31/31 [==============================] - 1s 44ms/step - loss: 0.0403 - accuracy: 0.9897 - top-5-accuracy: 1.0000 - val_loss: 0.3966 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
            "Epoch 43/60\n",
            "31/31 [==============================] - 1s 41ms/step - loss: 0.0578 - accuracy: 0.9846 - top-5-accuracy: 1.0000 - val_loss: 0.6816 - val_accuracy: 0.8137 - val_top-5-accuracy: 0.9938\n",
            "Epoch 44/60\n",
            "31/31 [==============================] - 1s 41ms/step - loss: 0.2164 - accuracy: 0.9208 - top-5-accuracy: 0.9990 - val_loss: 0.4123 - val_accuracy: 0.8882 - val_top-5-accuracy: 0.9938\n",
            "Epoch 45/60\n",
            "31/31 [==============================] - 2s 56ms/step - loss: 0.1288 - accuracy: 0.9568 - top-5-accuracy: 0.9990 - val_loss: 0.4464 - val_accuracy: 0.8696 - val_top-5-accuracy: 0.9938\n",
            "Epoch 46/60\n",
            "31/31 [==============================] - 2s 55ms/step - loss: 0.0979 - accuracy: 0.9640 - top-5-accuracy: 1.0000 - val_loss: 0.4544 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9938\n",
            "Epoch 47/60\n",
            "31/31 [==============================] - 1s 45ms/step - loss: 0.0479 - accuracy: 0.9835 - top-5-accuracy: 1.0000 - val_loss: 0.4529 - val_accuracy: 0.8820 - val_top-5-accuracy: 0.9876\n",
            "Epoch 48/60\n",
            "31/31 [==============================] - 1s 47ms/step - loss: 0.0279 - accuracy: 0.9969 - top-5-accuracy: 1.0000 - val_loss: 0.3820 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9876\n",
            "Epoch 49/60\n",
            "31/31 [==============================] - 1s 44ms/step - loss: 0.0259 - accuracy: 0.9928 - top-5-accuracy: 1.0000 - val_loss: 0.4488 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9876\n",
            "Epoch 50/60\n",
            "31/31 [==============================] - 1s 43ms/step - loss: 0.0233 - accuracy: 0.9949 - top-5-accuracy: 1.0000 - val_loss: 0.4282 - val_accuracy: 0.8758 - val_top-5-accuracy: 0.9876\n",
            "Epoch 51/60\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 0.0145 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.5335 - val_accuracy: 0.8634 - val_top-5-accuracy: 0.9938\n",
            "Epoch 52/60\n",
            "31/31 [==============================] - 2s 58ms/step - loss: 0.0135 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.5284 - val_accuracy: 0.8882 - val_top-5-accuracy: 0.9938\n",
            "Epoch 53/60\n",
            "31/31 [==============================] - 1s 41ms/step - loss: 0.0087 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.5423 - val_accuracy: 0.8696 - val_top-5-accuracy: 0.9876\n",
            "Epoch 54/60\n",
            "31/31 [==============================] - 1s 42ms/step - loss: 0.0080 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4117 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
            "Epoch 55/60\n",
            "31/31 [==============================] - 1s 42ms/step - loss: 0.0044 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4236 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9938\n",
            "Epoch 56/60\n",
            "31/31 [==============================] - 1s 48ms/step - loss: 0.0034 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4047 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9938\n",
            "Epoch 57/60\n",
            "31/31 [==============================] - 2s 49ms/step - loss: 0.0027 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4196 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9938\n",
            "Epoch 58/60\n",
            "31/31 [==============================] - 2s 67ms/step - loss: 0.0023 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4380 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9876\n",
            "Epoch 59/60\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 0.0019 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4353 - val_accuracy: 0.8882 - val_top-5-accuracy: 0.9938\n",
            "Epoch 60/60\n",
            "31/31 [==============================] - 1s 44ms/step - loss: 0.0019 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4295 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9938\n",
            "20/20 [==============================] - 0s 19ms/step - loss: 1.1012 - accuracy: 0.7656 - top-5-accuracy: 0.9689\n",
            "Test accuracy: 76.56%\n",
            "Test top 5 accuracy: 96.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES_VIZ = 25\n",
        "testsamples, labels = next(iter(testloader))\n",
        "testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n",
        "\n",
        "ground_truths = []\n",
        "preds = []\n",
        "videos = []\n",
        "\n",
        "for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
        "    # Generate gif\n",
        "    with io.BytesIO() as gif:\n",
        "        imageio.mimsave(gif, (testsample.numpy() * 255).astype(\"uint8\"), \"GIF\", fps=5)\n",
        "        videos.append(gif.getvalue())\n",
        "\n",
        "    # Get model prediction\n",
        "    output = model.predict(tf.expand_dims(testsample, axis=0))[0]\n",
        "    pred = np.argmax(output, axis=0)\n",
        "\n",
        "    ground_truths.append(label.numpy().astype(\"int\"))\n",
        "    preds.append(pred)\n",
        "\n",
        "\n",
        "def make_box_for_grid(image_widget, fit):\n",
        "    \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n",
        "\n",
        "    Source: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Styling.html\n",
        "    \"\"\"\n",
        "    # Make the caption\n",
        "    if fit is not None:\n",
        "        fit_str = \"'{}'\".format(fit)\n",
        "    else:\n",
        "        fit_str = str(fit)\n",
        "\n",
        "    h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n",
        "\n",
        "    # Make the green box with the image widget inside it\n",
        "    boxb = ipywidgets.widgets.Box()\n",
        "    boxb.children = [image_widget]\n",
        "\n",
        "    # Compose into a vertical box\n",
        "    vb = ipywidgets.widgets.VBox()\n",
        "    vb.layout.align_items = \"center\"\n",
        "    vb.children = [h, boxb]\n",
        "    return vb\n",
        "\n",
        "\n",
        "boxes = []\n",
        "for i in range(NUM_SAMPLES_VIZ):\n",
        "    ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n",
        "    true_class = info[\"label\"][str(ground_truths[i])]\n",
        "    pred_class = info[\"label\"][str(preds[i])]\n",
        "    caption = f\"T: {true_class} | P: {pred_class}\"\n",
        "\n",
        "    boxes.append(make_box_for_grid(ib, caption))\n",
        "\n",
        "ipywidgets.widgets.GridBox(\n",
        "    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\"))"
      ],
      "metadata": {
        "id": "MswwOuqOVRnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final thoughts\n",
        "\n",
        "With a vanilla implementation, we achieve ~79-80% Top-1 accuracy on the\n",
        "test dataset.\n",
        "\n",
        "The hyperparameters used in this tutorial were finalized by running a\n",
        "hyperparameter search using\n",
        "[W&B Sweeps](https://docs.wandb.ai/guides/sweeps).\n",
        "You can find out our sweeps result\n",
        "[here](https://wandb.ai/minimal-implementations/vivit/sweeps/66fp0lhz)\n",
        "and our quick analysis of the results\n",
        "[here](https://wandb.ai/minimal-implementations/vivit/reports/Hyperparameter-Tuning-Analysis--VmlldzoxNDEwNzcx).\n",
        "\n",
        "For further improvement, you could look into the following:\n",
        "\n",
        "- Using data augmentation for videos.\n",
        "- Using a better regularization scheme for training.\n",
        "- Apply different variants of the transformer model as in the paper.\n",
        "\n",
        "We would like to thank [Anurag Arnab](https://anuragarnab.github.io/)\n",
        "(first author of ViViT) for helpful discussion. We are grateful to\n",
        "[Weights and Biases](https://wandb.ai/site) program for helping with\n",
        "GPU credits.\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/video-vision-transformer)\n",
        "and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/video-vision-transformer-CT)."
      ],
      "metadata": {
        "id": "LZrVcp5kWv78"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}