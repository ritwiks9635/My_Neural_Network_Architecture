{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwiks9635/My_Neural_Network_Architecture/blob/main/End_to_end_Masked_Language_Modeling_with_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1OIwPNGkEx8"
      },
      "source": [
        "#**End-to-end Masked Language Modeling with BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Masked Language Modeling is a fill-in-the-blank task,\n",
        "where a model uses the context words surrounding a mask token to try to predict what the\n",
        "masked word should be.\n",
        "\n",
        "For an input that contains one or more mask tokens,\n",
        "the model will generate the most likely substitution for each.\n",
        "\n",
        "Example:\n",
        "\n",
        "- Input: \"I have watched this [MASK] and it was awesome.\"\n",
        "- Output: \"I have watched this movie and it was awesome.\"\n",
        "\n",
        "Masked language modeling is a great way to train a language\n",
        "model in a self-supervised setting (without human-annotated labels).\n",
        "Such a model can then be fine-tuned to accomplish various supervised\n",
        "NLP tasks.\n",
        "\n",
        "This example teaches you how to build a BERT model from scratch,\n",
        "train it with the masked language modeling task,\n",
        "and then fine-tune this model on a sentiment classification task.\n",
        "\n",
        "We will use the Keras `TextVectorization` and `MultiHeadAttention` layers\n",
        "to create a BERT Transformer-Encoder network architecture.\n",
        "\n",
        "Note: This example should be run with `tf-nightly`."
      ],
      "metadata": {
        "id": "VgDNrHalry0S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gHDuFTFrkHJv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iieImRiukWof"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    MAX_LEN = 256\n",
        "    BATCH_SIZE = 32\n",
        "    LR = 0.001\n",
        "    VOCAB_SIZE = 30000\n",
        "    EMBED_DIM = 128\n",
        "    NUM_HEAD = 8  # used in bert model\n",
        "    FF_DIM = 128  # used in bert model\n",
        "    NUM_LAYERS = 1\n",
        "\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Suqhfm1ZkYuI",
        "outputId": "62d50d65-eeb5-4377-9238-7475e77c5e09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  13.2M      0  0:00:06  0:00:06 --:--:-- 16.3M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUywlvrbkn4D",
        "outputId": "0b0a07d9-b8d4-48e7-aa44-527d4dbaf5e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6989daadd0ca>:29: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_data = train_df.append(test_df)\n"
          ]
        }
      ],
      "source": [
        "def get_text_list_from_files(files):\n",
        "    text_list = []\n",
        "    for name in files:\n",
        "        with open(name) as f:\n",
        "            for line in f:\n",
        "                text_list.append(line)\n",
        "    return text_list\n",
        "\n",
        "\n",
        "def get_data_from_text_files(folder_name):\n",
        "\n",
        "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
        "    pos_texts = get_text_list_from_files(pos_files)\n",
        "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
        "    neg_texts = get_text_list_from_files(neg_files)\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"review\": pos_texts + neg_texts,\n",
        "            \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),\n",
        "        }\n",
        "    )\n",
        "    df = df.sample(len(df)).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "train_df = get_data_from_text_files(\"train\")\n",
        "test_df  = get_data_from_text_files(\"test\")\n",
        "\n",
        "all_data = train_df.append(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "qCkmbTbIq63N",
        "outputId": "9843eefd-ac3b-4a66-8d4f-f4ea56678010"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  I caught this on Showtime tonight and was amaz...          1\n",
              "1  This movie is another fine example of what Jer...          1\n",
              "2  The \"documentary\", and we use that term loosel...          1\n",
              "3  This movie is the first of the six infamous Gu...          0\n",
              "4  This move is about as bad as they come. I was,...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3360f45-198c-48a5-b352-f8bb9fbcff50\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I caught this on Showtime tonight and was amaz...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This movie is another fine example of what Jer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The \"documentary\", and we use that term loosel...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This movie is the first of the six infamous Gu...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This move is about as bad as they come. I was,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3360f45-198c-48a5-b352-f8bb9fbcff50')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c3360f45-198c-48a5-b352-f8bb9fbcff50 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c3360f45-198c-48a5-b352-f8bb9fbcff50');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7e3b93db-b70c-49eb-9c36-9bfafb57d2e3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7e3b93db-b70c-49eb-9c36-9bfafb57d2e3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7e3b93db-b70c-49eb-9c36-9bfafb57d2e3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "all_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation\n",
        "\n",
        "We will use the `TextVectorization` layer to vectorize the text into integer token ids.\n",
        "It transforms a batch of strings into either\n",
        "a sequence of token indices (one sample = 1D array of integer token indices, in order)\n",
        "or a dense representation (one sample = 1D array of float values encoding an unordered set of tokens).\n",
        "\n",
        "Below, we define 3 preprocessing functions.\n",
        "\n",
        "1.  The `get_vectorize_layer` function builds the `TextVectorization` layer.\n",
        "2.  The `encode` function encodes raw text into integer token ids.\n",
        "3.  The `get_masked_input_and_labels` function will mask input token ids.\n",
        "It masks 15% of all input tokens in each sequence at random."
      ],
      "metadata": {
        "id": "NKyIXJFNr-D6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hV6lK9czts-J"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase,\"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ur1GQInrvY7C"
      },
      "outputs": [],
      "source": [
        "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
        "    \"\"\"Build Text vectorization layer\n",
        "\n",
        "    Args:\n",
        "      texts (list): List of string i.e input texts\n",
        "      vocab_size (int): vocab size\n",
        "      max_seq (int): Maximum sequence lenght.\n",
        "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
        "\n",
        "    Returns:\n",
        "        layers.Layer: Return TextVectorization Keras Layer\n",
        "    \"\"\"\n",
        "    vectorize_layer = TextVectorization(\n",
        "        max_tokens=vocab_size,\n",
        "        output_mode=\"int\",\n",
        "        standardize=custom_standardization,\n",
        "        output_sequence_length=max_seq,)\n",
        "    vectorize_layer.adapt(texts)\n",
        "\n",
        "    # Insert mask token in vocabulary\n",
        "    vocab = vectorize_layer.get_vocabulary()\n",
        "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
        "    vectorize_layer.set_vocabulary(vocab)\n",
        "    return vectorize_layer\n",
        "\n",
        "\n",
        "vectorize_layer = get_vectorize_layer(\n",
        "    all_data.review.values.tolist(),\n",
        "    config.VOCAB_SIZE,\n",
        "    config.MAX_LEN,\n",
        "    special_tokens=[\"[mask]\"],\n",
        ")\n",
        "\n",
        "# Get mask token id for masked language model\n",
        "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
        "\n",
        "\n",
        "def encode(texts):\n",
        "    encoded_texts = vectorize_layer(texts)\n",
        "    return encoded_texts.numpy()\n",
        "\n",
        "\n",
        "def get_masked_input_and_labels(encoded_texts):\n",
        "    # 15% BERT masking\n",
        "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
        "    # Do not mask special tokens\n",
        "    inp_mask[encoded_texts <= 2] = False\n",
        "    # Set targets to -1 by default, it means ignore\n",
        "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
        "    # Set labels for masked tokens\n",
        "    labels[inp_mask] = encoded_texts[inp_mask]\n",
        "\n",
        "    # Prepare input\n",
        "    encoded_texts_masked = np.copy(encoded_texts)\n",
        "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
        "    # This means leaving 10% unchanged\n",
        "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
        "    encoded_texts_masked[\n",
        "        inp_mask_2mask\n",
        "    ] = mask_token_id  # mask token is the last in the dict\n",
        "\n",
        "    # Set 10% to a random token\n",
        "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
        "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
        "        3, mask_token_id, inp_mask_2random.sum()\n",
        "    )\n",
        "\n",
        "    # Prepare sample_weights to pass to .fit() method\n",
        "    sample_weights = np.ones(labels.shape)\n",
        "    sample_weights[labels == -1] = 0\n",
        "\n",
        "    # y_labels would be same as encoded_texts i.e input tokens\n",
        "    y_labels = np.copy(encoded_texts)\n",
        "\n",
        "    return encoded_texts_masked, y_labels, sample_weights\n",
        "\n",
        "\n",
        "# We have 25000 examples for training\n",
        "x_train = encode(train_df.review.values)  # encode reviews with vectorizer\n",
        "y_train = train_df.sentiment.values\n",
        "train_classifier_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .shuffle(1000)\n",
        "    .batch(config.BATCH_SIZE)\n",
        ")\n",
        "\n",
        "# We have 25000 examples for testing\n",
        "x_test = encode(test_df.review.values)\n",
        "y_test = test_df.sentiment.values\n",
        "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
        "    config.BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Build dataset for end to end model input (will be used at the end)\n",
        "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_df.review.values, y_test)\n",
        ").batch(config.BATCH_SIZE)\n",
        "\n",
        "# Prepare data for masked language model\n",
        "x_all_review = encode(all_data.review.values)\n",
        "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
        "    x_all_review\n",
        ")\n",
        "\n",
        "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_masked_train, y_masked_labels, sample_weights)\n",
        ")\n",
        "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOGb78ltv6K-",
        "outputId": "45c902a9-de65-4134-89b8-a2193f55d04c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(None, 256), dtype=tf.int64, name=None), TensorSpec(shape=(None, 256), dtype=tf.int64, name=None), TensorSpec(shape=(None, 256), dtype=tf.float64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "mlm_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create BERT model (Pretraining Model) for masked language modeling\n",
        "\n",
        "We will create a BERT-like pretraining model architecture\n",
        "using the `MultiHeadAttention` layer.\n",
        "It will take token ids as inputs (including masked tokens)\n",
        "and it will predict the correct ids for the masked input tokens."
      ],
      "metadata": {
        "id": "trpShpIusIUZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Mgn64vJ9wiKP"
      },
      "outputs": [],
      "source": [
        "def bert_module(query, key, value, i):\n",
        "    # Multi headed self-attention\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=config.NUM_HEAD,\n",
        "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
        "        name=\"encoder_{}/multiheadattention\".format(i),\n",
        "    )(query, key, value)\n",
        "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
        "        attention_output\n",
        "    )\n",
        "    attention_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
        "    )(query + attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ffn = keras.Sequential(\n",
        "        [\n",
        "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
        "            layers.Dense(config.EMBED_DIM),\n",
        "        ],\n",
        "        name=\"encoder_{}/ffn\".format(i),\n",
        "    )\n",
        "    ffn_output = ffn(attention_output)\n",
        "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
        "        ffn_output\n",
        "    )\n",
        "    sequence_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
        "    )(attention_output + ffn_output)\n",
        "    return sequence_output\n",
        "\n",
        "\n",
        "def get_pos_encoding_matrix(max_len, d_emb):\n",
        "    pos_enc = np.array(\n",
        "        [\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
        "            if pos != 0\n",
        "            else np.zeros(d_emb)\n",
        "            for pos in range(max_len)\n",
        "        ]\n",
        "    )\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
        "    return pos_enc\n",
        "\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "    reduction=tf.keras.losses.Reduction.NONE\n",
        ")\n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "\n",
        "class MaskedLanguageModel(tf.keras.Model):\n",
        "    def train_step(self, inputs):\n",
        "        if len(inputs) == 3:\n",
        "            features, labels, sample_weight = inputs\n",
        "        else:\n",
        "            features, labels = inputs\n",
        "            sample_weight = None\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(features, training=True)\n",
        "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Compute our own metrics\n",
        "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We list our `Metric` objects here so that `reset_states()` can be\n",
        "        # called automatically at the start of each epoch\n",
        "        # or at the start of `evaluate()`.\n",
        "        # If you don't implement this property, you have to call\n",
        "        # `reset_states()` yourself at the time of your choosing.\n",
        "        return [loss_tracker]\n",
        "\n",
        "\n",
        "def create_masked_language_bert_model():\n",
        "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
        "\n",
        "    word_embeddings = layers.Embedding(\n",
        "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
        "    )(inputs)\n",
        "    position_embeddings = layers.Embedding(\n",
        "        input_dim=config.MAX_LEN,\n",
        "        output_dim=config.EMBED_DIM,\n",
        "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
        "        name=\"position_embedding\",\n",
        "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
        "    embeddings = word_embeddings + position_embeddings\n",
        "\n",
        "    encoder_output = embeddings\n",
        "    for i in range(config.NUM_LAYERS):\n",
        "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
        "\n",
        "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
        "        encoder_output\n",
        "    )\n",
        "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    mlm_model.compile(optimizer=optimizer)\n",
        "    return mlm_model\n",
        "\n",
        "\n",
        "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
        "token2id = {y: x for x, y in id2token.items()}\n",
        "\n",
        "\n",
        "class MaskedTextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self, sample_tokens, top_k=5):\n",
        "        self.sample_tokens = sample_tokens\n",
        "        self.k = top_k\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
        "\n",
        "    def convert_ids_to_tokens(self, id):\n",
        "        return id2token[id]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        prediction = self.model.predict(self.sample_tokens)\n",
        "\n",
        "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
        "        masked_index = masked_index[1]\n",
        "        mask_prediction = prediction[0][masked_index]\n",
        "\n",
        "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
        "        values = mask_prediction[0][top_indices]\n",
        "\n",
        "        for i in range(len(top_indices)):\n",
        "            p = top_indices[i]\n",
        "            v = values[i]\n",
        "            tokens = np.copy(sample_tokens[0])\n",
        "            tokens[masked_index[0]] = p\n",
        "            result = {\n",
        "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
        "                \"prediction\": self.decode(tokens),\n",
        "                \"probability\": v,\n",
        "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
        "            }\n",
        "            pprint(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRySRpbrxWqu",
        "outputId": "98860727-0bdc-4a0a-8e05-f26903cc6efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"masked_bert_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 256)]                0         []                            \n",
            "                                                                                                  \n",
            " word_embedding (Embedding)  (None, 256, 128)             3840000   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.add (TFOpLambda)    (None, 256, 128)             0         ['word_embedding[0][0]']      \n",
            "                                                                                                  \n",
            " encoder_0/multiheadattenti  (None, 256, 128)             66048     ['tf.math.add[0][0]',         \n",
            " on (MultiHeadAttention)                                             'tf.math.add[0][0]',         \n",
            "                                                                     'tf.math.add[0][0]']         \n",
            "                                                                                                  \n",
            " encoder_0/att_dropout (Dro  (None, 256, 128)             0         ['encoder_0/multiheadattention\n",
            " pout)                                                              [0][0]']                      \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 256, 128)             0         ['tf.math.add[0][0]',         \n",
            " Lambda)                                                             'encoder_0/att_dropout[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " encoder_0/att_layernormali  (None, 256, 128)             256       ['tf.__operators__.add[0][0]']\n",
            " zation (LayerNormalization                                                                       \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " encoder_0/ffn (Sequential)  (None, 256, 128)             33024     ['encoder_0/att_layernormaliza\n",
            "                                                                    tion[0][0]']                  \n",
            "                                                                                                  \n",
            " encoder_0/ffn_dropout (Dro  (None, 256, 128)             0         ['encoder_0/ffn[0][0]']       \n",
            " pout)                                                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TF  (None, 256, 128)             0         ['encoder_0/att_layernormaliza\n",
            " OpLambda)                                                          tion[0][0]',                  \n",
            "                                                                     'encoder_0/ffn_dropout[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " encoder_0/ffn_layernormali  (None, 256, 128)             256       ['tf.__operators__.add_1[0][0]\n",
            " zation (LayerNormalization                                         ']                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " mlm_cls (Dense)             (None, 256, 30000)           3870000   ['encoder_0/ffn_layernormaliza\n",
            "                                                                    tion[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7809584 (29.79 MB)\n",
            "Trainable params: 7809584 (29.79 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
        "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
        "\n",
        "bert_masked_model = create_masked_language_bert_model()\n",
        "bert_masked_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSyEDsEhxoFl",
        "outputId": "f31dd202-937c-43ee-a464-7c6a1884f0a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 0s 302ms/step\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'i',\n",
            " 'prediction': 'i have watched this i and it was awesome',\n",
            " 'probability': 0.065962255}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'this',\n",
            " 'prediction': 'i have watched this this and it was awesome',\n",
            " 'probability': 0.056016315}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.03828829}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'a',\n",
            " 'prediction': 'i have watched this a and it was awesome',\n",
            " 'probability': 0.029539404}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'it',\n",
            " 'prediction': 'i have watched this it and it was awesome',\n",
            " 'probability': 0.02796124}\n",
            "1563/1563 [==============================] - 229s 140ms/step - loss: 6.9963\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.2960384}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'is',\n",
            " 'prediction': 'i have watched this is and it was awesome',\n",
            " 'probability': 0.16906296}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.15212104}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'one',\n",
            " 'prediction': 'i have watched this one and it was awesome',\n",
            " 'probability': 0.046166953}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'was',\n",
            " 'prediction': 'i have watched this was and it was awesome',\n",
            " 'probability': 0.03143475}\n",
            "1563/1563 [==============================] - 207s 132ms/step - loss: 6.4703\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.34502888}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.14441633}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'is',\n",
            " 'prediction': 'i have watched this is and it was awesome',\n",
            " 'probability': 0.10515543}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'one',\n",
            " 'prediction': 'i have watched this one and it was awesome',\n",
            " 'probability': 0.05378418}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'was',\n",
            " 'prediction': 'i have watched this was and it was awesome',\n",
            " 'probability': 0.033776496}\n",
            "1563/1563 [==============================] - 206s 132ms/step - loss: 5.9187\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.46279964}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.22937068}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'one',\n",
            " 'prediction': 'i have watched this one and it was awesome',\n",
            " 'probability': 0.08119075}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'is',\n",
            " 'prediction': 'i have watched this is and it was awesome',\n",
            " 'probability': 0.03930868}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'was',\n",
            " 'prediction': 'i have watched this was and it was awesome',\n",
            " 'probability': 0.015914276}\n",
            "1563/1563 [==============================] - 206s 132ms/step - loss: 5.5237\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.47609314}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.3319434}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'one',\n",
            " 'prediction': 'i have watched this one and it was awesome',\n",
            " 'probability': 0.018316424}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'episode',\n",
            " 'prediction': 'i have watched this episode and it was awesome',\n",
            " 'probability': 0.016008157}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'series',\n",
            " 'prediction': 'i have watched this series and it was awesome',\n",
            " 'probability': 0.0073277894}\n",
            "1563/1563 [==============================] - 206s 132ms/step - loss: 5.1079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
        "bert_masked_model.save(\"bert_mlm_imdb.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune a sentiment classification model\n",
        "\n",
        "We will fine-tune our self-supervised model on a downstream task of sentiment classification.\n",
        "To do this, let's create a classifier by adding a pooling layer and a `Dense` layer on top of the\n",
        "pretrained BERT features."
      ],
      "metadata": {
        "id": "dq7Ew1GGsa3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlm_model = keras.models.load_model(\"bert_mlm_imdb.h5\", custom_objects = {\"MaskedLanguageModel\" : MaskedLanguageModel})\n",
        "pretrained_bert_model = tf.keras.Model(\n",
        "        mlm_model.input, mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output)\n",
        "\n",
        "pretrained_bert_model.trainable = False\n",
        "\n",
        "def create_classifier_bert_model():\n",
        "    inputs = keras.Input((config.MAX_LEN,), dtype = tf.int64)\n",
        "    sequence_output = pretrained_bert_model(inputs)\n",
        "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
        "    hidden_layer = layers.Dense(64, activation = \"relu\")(pooled_output)\n",
        "    outputs = layers.Dense(1, activation = \"sigmoid\")(hidden_layer)\n",
        "    classifier_model = keras.Model(inputs, outputs, name = \"classification\")\n",
        "    classifier_model.compile(\n",
        "        optimizer = keras.optimizers.Adam(),\n",
        "        loss   = \"binary_crossentropy\",\n",
        "        metrics = [\"accuracy\"])\n",
        "    return classifier_model\n",
        "\n",
        "\n",
        "classifier_model = create_classifier_bert_model()\n",
        "classifier_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6pG1AI-zzkv",
        "outputId": "77c3e9f7-5065-4ab6-e4ac-64b5eae088cb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 256)]             0         \n",
            "                                                                 \n",
            " model_1 (Functional)        (None, 256, 128)          3939584   \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Gl  (None, 128)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3947905 (15.06 MB)\n",
            "Trainable params: 8321 (32.50 KB)\n",
            "Non-trainable params: 3939584 (15.03 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the classifier with frozen BERT stage\n",
        "classifier_model.fit(\n",
        "    train_classifier_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_classifier_ds,\n",
        ")\n",
        "\n",
        "# Unfreeze the BERT model for fine-tuning\n",
        "pretrained_bert_model.trainable = True\n",
        "optimizer = keras.optimizers.Adam()\n",
        "classifier_model.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "classifier_model.fit(\n",
        "    train_classifier_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_classifier_ds,\n",
        ")"
      ],
      "metadata": {
        "id": "ox4HOsh19Yzb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7368484-ea65-402c-dc04-2e335c870f4a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 17s 22ms/step - loss: 0.6722 - accuracy: 0.6058 - val_loss: 0.6405 - val_accuracy: 0.6326\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.6572 - accuracy: 0.6170 - val_loss: 0.6448 - val_accuracy: 0.6270\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 12s 15ms/step - loss: 0.6541 - accuracy: 0.6222 - val_loss: 0.6336 - val_accuracy: 0.6394\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 12s 15ms/step - loss: 0.6451 - accuracy: 0.6277 - val_loss: 0.6387 - val_accuracy: 0.6354\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 12s 16ms/step - loss: 0.6443 - accuracy: 0.6295 - val_loss: 0.7278 - val_accuracy: 0.5636\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 72s 88ms/step - loss: 0.4603 - accuracy: 0.7753 - val_loss: 0.3511 - val_accuracy: 0.8435\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.2794 - accuracy: 0.8828 - val_loss: 0.3229 - val_accuracy: 0.8646\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 29s 38ms/step - loss: 0.1418 - accuracy: 0.9450 - val_loss: 0.4258 - val_accuracy: 0.8570\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 25s 31ms/step - loss: 0.0566 - accuracy: 0.9816 - val_loss: 0.5792 - val_accuracy: 0.8423\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 23s 29ms/step - loss: 0.0359 - accuracy: 0.9872 - val_loss: 0.7165 - val_accuracy: 0.8361\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78f3a97084c0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an end-to-end model and evaluate it\n",
        "\n",
        "When you want to deploy a model, it's best if it already includes its preprocessing\n",
        "pipeline, so that you don't have to reimplement the preprocessing logic in your\n",
        "production environment. Let's create an end-to-end model that incorporates\n",
        "the `TextVectorization` layer, and let's evaluate. Our model will accept raw strings\n",
        "as input."
      ],
      "metadata": {
        "id": "7u2Ze3brszNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_end_to_end(model):\n",
        "    inputs_string = keras.Input(shape = (1,), dtype = \"string\")\n",
        "    indices = vectorize_layer(inputs_string)\n",
        "    outputs = model(indices)\n",
        "    end_to_end_model = keras.Model(inputs_string, outputs, name = \"end_to_end_model\")\n",
        "    end_to_end_model.compile(\n",
        "        optimizer = keras.optimizers.Adam(learning_rate = config.LR),\n",
        "        loss = \"binary_crossentropy\",\n",
        "        metrics = [\"accuracy\"])\n",
        "    return end_to_end_model\n",
        "\n",
        "\n",
        "end_to_end_classification_model = get_end_to_end(classifier_model)\n",
        "end_to_end_classification_model.evaluate(test_raw_classifier_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taJikDalkVMF",
        "outputId": "ac4ba6cd-cc23-47ef-d4cc-3aab11a784fb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 9s 10ms/step - loss: 0.7165 - accuracy: 0.8361\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7164883017539978, 0.836080014705658]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}