{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwiks9635/My_Neural_Network_Architecture/blob/main/Audio_Classification_with_Hugging_Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VvhKT5sLACc"
      },
      "source": [
        "#**Audio Classification with Hugging Face Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Identification of speech commands, also known as *keyword spotting* (KWS),\n",
        "is important from an engineering perspective for a wide range of applications,\n",
        "from indexing audio databases and indexing keywords, to running speech models locally\n",
        "on microcontrollers. Currently, many human-computer interfaces (HCI) like Google\n",
        "Assistant, Microsoft Cortana, Amazon Alexa, Apple Siri and others rely on keyword\n",
        "spotting. There is a significant amount of research in the field by all major companies,\n",
        "notably Google and Baidu.\n",
        "\n",
        "In the past decade, deep learning has led to significant performance\n",
        "gains on this task. Though low-level audio features extracted from raw audio like MFCC or\n",
        "mel-filterbanks have been used for decades, the design of these low-level features\n",
        "are [flawed by biases](https://arxiv.org/abs/2101.08596). Moreover, deep learning models\n",
        "trained on these low-level features can easily overfit to noise or signals irrelevant to the\n",
        "task.  This makes it is essential for any system to learn speech representations that make\n",
        "high-level information, such as acoustic and linguistic content, including phonemes,\n",
        "words, semantic meanings, tone, speaker characteristics from speech signals available to\n",
        "solve the downstream task. [Wav2Vec 2.0](https://arxiv.org/abs/2006.11477), which solves a\n",
        "self-supervised contrastive learning task to learn high-level speech representations,\n",
        "provides a great alternative to traditional low-level features for training deep learning\n",
        "models for KWS.\n",
        "\n",
        "In this notebook, we train the Wav2Vec 2.0 (base) model, built on the\n",
        "Hugging Face Transformers library, in an end-to-end fashion on the keyword spotting task and\n",
        "achieve state-of-the-art results on the Google Speech Commands Dataset."
      ],
      "metadata": {
        "id": "jL-yACtDNDsX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVxYmYWWAgkT"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets\n",
        "!pip install huggingface-hub\n",
        "!pip install joblib\n",
        "!pip install librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lDxQyFdVAnWF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Only log error messages\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "# Set random seed\n",
        "tf.keras.utils.set_random_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZYuEDbrcAuGb"
      },
      "outputs": [],
      "source": [
        "# Maximum duration of the input audio file we feed to our Wav2Vec 2.0 model.\n",
        "MAX_DURATION = 1\n",
        "# Sampling rate is the number of samples of audio recorded every second\n",
        "SAMPLING_RATE = 16000\n",
        "BATCH_SIZE = 16  # Batch-size for training and evaluating our model.\n",
        "NUM_CLASSES = 10  # Number of classes our dataset will have (11 in our case).\n",
        "HIDDEN_DIM = 768  # Dimension of our model output (768 in case of Wav2Vec 2.0 - Base).\n",
        "MAX_SEQ_LENGTH = MAX_DURATION * SAMPLING_RATE  # Maximum length of the input audio file.\n",
        "# Wav2Vec 2.0 results in an output frequency with a stride of about 20ms.\n",
        "MAX_FRAMES = 49\n",
        "MAX_EPOCHS = 5  # Maximum number of training epochs.\n",
        "\n",
        "MODEL_CHECKPOINT = \"facebook/wav2vec2-base\"  # Name of pretrained model from Hugging Face Model Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Google Speech Commands Dataset\n",
        "\n",
        "\n",
        "\n",
        "We now download the [Google Speech Commands V1 Dataset](https://arxiv.org/abs/1804.03209),\n",
        "a popular benchmark for training and evaluating deep learning models built for solving the KWS task.\n",
        "The dataset consists of a total of 60,973 audio files, each of 1 second duration,\n",
        "divided into ten classes of keywords (\"Yes\", \"No\", \"Up\", \"Down\", \"Left\", \"Right\", \"On\",\n",
        "\"Off\", \"Stop\", and \"Go\"), a class for silence, and an unknown class to include the false\n",
        "positive. We load the dataset from [Hugging Face Datasets](https://github.com/huggingface/datasets).\n",
        "This can be easily done with the `load_dataset` function."
      ],
      "metadata": {
        "id": "hU52FZyvNObY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8OVdw2KBDqo",
        "outputId": "280c19e8-7503-4f86-ba70-7d799c38ac30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1429: FutureWarning: The repository for superb contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/superb\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "speech_commands_v1 = load_dataset(\"superb\", \"ks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has the following fields:\n",
        "\n",
        "- **file**: the path to the raw .wav file of the audio\n",
        "- **audio**: the audio file sampled at 16kHz\n",
        "- **label**: label ID of the audio utterance"
      ],
      "metadata": {
        "id": "ZyzTUGWkNUbe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCqmvF9tBae2",
        "outputId": "f09c2ef8-26ca-41f3-8531-f31568daed5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['file', 'audio', 'label'],\n",
            "        num_rows: 51094\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['file', 'audio', 'label'],\n",
            "        num_rows: 6798\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['file', 'audio', 'label'],\n",
            "        num_rows: 3081\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(speech_commands_v1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing\n",
        "\n",
        "\n",
        "\n",
        "For the sake of demonstrating the workflow, in this notebook we only take\n",
        "small stratified balanced splits (50%) of the train as our training and test sets.\n",
        "We can easily split the dataset using the `train_test_split` method which expects\n",
        "the split size and the name of the column relative to which you want to stratify.\n",
        "\n",
        "Post splitting the dataset, we remove the `unknown` and `silence` classes and only\n",
        "focus on the ten main classes. The `filter` method does that easily for you.\n",
        "\n",
        "Next we sample our train and test splits to a multiple of the `BATCH_SIZE` to\n",
        "facilitate smooth training and inference. You can achieve that using the `select`\n",
        "method which expects the indices of the samples you want to keep. Rest all are\n",
        "discarded."
      ],
      "metadata": {
        "id": "CacLmNfYNacF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cboepc0sDwv-",
        "outputId": "8d3df17f-4387-4cb7-bfcd-179e4c1509df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['file', 'audio', 'label'],\n",
            "        num_rows: 25536\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['file', 'audio', 'label'],\n",
            "        num_rows: 25536\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "speech_commands_v1 = speech_commands_v1[\"train\"].train_test_split(\n",
        "    train_size=0.5, test_size=0.5, stratify_by_column=\"label\"\n",
        ")\n",
        "\n",
        "speech_commands_v1 = speech_commands_v1.filter(\n",
        "    lambda x: x[\"label\"]\n",
        "    != (\n",
        "        speech_commands_v1[\"train\"].features[\"label\"].names.index(\"_unknown_\")\n",
        "        and speech_commands_v1[\"train\"].features[\"label\"].names.index(\"_silence_\")\n",
        "    )\n",
        ")\n",
        "\n",
        "speech_commands_v1[\"train\"] = speech_commands_v1[\"train\"].select(\n",
        "    [i for i in range((len(speech_commands_v1[\"train\"]) // BATCH_SIZE) * BATCH_SIZE)]\n",
        ")\n",
        "speech_commands_v1[\"test\"] = speech_commands_v1[\"test\"].select(\n",
        "    [i for i in range((len(speech_commands_v1[\"test\"]) // BATCH_SIZE) * BATCH_SIZE)]\n",
        ")\n",
        "\n",
        "print(speech_commands_v1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, you can check the actual labels corresponding to each label ID."
      ],
      "metadata": {
        "id": "YxMQkAaXNifp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzQ7NuReMeHp",
        "outputId": "976dadab-fb12-4e5b-8bcd-fac35c4c9e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'0': 'yes', '1': 'no', '2': 'up', '3': 'down', '4': 'left', '5': 'right', '6': 'on', '7': 'off', '8': 'stop', '9': 'go', '10': '_silence_', '11': '_unknown_'}\n"
          ]
        }
      ],
      "source": [
        "labels = speech_commands_v1[\"train\"].features[\"label\"].names\n",
        "\n",
        "label2id = dict()\n",
        "id2label = dict()\n",
        "\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label\n",
        "print(id2label)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can feed the audio utterance samples to our model, we need to\n",
        "pre-process them. This is done by a Hugging Face Transformers \"Feature Extractor\"\n",
        "which will (as the name indicates) re-sample your inputs to the sampling rate\n",
        "the model expects (in-case they exist with a different sampling rate), as well\n",
        "as generate the other inputs that model requires.\n",
        "\n",
        "To do all of this, we instantiate our `Feature Extractor` with the\n",
        "`AutoFeatureExtractor.from_pretrained`, which will ensure:\n",
        "\n",
        "We get a `Feature Extractor` that corresponds to the model architecture we want to use.\n",
        "We download the config that was used when pretraining this specific checkpoint.\n",
        "This will be cached so that it's not downloaded again the next time we run the cell.\n",
        "\n",
        "The `from_pretrained()` method expects the name of a model from the Hugging Face Hub. This is\n",
        "exactly similar to `MODEL_CHECKPOINT` and we just pass that.\n",
        "\n",
        "We write a simple function that helps us in the pre-processing that is compatible\n",
        "with Hugging Face Datasets. To summarize, our pre-processing function should:\n",
        "\n",
        "- Call the audio column to load and if necessary resample the audio file.\n",
        "- Check the sampling rate of the audio file matches the sampling rate of the audio data a\n",
        "model was pretrained with. You can find this information on the Wav2Vec 2.0 model card.\n",
        "- Set a maximum input length so longer inputs are batched without being truncated."
      ],
      "metadata": {
        "id": "fGxPiVaiNm8k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNuCOQbfOQcg",
        "outputId": "c8c2ff82-3f4f-4629-c705-8367e6dcd4a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:365: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoFeatureExtractor\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
        "    MODEL_CHECKPOINT, return_attention_mask = True)\n",
        "\n",
        "\n",
        "def preprocess_function(example):\n",
        "    audio_array = [x[\"array\"] for x in example[\"audio\"]]\n",
        "    inputs = feature_extractor(\n",
        "        audio_array,\n",
        "        sampling_rate = feature_extractor.sampling_rate,\n",
        "        max_length = MAX_SEQ_LENGTH,\n",
        "        truncation = True,\n",
        "        padding = True)\n",
        "    return inputs\n",
        "\n",
        "\n",
        "# This line with pre-process our speech_commands_v1 dataset. We also remove the \"audio\"\n",
        "# and \"file\" columns as they will be of no use to us while training.\n",
        "processed_speech_commands_v1 = speech_commands_v1.map(\n",
        "    preprocess_function, remove_columns=[\"audio\", \"file\"], batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9HdMhSvxUx1z"
      },
      "outputs": [],
      "source": [
        "train = processed_speech_commands_v1[\"train\"].shuffle(seed = 42).with_format(\"numpy\")[:]\n",
        "test = processed_speech_commands_v1[\"test\"].shuffle(seed = 42).with_format(\"numpy\")[:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Wav2Vec 2.0 with Classification-Head\n",
        "\n",
        "\n",
        "\n",
        "We now define our model. To be precise, we define a Wav2Vec 2.0 model and add a\n",
        "Classification-Head on top to output a probability distribution of all classes for each\n",
        "input audio sample. Since the model might get complex we first define the Wav2Vec\n",
        "2.0 model with Classification-Head as a Keras layer and then build the model using that.\n",
        "\n",
        "We instantiate our main Wav2Vec 2.0 model using the `TFWav2Vec2Model` class. This will\n",
        "instantiate a model which will output 768 or 1024 dimensional embeddings according to\n",
        "the config you choose (BASE or LARGE). The `from_pretrained()` additionally helps you\n",
        "load pre-trained weights from the Hugging Face Model Hub. It will download the pre-trained weights\n",
        "together with the config corresponding to the name of the model you have mentioned when\n",
        "calling the method. For our task, we choose the BASE variant of the model that has\n",
        "just been pre-trained, since we fine-tune over it."
      ],
      "metadata": {
        "id": "rriBVMKwNtWW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "h8PsCGMXXpKd"
      },
      "outputs": [],
      "source": [
        "from transformers import TFWav2Vec2Model\n",
        "\n",
        "\n",
        "def mean_pool(hidden_states, feature_lengths):\n",
        "    attenion_mask = tf.sequence_mask(\n",
        "        feature_lengths, maxlen=MAX_FRAMES, dtype=tf.dtypes.int64\n",
        "    )\n",
        "    padding_mask = tf.cast(\n",
        "        tf.reverse(tf.cumsum(tf.reverse(attenion_mask, [-1]), -1), [-1]),\n",
        "        dtype=tf.dtypes.bool,\n",
        "    )\n",
        "    hidden_states = tf.where(\n",
        "        tf.broadcast_to(\n",
        "            tf.expand_dims(~padding_mask, -1), (BATCH_SIZE, MAX_FRAMES, HIDDEN_DIM)\n",
        "        ),\n",
        "        0.0,\n",
        "        hidden_states,\n",
        "    )\n",
        "    pooled_state = tf.math.reduce_sum(hidden_states, axis=1) / tf.reshape(\n",
        "        tf.math.reduce_sum(tf.cast(padding_mask, dtype=tf.dtypes.float32), axis=1),\n",
        "        [-1, 1],\n",
        "    )\n",
        "    return pooled_state\n",
        "\n",
        "\n",
        "class TFWav2Vec2ForAudioClassification(layers.Layer):\n",
        "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
        "\n",
        "    def __init__(self, model_checkpoint, num_classes):\n",
        "        super().__init__()\n",
        "        # Instantiate the Wav2Vec 2.0 model without the Classification-Head\n",
        "        self.wav2vec2 = TFWav2Vec2Model.from_pretrained(\n",
        "            model_checkpoint, apply_spec_augment=False, from_pt=True\n",
        "        )\n",
        "        self.pooling = layers.GlobalAveragePooling1D()\n",
        "        # Drop-out layer before the final Classification-Head\n",
        "        self.intermediate_layer_dropout = layers.Dropout(0.5)\n",
        "        # Classification-Head\n",
        "        self.final_layer = layers.Dense(num_classes, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # We take only the first output in the returned dictionary corresponding to the\n",
        "        # output of the last layer of Wav2vec 2.0\n",
        "        hidden_states = self.wav2vec2(inputs[\"input_values\"])[0]\n",
        "\n",
        "        # If attention mask does exist then mean-pool only un-masked output frames\n",
        "        if tf.is_tensor(inputs[\"attention_mask\"]):\n",
        "            # Get the length of each audio input by summing up the attention_mask\n",
        "            # (attention_mask = (BATCH_SIZE x MAX_SEQ_LENGTH) ∈ {1,0})\n",
        "            audio_lengths = tf.cumsum(inputs[\"attention_mask\"], -1)[:, -1]\n",
        "            # Get the number of Wav2Vec 2.0 output frames for each corresponding audio input\n",
        "            # length\n",
        "            feature_lengths = self.wav2vec2.wav2vec2._get_feat_extract_output_lengths(\n",
        "                audio_lengths\n",
        "            )\n",
        "            pooled_state = mean_pool(hidden_states, feature_lengths)\n",
        "        # If attention mask does not exist then mean-pool only all output frames\n",
        "        else:\n",
        "            pooled_state = self.pooling(hidden_states)\n",
        "\n",
        "        intermediate_state = self.intermediate_layer_dropout(pooled_state)\n",
        "        final_state = self.final_layer(intermediate_state)\n",
        "\n",
        "        return final_state"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building and Compiling the model\n",
        "\n",
        "\n",
        "\n",
        "We now build and compile our model. We use the `SparseCategoricalCrossentropy`\n",
        "to train our model since it is a classification task. Following much of literature\n",
        "we evaluate our model on the `accuracy` metric."
      ],
      "metadata": {
        "id": "-alcCfeeN0zb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RRS9Ear8XsJw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316,
          "referenced_widgets": [
            "294c5d7e9da543c6a26a2b23f2fdfeee",
            "061e3b29e88345b38a80c38d97d89b8c",
            "77e97a04c94345af85889f0d9575f14c",
            "f9a6c1178d0c45c794d24e6b7922c7e8",
            "c30cafb66eb24adaa0eb6ec0aa0f3cc5",
            "5b4930b2948f4be397c0dc178d844fdc",
            "12aa32d733454a88a56795d951311349",
            "03eb2675b1c84329afee56b68c978bb6",
            "6249680ca85b440cb065d7e458bd2424",
            "f2b5528fa3134854af409aa8660a5c0c",
            "c5fe75f7690f4925ba7d3496d1d6a5bf"
          ]
        },
        "outputId": "34b65eb9-5b5d-4ae3-94aa-b9b14d3dc2d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:365: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "294c5d7e9da543c6a26a2b23f2fdfeee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "TFWav2Vec2Model has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tune this model, you need a GPU or a TPU\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFWav2Vec2Model: ['quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_hid.bias', 'project_q.bias', 'project_hid.weight', 'project_q.weight', 'quantizer.weight_proj.bias']\n",
            "- This IS expected if you are initializing TFWav2Vec2Model from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFWav2Vec2Model from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFWav2Vec2Model were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWav2Vec2Model for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "def build_model():\n",
        "    # Model's input\n",
        "    inputs = {\n",
        "        \"input_values\": tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype=\"float32\"),\n",
        "        \"attention_mask\": tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype=\"int32\"),\n",
        "    }\n",
        "    # Instantiate the Wav2Vec 2.0 model with Classification-Head using the desired\n",
        "    # pre-trained checkpoint\n",
        "    wav2vec2_model = TFWav2Vec2ForAudioClassification(MODEL_CHECKPOINT, NUM_CLASSES)(\n",
        "        inputs\n",
        "    )\n",
        "    # Model\n",
        "    model = tf.keras.Model(inputs, wav2vec2_model)\n",
        "    # Loss\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    # Optimizer\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
        "    # Compile and return\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model\n",
        "\n",
        "Before we start training our model, we divide the inputs into its\n",
        "dependent and independent variables."
      ],
      "metadata": {
        "id": "mezdJ4vLN8_i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4AfpfxAtbWb9"
      },
      "outputs": [],
      "source": [
        "train_x = {x: y for x, y in train.items() if x != \"label\"}\n",
        "test_x = {x: y for x, y in test.items() if x != \"label\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOMilDimf-Sq"
      },
      "outputs": [],
      "source": [
        "model.fit(train_x, train[\"label\"],\n",
        "        validation_data = (test_x, test[\"label\"]),\n",
        "        batch_size = BATCH_SIZE,\n",
        "        epochs = MAX_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Now that we have trained our model, we predict the classes\n",
        "for audio samples in the test set using the `model.predict()` method! We see\n",
        "the model predictions are not that great as it has been trained on a very small\n",
        "number of samples for just 1 epoch. For best results, we recommend training on\n",
        "the complete dataset for at least 5 epochs!"
      ],
      "metadata": {
        "id": "dYtLmqRmOKzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(test_x)"
      ],
      "metadata": {
        "id": "5mauHt3aMWqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we try to infer the model we trained on a randomly sampled audio file.\n",
        "We hear the audio file and then also see how well our model was able to predict!"
      ],
      "metadata": {
        "id": "lScB466SOP6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "rand_int = random.randint(0, len(test_x))\n",
        "\n",
        "ipd.Audio(data=np.asarray(test_x[\"input_values\"][rand_int]), autoplay=True, rate=16000)\n",
        "\n",
        "print(\"Original Label is \", id2label[str(test[\"label\"][rand_int])])\n",
        "print(\"Predicted Label is \", id2label[str(np.argmax((preds[rand_int])))])"
      ],
      "metadata": {
        "id": "DXqbzqQAMXyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can push this model to Hugging Face Model Hub and also share it with all your friends,\n",
        "family, favorite pets: they can all load it with the identifier\n",
        "`\"your-username/the-name-you-picked\"`, for instance:\n",
        "\n",
        "```python\n",
        "model.push_to_hub(\"wav2vec2-ks\", organization=\"keras-io\")\n",
        "tokenizer.push_to_hub(\"wav2vec2-ks\", organization=\"keras-io\")\n",
        "```\n",
        "And after you push your model this is how you can load it in the future!\n",
        "\n",
        "```python\n",
        "from transformers import TFWav2Vec2Model\n",
        "\n",
        "model = TFWav2Vec2Model.from_pretrained(\"your-username/my-awesome-model\", from_pt=True)\n",
        "```"
      ],
      "metadata": {
        "id": "db5o1d7COVpB"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbbtJNKreTBp9dwASa6rHp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "294c5d7e9da543c6a26a2b23f2fdfeee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_061e3b29e88345b38a80c38d97d89b8c",
              "IPY_MODEL_77e97a04c94345af85889f0d9575f14c",
              "IPY_MODEL_f9a6c1178d0c45c794d24e6b7922c7e8"
            ],
            "layout": "IPY_MODEL_c30cafb66eb24adaa0eb6ec0aa0f3cc5"
          }
        },
        "061e3b29e88345b38a80c38d97d89b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b4930b2948f4be397c0dc178d844fdc",
            "placeholder": "​",
            "style": "IPY_MODEL_12aa32d733454a88a56795d951311349",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "77e97a04c94345af85889f0d9575f14c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03eb2675b1c84329afee56b68c978bb6",
            "max": 380267417,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6249680ca85b440cb065d7e458bd2424",
            "value": 380267417
          }
        },
        "f9a6c1178d0c45c794d24e6b7922c7e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2b5528fa3134854af409aa8660a5c0c",
            "placeholder": "​",
            "style": "IPY_MODEL_c5fe75f7690f4925ba7d3496d1d6a5bf",
            "value": " 380M/380M [00:04&lt;00:00, 64.9MB/s]"
          }
        },
        "c30cafb66eb24adaa0eb6ec0aa0f3cc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b4930b2948f4be397c0dc178d844fdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12aa32d733454a88a56795d951311349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03eb2675b1c84329afee56b68c978bb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6249680ca85b440cb065d7e458bd2424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2b5528fa3134854af409aa8660a5c0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5fe75f7690f4925ba7d3496d1d6a5bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}