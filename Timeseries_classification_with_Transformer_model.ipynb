{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwiks9635/My_Neural_Network_Architecture/blob/main/Timeseries_classification_with_Transformer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Timeseries classification with a Transformer model**"
      ],
      "metadata": {
        "id": "6VJG8UWInE7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This is the Transformer architecture from\n",
        "[Attention Is All You Need](https://arxiv.org/abs/1706.03762),\n",
        "applied to timeseries instead of natural language.\n",
        "\n",
        "This example requires TensorFlow 2.4 or higher.\n",
        "\n",
        "## Load the dataset\n",
        "\n",
        "We are going to use the same dataset and preprocessing as the\n",
        "[TimeSeries Classification from Scratch](https://keras.io/examples/timeseries/timeseries_classification_from_scratch)\n",
        "example."
      ],
      "metadata": {
        "id": "vBAXQQoH0d-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "\n",
        "def readucr(filename):\n",
        "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
        "    y = data[:, 0]\n",
        "    x = data[:, 1:]\n",
        "    return x, y.astype(int)\n",
        "\n",
        "\n",
        "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
        "\n",
        "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
        "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n",
        "\n",
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "n_classes = len(np.unique(y_train))\n",
        "\n",
        "idx = np.random.permutation(len(x_train))\n",
        "x_train = x_train[idx]\n",
        "y_train = y_train[idx]\n",
        "\n",
        "y_train[y_train == -1] = 0\n",
        "y_test[y_test == -1] = 0"
      ],
      "metadata": {
        "id": "iFSr_kFmnLmr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET95VHlZnZlM",
        "outputId": "5407b556-b915-4a1d-e3c8-37d7891b0bd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3601, 500, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "Our model processes a tensor of shape `(batch size, sequence length, features)`,\n",
        "where `sequence length` is the number of time steps and `features` is each input\n",
        "timeseries.\n",
        "\n",
        "You can replace your classification RNN layers with this one: the\n",
        "inputs are fully compatible!\n",
        "\n",
        "We include residual connections, layer normalization, and dropout.\n",
        "The resulting layer can be stacked multiple times.\n",
        "\n",
        "The projection layers are implemented through `keras.layers.Conv1D`."
      ],
      "metadata": {
        "id": "GS6cH_550nUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout = 0):\n",
        "    x = layers.MultiHeadAttention(key_dim = head_size, num_heads = num_heads, dropout = dropout)(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon = 1e-6)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    x = layers.Conv1D(filters = ff_dim, kernel_size = 1, activation = \"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters = inputs.shape[-1], kernel_size = 1)(x)\n",
        "    x = layers.LayerNormalization(epsilon = 1e-6)(x)\n",
        "    return x + res"
      ],
      "metadata": {
        "id": "s2W0sx8Wn-g2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main part of our model is now complete. We can stack multiple of those\n",
        "`transformer_encoder` blocks and we can also proceed to add the final\n",
        "Multi-Layer Perceptron classification head. Apart from a stack of `Dense`\n",
        "layers, we need to reduce the output tensor of the `TransformerEncoder` part of\n",
        "our model down to a vector of features for each data point in the current\n",
        "batch. A common way to achieve this is to use a pooling layer. For\n",
        "this example, a `GlobalAveragePooling1D` layer is sufficient."
      ],
      "metadata": {
        "id": "l2YBIpcR0qbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
        "    return keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "GTMTsXLosVkz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluate"
      ],
      "metadata": {
        "id": "PWiz7l260vVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25)"
      ],
      "metadata": {
        "id": "OhH5liFVu7dd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer = keras.optimizers.Adam(learning_rate = 1e-4),\n",
        "    loss = \"sparse_categorical_crossentropy\",\n",
        "    metrics = [\"sparse_categorical_accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience = 10, restore_best_weights = True)]\n",
        "\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    validation_split = 0.2,\n",
        "    epochs = 150,\n",
        "    batch_size = 32,\n",
        "    callbacks = callbacks)\n",
        "\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pUrQda2vCya",
        "outputId": "f4cc7feb-43a6-4854-dd75-1ed5b311f7e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 500, 1)]             0         []                            \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 500, 1)               7169      ['input_1[0][0]',             \n",
            " iHeadAttention)                                                     'input_1[0][0]']             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 500, 1)               0         ['multi_head_attention[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 500, 1)               2         ['dropout[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 500, 4)               8         ['layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 500, 4)               0         ['conv1d[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)           (None, 500, 1)               5         ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 500, 1)               2         ['conv1d_1[0][0]']            \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 500, 1)               0         ['layer_normalization[0][0]', \n",
            " Lambda)                                                             'input_1[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TF  (None, 500, 1)               0         ['layer_normalization_1[0][0]'\n",
            " OpLambda)                                                          , 'tf.__operators__.add[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 500, 1)               7169      ['tf.__operators__.add_1[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'tf.__operators__.add_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 500, 1)               0         ['multi_head_attention_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 500, 1)               2         ['dropout_2[0][0]']           \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 500, 4)               8         ['layer_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 500, 4)               0         ['conv1d_2[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, 500, 1)               5         ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 500, 1)               2         ['conv1d_3[0][0]']            \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TF  (None, 500, 1)               0         ['layer_normalization_2[0][0]'\n",
            " OpLambda)                                                          , 'tf.__operators__.add_1[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TF  (None, 500, 1)               0         ['layer_normalization_3[0][0]'\n",
            " OpLambda)                                                          , 'tf.__operators__.add_2[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (Mu  (None, 500, 1)               7169      ['tf.__operators__.add_3[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'tf.__operators__.add_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 500, 1)               0         ['multi_head_attention_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 500, 1)               2         ['dropout_4[0][0]']           \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)           (None, 500, 4)               8         ['layer_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 500, 4)               0         ['conv1d_4[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)           (None, 500, 1)               5         ['dropout_5[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 500, 1)               2         ['conv1d_5[0][0]']            \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TF  (None, 500, 1)               0         ['layer_normalization_4[0][0]'\n",
            " OpLambda)                                                          , 'tf.__operators__.add_3[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TF  (None, 500, 1)               0         ['layer_normalization_5[0][0]'\n",
            " OpLambda)                                                          , 'tf.__operators__.add_4[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (Mu  (None, 500, 1)               7169      ['tf.__operators__.add_5[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'tf.__operators__.add_5[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)         (None, 500, 1)               0         ['multi_head_attention_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_6 (Lay  (None, 500, 1)               2         ['dropout_6[0][0]']           \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)           (None, 500, 4)               8         ['layer_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)         (None, 500, 4)               0         ['conv1d_6[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)           (None, 500, 1)               5         ['dropout_7[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_7 (Lay  (None, 500, 1)               2         ['conv1d_7[0][0]']            \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TF  (None, 500, 1)               0         ['layer_normalization_6[0][0]'\n",
            " OpLambda)                                                          , 'tf.__operators__.add_5[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TF  (None, 500, 1)               0         ['layer_normalization_7[0][0]'\n",
            " OpLambda)                                                          , 'tf.__operators__.add_6[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d (  (None, 500)                  0         ['tf.__operators__.add_7[0][0]\n",
            " GlobalAveragePooling1D)                                            ']                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 128)                  64128     ['global_average_pooling1d[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)         (None, 128)                  0         ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 2)                    258       ['dropout_8[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 93130 (363.79 KB)\n",
            "Trainable params: 93130 (363.79 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/150\n",
            "90/90 [==============================] - 32s 246ms/step - loss: 0.7516 - sparse_categorical_accuracy: 0.6184 - val_loss: 0.6608 - val_sparse_categorical_accuracy: 0.6311\n",
            "Epoch 2/150\n",
            "90/90 [==============================] - 22s 240ms/step - loss: 0.7073 - sparse_categorical_accuracy: 0.6417 - val_loss: 0.6249 - val_sparse_categorical_accuracy: 0.6602\n",
            "Epoch 3/150\n",
            "90/90 [==============================] - 21s 233ms/step - loss: 0.6723 - sparse_categorical_accuracy: 0.6545 - val_loss: 0.6132 - val_sparse_categorical_accuracy: 0.6588\n",
            "Epoch 4/150\n",
            "90/90 [==============================] - 21s 234ms/step - loss: 0.6152 - sparse_categorical_accuracy: 0.6882 - val_loss: 0.5997 - val_sparse_categorical_accuracy: 0.6685\n",
            "Epoch 5/150\n",
            "90/90 [==============================] - 21s 234ms/step - loss: 0.5817 - sparse_categorical_accuracy: 0.7031 - val_loss: 0.5902 - val_sparse_categorical_accuracy: 0.6796\n",
            "Epoch 6/150\n",
            "90/90 [==============================] - 21s 236ms/step - loss: 0.5789 - sparse_categorical_accuracy: 0.7247 - val_loss: 0.5732 - val_sparse_categorical_accuracy: 0.6879\n",
            "Epoch 7/150\n",
            "90/90 [==============================] - 22s 244ms/step - loss: 0.5451 - sparse_categorical_accuracy: 0.7288 - val_loss: 0.5624 - val_sparse_categorical_accuracy: 0.6963\n",
            "Epoch 8/150\n",
            "90/90 [==============================] - 21s 236ms/step - loss: 0.5334 - sparse_categorical_accuracy: 0.7292 - val_loss: 0.5510 - val_sparse_categorical_accuracy: 0.7184\n",
            "Epoch 9/150\n",
            "90/90 [==============================] - 22s 246ms/step - loss: 0.5104 - sparse_categorical_accuracy: 0.7545 - val_loss: 0.5432 - val_sparse_categorical_accuracy: 0.7157\n",
            "Epoch 10/150\n",
            "90/90 [==============================] - 21s 238ms/step - loss: 0.4976 - sparse_categorical_accuracy: 0.7580 - val_loss: 0.5360 - val_sparse_categorical_accuracy: 0.7226\n",
            "Epoch 11/150\n",
            "90/90 [==============================] - 21s 238ms/step - loss: 0.4695 - sparse_categorical_accuracy: 0.7833 - val_loss: 0.5245 - val_sparse_categorical_accuracy: 0.7434\n",
            "Epoch 12/150\n",
            "90/90 [==============================] - 21s 239ms/step - loss: 0.4577 - sparse_categorical_accuracy: 0.7830 - val_loss: 0.5187 - val_sparse_categorical_accuracy: 0.7420\n",
            "Epoch 13/150\n",
            "90/90 [==============================] - 22s 247ms/step - loss: 0.4465 - sparse_categorical_accuracy: 0.7906 - val_loss: 0.5140 - val_sparse_categorical_accuracy: 0.7420\n",
            "Epoch 14/150\n",
            "90/90 [==============================] - 22s 247ms/step - loss: 0.4513 - sparse_categorical_accuracy: 0.7826 - val_loss: 0.5096 - val_sparse_categorical_accuracy: 0.7365\n",
            "Epoch 15/150\n",
            "90/90 [==============================] - 22s 247ms/step - loss: 0.4430 - sparse_categorical_accuracy: 0.7934 - val_loss: 0.4989 - val_sparse_categorical_accuracy: 0.7614\n",
            "Epoch 16/150\n",
            "90/90 [==============================] - 22s 248ms/step - loss: 0.4267 - sparse_categorical_accuracy: 0.8111 - val_loss: 0.4912 - val_sparse_categorical_accuracy: 0.7587\n",
            "Epoch 17/150\n",
            "90/90 [==============================] - 22s 240ms/step - loss: 0.4087 - sparse_categorical_accuracy: 0.8170 - val_loss: 0.4855 - val_sparse_categorical_accuracy: 0.7670\n",
            "Epoch 18/150\n",
            "90/90 [==============================] - 22s 239ms/step - loss: 0.4009 - sparse_categorical_accuracy: 0.8233 - val_loss: 0.4786 - val_sparse_categorical_accuracy: 0.7698\n",
            "Epoch 19/150\n",
            "90/90 [==============================] - 22s 240ms/step - loss: 0.3908 - sparse_categorical_accuracy: 0.8257 - val_loss: 0.4709 - val_sparse_categorical_accuracy: 0.7809\n",
            "Epoch 20/150\n",
            "90/90 [==============================] - 22s 240ms/step - loss: 0.3802 - sparse_categorical_accuracy: 0.8365 - val_loss: 0.4678 - val_sparse_categorical_accuracy: 0.7836\n",
            "Epoch 21/150\n",
            "90/90 [==============================] - 22s 247ms/step - loss: 0.3849 - sparse_categorical_accuracy: 0.8385 - val_loss: 0.4683 - val_sparse_categorical_accuracy: 0.7864\n",
            "Epoch 22/150\n",
            "90/90 [==============================] - 22s 248ms/step - loss: 0.3698 - sparse_categorical_accuracy: 0.8465 - val_loss: 0.4672 - val_sparse_categorical_accuracy: 0.7836\n",
            "Epoch 23/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.3670 - sparse_categorical_accuracy: 0.8417 - val_loss: 0.4595 - val_sparse_categorical_accuracy: 0.7933\n",
            "Epoch 24/150\n",
            "90/90 [==============================] - 22s 242ms/step - loss: 0.3489 - sparse_categorical_accuracy: 0.8556 - val_loss: 0.4548 - val_sparse_categorical_accuracy: 0.7933\n",
            "Epoch 25/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.3486 - sparse_categorical_accuracy: 0.8486 - val_loss: 0.4562 - val_sparse_categorical_accuracy: 0.7947\n",
            "Epoch 26/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.3377 - sparse_categorical_accuracy: 0.8660 - val_loss: 0.4530 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 27/150\n",
            "90/90 [==============================] - 22s 250ms/step - loss: 0.3289 - sparse_categorical_accuracy: 0.8653 - val_loss: 0.4453 - val_sparse_categorical_accuracy: 0.7892\n",
            "Epoch 28/150\n",
            "90/90 [==============================] - 22s 250ms/step - loss: 0.3283 - sparse_categorical_accuracy: 0.8694 - val_loss: 0.4424 - val_sparse_categorical_accuracy: 0.7920\n",
            "Epoch 29/150\n",
            "90/90 [==============================] - 22s 250ms/step - loss: 0.3180 - sparse_categorical_accuracy: 0.8781 - val_loss: 0.4428 - val_sparse_categorical_accuracy: 0.7864\n",
            "Epoch 30/150\n",
            "90/90 [==============================] - 22s 250ms/step - loss: 0.3139 - sparse_categorical_accuracy: 0.8726 - val_loss: 0.4426 - val_sparse_categorical_accuracy: 0.7836\n",
            "Epoch 31/150\n",
            "90/90 [==============================] - 22s 250ms/step - loss: 0.3070 - sparse_categorical_accuracy: 0.8781 - val_loss: 0.4359 - val_sparse_categorical_accuracy: 0.8017\n",
            "Epoch 32/150\n",
            "90/90 [==============================] - 22s 241ms/step - loss: 0.3021 - sparse_categorical_accuracy: 0.8851 - val_loss: 0.4361 - val_sparse_categorical_accuracy: 0.7975\n",
            "Epoch 33/150\n",
            "90/90 [==============================] - 22s 242ms/step - loss: 0.3074 - sparse_categorical_accuracy: 0.8781 - val_loss: 0.4307 - val_sparse_categorical_accuracy: 0.8003\n",
            "Epoch 34/150\n",
            "90/90 [==============================] - 22s 242ms/step - loss: 0.2939 - sparse_categorical_accuracy: 0.8851 - val_loss: 0.4273 - val_sparse_categorical_accuracy: 0.8058\n",
            "Epoch 35/150\n",
            "90/90 [==============================] - 22s 241ms/step - loss: 0.2864 - sparse_categorical_accuracy: 0.8906 - val_loss: 0.4216 - val_sparse_categorical_accuracy: 0.8072\n",
            "Epoch 36/150\n",
            "90/90 [==============================] - 22s 250ms/step - loss: 0.2771 - sparse_categorical_accuracy: 0.8997 - val_loss: 0.4273 - val_sparse_categorical_accuracy: 0.7989\n",
            "Epoch 37/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.2820 - sparse_categorical_accuracy: 0.8920 - val_loss: 0.4229 - val_sparse_categorical_accuracy: 0.7961\n",
            "Epoch 38/150\n",
            "90/90 [==============================] - 22s 241ms/step - loss: 0.2811 - sparse_categorical_accuracy: 0.8934 - val_loss: 0.4185 - val_sparse_categorical_accuracy: 0.8017\n",
            "Epoch 39/150\n",
            "90/90 [==============================] - 22s 241ms/step - loss: 0.2766 - sparse_categorical_accuracy: 0.8972 - val_loss: 0.4153 - val_sparse_categorical_accuracy: 0.8044\n",
            "Epoch 40/150\n",
            "90/90 [==============================] - 22s 241ms/step - loss: 0.2759 - sparse_categorical_accuracy: 0.8951 - val_loss: 0.4125 - val_sparse_categorical_accuracy: 0.8100\n",
            "Epoch 41/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.2596 - sparse_categorical_accuracy: 0.9073 - val_loss: 0.4145 - val_sparse_categorical_accuracy: 0.8044\n",
            "Epoch 42/150\n",
            "90/90 [==============================] - 22s 241ms/step - loss: 0.2605 - sparse_categorical_accuracy: 0.9038 - val_loss: 0.4141 - val_sparse_categorical_accuracy: 0.8017\n",
            "Epoch 43/150\n",
            "90/90 [==============================] - 22s 241ms/step - loss: 0.2661 - sparse_categorical_accuracy: 0.9028 - val_loss: 0.4118 - val_sparse_categorical_accuracy: 0.8086\n",
            "Epoch 44/150\n",
            "90/90 [==============================] - 22s 242ms/step - loss: 0.2586 - sparse_categorical_accuracy: 0.9049 - val_loss: 0.4114 - val_sparse_categorical_accuracy: 0.8044\n",
            "Epoch 45/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.2485 - sparse_categorical_accuracy: 0.9115 - val_loss: 0.4125 - val_sparse_categorical_accuracy: 0.8058\n",
            "Epoch 46/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.2495 - sparse_categorical_accuracy: 0.9069 - val_loss: 0.4109 - val_sparse_categorical_accuracy: 0.8072\n",
            "Epoch 47/150\n",
            "90/90 [==============================] - 22s 241ms/step - loss: 0.2439 - sparse_categorical_accuracy: 0.9059 - val_loss: 0.4124 - val_sparse_categorical_accuracy: 0.8044\n",
            "Epoch 48/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.2333 - sparse_categorical_accuracy: 0.9149 - val_loss: 0.4050 - val_sparse_categorical_accuracy: 0.8100\n",
            "Epoch 49/150\n",
            "90/90 [==============================] - 22s 241ms/step - loss: 0.2322 - sparse_categorical_accuracy: 0.9139 - val_loss: 0.4072 - val_sparse_categorical_accuracy: 0.8114\n",
            "Epoch 50/150\n",
            "90/90 [==============================] - 22s 241ms/step - loss: 0.2270 - sparse_categorical_accuracy: 0.9108 - val_loss: 0.4058 - val_sparse_categorical_accuracy: 0.8072\n",
            "Epoch 51/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.2274 - sparse_categorical_accuracy: 0.9226 - val_loss: 0.4103 - val_sparse_categorical_accuracy: 0.8058\n",
            "Epoch 52/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.2225 - sparse_categorical_accuracy: 0.9222 - val_loss: 0.4055 - val_sparse_categorical_accuracy: 0.8114\n",
            "Epoch 53/150\n",
            "90/90 [==============================] - 22s 241ms/step - loss: 0.2290 - sparse_categorical_accuracy: 0.9142 - val_loss: 0.4003 - val_sparse_categorical_accuracy: 0.8072\n",
            "Epoch 54/150\n",
            "90/90 [==============================] - 22s 241ms/step - loss: 0.2220 - sparse_categorical_accuracy: 0.9146 - val_loss: 0.4033 - val_sparse_categorical_accuracy: 0.8128\n",
            "Epoch 55/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.2133 - sparse_categorical_accuracy: 0.9267 - val_loss: 0.3950 - val_sparse_categorical_accuracy: 0.8155\n",
            "Epoch 56/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.2093 - sparse_categorical_accuracy: 0.9299 - val_loss: 0.4020 - val_sparse_categorical_accuracy: 0.8169\n",
            "Epoch 57/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.2122 - sparse_categorical_accuracy: 0.9229 - val_loss: 0.3985 - val_sparse_categorical_accuracy: 0.8141\n",
            "Epoch 58/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.2109 - sparse_categorical_accuracy: 0.9278 - val_loss: 0.3996 - val_sparse_categorical_accuracy: 0.8128\n",
            "Epoch 59/150\n",
            "90/90 [==============================] - 22s 242ms/step - loss: 0.2005 - sparse_categorical_accuracy: 0.9285 - val_loss: 0.3944 - val_sparse_categorical_accuracy: 0.8239\n",
            "Epoch 60/150\n",
            "90/90 [==============================] - 22s 241ms/step - loss: 0.2080 - sparse_categorical_accuracy: 0.9271 - val_loss: 0.3941 - val_sparse_categorical_accuracy: 0.8211\n",
            "Epoch 61/150\n",
            "90/90 [==============================] - 22s 242ms/step - loss: 0.1952 - sparse_categorical_accuracy: 0.9347 - val_loss: 0.3965 - val_sparse_categorical_accuracy: 0.8114\n",
            "Epoch 62/150\n",
            "90/90 [==============================] - 22s 250ms/step - loss: 0.1975 - sparse_categorical_accuracy: 0.9285 - val_loss: 0.3957 - val_sparse_categorical_accuracy: 0.8225\n",
            "Epoch 63/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.2058 - sparse_categorical_accuracy: 0.9253 - val_loss: 0.3897 - val_sparse_categorical_accuracy: 0.8128\n",
            "Epoch 64/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.1903 - sparse_categorical_accuracy: 0.9340 - val_loss: 0.3952 - val_sparse_categorical_accuracy: 0.8239\n",
            "Epoch 65/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.1898 - sparse_categorical_accuracy: 0.9285 - val_loss: 0.3924 - val_sparse_categorical_accuracy: 0.8211\n",
            "Epoch 66/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.1934 - sparse_categorical_accuracy: 0.9299 - val_loss: 0.3914 - val_sparse_categorical_accuracy: 0.8225\n",
            "Epoch 67/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.1905 - sparse_categorical_accuracy: 0.9347 - val_loss: 0.3948 - val_sparse_categorical_accuracy: 0.8141\n",
            "Epoch 68/150\n",
            "90/90 [==============================] - 22s 249ms/step - loss: 0.1842 - sparse_categorical_accuracy: 0.9337 - val_loss: 0.3927 - val_sparse_categorical_accuracy: 0.8100\n",
            "Epoch 69/150\n",
            "90/90 [==============================] - 22s 241ms/step - loss: 0.1870 - sparse_categorical_accuracy: 0.9344 - val_loss: 0.3930 - val_sparse_categorical_accuracy: 0.8211\n",
            "Epoch 70/150\n",
            "90/90 [==============================] - 22s 241ms/step - loss: 0.1754 - sparse_categorical_accuracy: 0.9438 - val_loss: 0.3892 - val_sparse_categorical_accuracy: 0.8183\n",
            "Epoch 71/150\n",
            "90/90 [==============================] - 22s 248ms/step - loss: 0.1689 - sparse_categorical_accuracy: 0.9455 - val_loss: 0.3901 - val_sparse_categorical_accuracy: 0.8225\n",
            "Epoch 72/150\n",
            "90/90 [==============================] - 22s 248ms/step - loss: 0.1809 - sparse_categorical_accuracy: 0.9406 - val_loss: 0.3959 - val_sparse_categorical_accuracy: 0.8197\n",
            "Epoch 73/150\n",
            "90/90 [==============================] - 22s 248ms/step - loss: 0.1697 - sparse_categorical_accuracy: 0.9382 - val_loss: 0.3932 - val_sparse_categorical_accuracy: 0.8225\n",
            "Epoch 74/150\n",
            "90/90 [==============================] - 22s 248ms/step - loss: 0.1656 - sparse_categorical_accuracy: 0.9483 - val_loss: 0.3932 - val_sparse_categorical_accuracy: 0.8155\n",
            "Epoch 75/150\n",
            "90/90 [==============================] - 22s 239ms/step - loss: 0.1774 - sparse_categorical_accuracy: 0.9406 - val_loss: 0.3860 - val_sparse_categorical_accuracy: 0.8225\n",
            "Epoch 76/150\n",
            "90/90 [==============================] - 22s 240ms/step - loss: 0.1593 - sparse_categorical_accuracy: 0.9490 - val_loss: 0.3865 - val_sparse_categorical_accuracy: 0.8141\n",
            "Epoch 77/150\n",
            "90/90 [==============================] - 22s 248ms/step - loss: 0.1697 - sparse_categorical_accuracy: 0.9410 - val_loss: 0.3845 - val_sparse_categorical_accuracy: 0.8169\n",
            "Epoch 78/150\n",
            "90/90 [==============================] - 22s 248ms/step - loss: 0.1594 - sparse_categorical_accuracy: 0.9476 - val_loss: 0.3888 - val_sparse_categorical_accuracy: 0.8155\n",
            "Epoch 79/150\n",
            "90/90 [==============================] - 22s 248ms/step - loss: 0.1577 - sparse_categorical_accuracy: 0.9455 - val_loss: 0.3887 - val_sparse_categorical_accuracy: 0.8169\n",
            "Epoch 80/150\n",
            "90/90 [==============================] - 22s 240ms/step - loss: 0.1649 - sparse_categorical_accuracy: 0.9413 - val_loss: 0.3908 - val_sparse_categorical_accuracy: 0.8183\n",
            "Epoch 81/150\n",
            "90/90 [==============================] - 22s 247ms/step - loss: 0.1560 - sparse_categorical_accuracy: 0.9483 - val_loss: 0.3929 - val_sparse_categorical_accuracy: 0.8141\n",
            "Epoch 82/150\n",
            "90/90 [==============================] - 22s 239ms/step - loss: 0.1570 - sparse_categorical_accuracy: 0.9493 - val_loss: 0.3921 - val_sparse_categorical_accuracy: 0.8169\n",
            "Epoch 83/150\n",
            "90/90 [==============================] - 22s 248ms/step - loss: 0.1509 - sparse_categorical_accuracy: 0.9514 - val_loss: 0.3891 - val_sparse_categorical_accuracy: 0.8211\n",
            "Epoch 84/150\n",
            "90/90 [==============================] - 22s 248ms/step - loss: 0.1453 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.3929 - val_sparse_categorical_accuracy: 0.8211\n",
            "Epoch 85/150\n",
            "90/90 [==============================] - 22s 247ms/step - loss: 0.1494 - sparse_categorical_accuracy: 0.9524 - val_loss: 0.3921 - val_sparse_categorical_accuracy: 0.8169\n",
            "Epoch 86/150\n",
            "90/90 [==============================] - 22s 247ms/step - loss: 0.1524 - sparse_categorical_accuracy: 0.9538 - val_loss: 0.3900 - val_sparse_categorical_accuracy: 0.8183\n",
            "Epoch 87/150\n",
            "90/90 [==============================] - 22s 248ms/step - loss: 0.1422 - sparse_categorical_accuracy: 0.9476 - val_loss: 0.3942 - val_sparse_categorical_accuracy: 0.8239\n",
            "42/42 [==============================] - 3s 82ms/step - loss: 0.3627 - sparse_categorical_accuracy: 0.8477\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3627089560031891, 0.8477272987365723]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "\n",
        "In about 110-120 epochs (25s each on Colab), the model reaches a training\n",
        "accuracy of ~0.95, validation accuracy of ~84 and a testing\n",
        "accuracy of ~85, without hyperparameter tuning. And that is for a model\n",
        "with less than 100k parameters. Of course, parameter count and accuracy could be\n",
        "improved by a hyperparameter search and a more sophisticated learning rate\n",
        "schedule, or a different optimizer."
      ],
      "metadata": {
        "id": "sYfc6anJ00hd"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}